{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a feature vector of image tokens, compute its attention pattern to text tokens. This is crucial tool to find circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import join\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"/n/holylabs/LABS/sompolinsky_lab/Everyone/xupan/DiffusionObjectRelation/PixArt-alpha\")\n",
    "from diffusion.utils.misc import read_config, set_random_seed, \\\n",
    "    init_random_seed, DebugUnderflowOverflow\n",
    "sys.path.append(\"/n/holylabs/LABS/sompolinsky_lab/Everyone/xupan/DiffusionObjectRelation\")\n",
    "from utils.pixart_sampling_utils import visualize_prompts, load_embed_and_mask\n",
    "from utils.pixart_utils import construct_diffuser_pipeline_from_config, construct_pixart_transformer_from_config, state_dict_convert\n",
    "from utils.attention_map_store_utils import replace_attn_processor, AttnProcessor2_0_Store, PixArtAttentionVisualizer_Store\n",
    "\n",
    "# subclass a new pipeline from PixArtAlphaPipeline\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch import vmap # Import vmap\n",
    "from typing import Callable, List, Optional, Tuple, Union, Dict, Any\n",
    "from diffusers.pipelines.pipeline_utils import DiffusionPipeline, ImagePipelineOutput\n",
    "from diffusers.pipelines.pixart_alpha.pipeline_pixart_alpha import retrieve_timesteps, PixArtAlphaPipeline\n",
    "import numpy as np\n",
    "# from diffusers.pipelines.pixart_alpha import EXAMPLE_DOC_STRING # Keep commented\n",
    "\n",
    "class PixArtAlphaPipeline_custom_jacobian(PixArtAlphaPipeline):\n",
    "    \n",
    "    # @replace_example_docstring(EXAMPLE_DOC_STRING)\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        negative_prompt: str = \"\",\n",
    "        num_inference_steps: int = 20,\n",
    "        timesteps: List[int] = None,\n",
    "        sigmas: List[float] = None,\n",
    "        guidance_scale: float = 4.5,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        prompt_attention_mask: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        callback: Optional[Callable[[int, int, torch.Tensor], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        clean_caption: bool = True,\n",
    "        use_resolution_binning: bool = True,\n",
    "        max_sequence_length: int = 120,\n",
    "        return_sample_pred_traj: bool = False,\n",
    "        device: str = \"cuda\",\n",
    "        weight_dtype: torch.dtype = torch.float16,\n",
    "        jacobian: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Union[ImagePipelineOutput, Tuple]:\n",
    "        \"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n",
    "                instead.\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
    "                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
    "                less than `1`).\n",
    "            num_inference_steps (`int`, *optional*, defaults to 100):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n",
    "                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n",
    "                passed will be used. Must be in descending order.\n",
    "            sigmas (`List[float]`, *optional*):\n",
    "                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n",
    "                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n",
    "                will be used.\n",
    "            guidance_scale (`float`, *optional*, defaults to 4.5):\n",
    "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
    "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
    "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
    "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
    "                usually at the expense of lower image quality.\n",
    "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate per prompt.\n",
    "            height (`int`, *optional*, defaults to self.unet.config.sample_size):\n",
    "                The height in pixels of the generated image.\n",
    "            width (`int`, *optional*, defaults to self.unet.config.sample_size):\n",
    "                The width in pixels of the generated image.\n",
    "            eta (`float`, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (Î·) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
    "                [`schedulers.DDIMScheduler`], will be ignored for others.\n",
    "            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
    "                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
    "                to make generation deterministic.\n",
    "            latents (`torch.Tensor`, *optional*):\n",
    "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
    "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
    "                tensor will ge generated by sampling using the supplied random `generator`.\n",
    "            prompt_embeds (`torch.Tensor`, *optional*):\n",
    "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
    "                provided, text embeddings will be generated from `prompt` input argument.\n",
    "            prompt_attention_mask (`torch.Tensor`, *optional*): Pre-generated attention mask for text embeddings.\n",
    "            negative_prompt_embeds (`torch.Tensor`, *optional*):\n",
    "                Pre-generated negative text embeddings. For PixArt-Alpha this negative prompt should be \"\". If not\n",
    "                provided, negative_prompt_embeds will be generated from `negative_prompt` input argument.\n",
    "            negative_prompt_attention_mask (`torch.Tensor`, *optional*):\n",
    "                Pre-generated attention mask for negative text embeddings.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.stable_diffusion.IFPipelineOutput`] instead of a plain tuple.\n",
    "            callback (`Callable`, *optional*):\n",
    "                A function that will be called every `callback_steps` steps during inference. The function will be\n",
    "                called with the following arguments: `callback(step: int, timestep: int, latents: torch.Tensor)`.\n",
    "            callback_steps (`int`, *optional*, defaults to 1):\n",
    "                The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
    "                called at every step.\n",
    "            clean_caption (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to clean the caption before creating embeddings. Requires `beautifulsoup4` and `ftfy` to\n",
    "                be installed. If the dependencies are not installed, the embeddings will be created from the raw\n",
    "                prompt.\n",
    "            use_resolution_binning (`bool` defaults to `True`):\n",
    "                If set to `True`, the requested height and width are first mapped to the closest resolutions using\n",
    "                `ASPECT_RATIO_1024_BIN`. After the produced latents are decoded into images, they are resized back to\n",
    "                the requested resolution. Useful for generating non-square images.\n",
    "            max_sequence_length (`int` defaults to 120): Maximum sequence length to use with the `prompt`.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.ImagePipelineOutput`] or `tuple`:\n",
    "                If `return_dict` is `True`, [`~pipelines.ImagePipelineOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is a list with the generated images\n",
    "        \"\"\"\n",
    "        if \"mask_feature\" in kwargs:\n",
    "            deprecation_message = \"The use of `mask_feature` is deprecated. It is no longer used in any computation and that doesn't affect the end results. It will be removed in a future version.\"\n",
    "            # deprecate(\"mask_feature\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        height = height or self.transformer.config.sample_size * self.vae_scale_factor\n",
    "        width = width or self.transformer.config.sample_size * self.vae_scale_factor\n",
    "        # if use_resolution_binning:\n",
    "        #     if self.transformer.config.sample_size == 128:\n",
    "        #         aspect_ratio_bin = ASPECT_RATIO_1024_BIN\n",
    "        #     elif self.transformer.config.sample_size == 64:\n",
    "        #         aspect_ratio_bin = ASPECT_RATIO_512_BIN\n",
    "        #     elif self.transformer.config.sample_size == 32:\n",
    "        #         aspect_ratio_bin = ASPECT_RATIO_256_BIN\n",
    "        #     else:\n",
    "        #         raise ValueError(\"Invalid sample size\")\n",
    "        #     orig_height, orig_width = height, width\n",
    "        #     height, width = self.image_processor.classify_height_width_bin(height, width, ratios=aspect_ratio_bin)\n",
    "\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            height,\n",
    "            width,\n",
    "            negative_prompt,\n",
    "            callback_steps,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            prompt_attention_mask,\n",
    "            negative_prompt_attention_mask,\n",
    "        )\n",
    "\n",
    "        # 2. Default height and width to transformer\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "        \n",
    "        # Save original batch size for Jacobian calculation later\n",
    "        orig_batch_size = batch_size\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            prompt_attention_mask,\n",
    "            negative_prompt_embeds,\n",
    "            negative_prompt_attention_mask,\n",
    "        ) = self.encode_prompt(\n",
    "            prompt,\n",
    "            do_classifier_free_guidance,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            device=device,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            prompt_attention_mask=prompt_attention_mask,\n",
    "            negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
    "            clean_caption=clean_caption,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "        )\n",
    "        if do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "            prompt_attention_mask = torch.cat([negative_prompt_attention_mask, prompt_attention_mask], dim=0)\n",
    "        # print(prompt_embeds.shape)\n",
    "        # print(prompt_attention_mask.shape)\n",
    "        # 4. Prepare timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(\n",
    "            self.scheduler, num_inference_steps, device, timesteps, sigmas\n",
    "        )\n",
    "\n",
    "        # 5. Prepare latents.\n",
    "        latent_channels = self.transformer.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            latent_channels,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 6.1 Prepare micro-conditions.\n",
    "        added_cond_kwargs = {\"resolution\": None, \"aspect_ratio\": None}\n",
    "        if self.transformer.config.sample_size == 128:\n",
    "            resolution = torch.tensor([height, width]).repeat(batch_size * num_images_per_prompt, 1)\n",
    "            aspect_ratio = torch.tensor([float(height / width)]).repeat(batch_size * num_images_per_prompt, 1)\n",
    "            resolution = resolution.to(dtype=prompt_embeds.dtype, device=device)\n",
    "            aspect_ratio = aspect_ratio.to(dtype=prompt_embeds.dtype, device=device)\n",
    "\n",
    "            if do_classifier_free_guidance:\n",
    "                resolution = torch.cat([resolution, resolution], dim=0)\n",
    "                aspect_ratio = torch.cat([aspect_ratio, aspect_ratio], dim=0)\n",
    "\n",
    "            added_cond_kwargs = {\"resolution\": resolution, \"aspect_ratio\": aspect_ratio}\n",
    "\n",
    "        # 7. Denoising loop\n",
    "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
    "\n",
    "        pred_traj = []\n",
    "        latents_traj = []\n",
    "        t_traj = []\n",
    "        jacobian_traj = []\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                # Expand latents for classifier free guidance\n",
    "                # latents shape is (orig_batch_size, C, H, W)\n",
    "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                # latent_model_input shape is (2*orig_batch_size, C, H, W) or (orig_batch_size, C, H, W)\n",
    "\n",
    "                # Scale the latents (scheduler input)\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # Prepare timestep embedding\n",
    "                current_timestep = t\n",
    "                if not torch.is_tensor(current_timestep):\n",
    "                    is_mps = latent_model_input.device.type == \"mps\"\n",
    "                    if isinstance(current_timestep, float):\n",
    "                        dtype = torch.float32 if is_mps else torch.float64\n",
    "                    else:\n",
    "                        dtype = torch.int32 if is_mps else torch.int64\n",
    "                    current_timestep = torch.tensor([current_timestep], dtype=dtype, device=latent_model_input.device)\n",
    "                elif len(current_timestep.shape) == 0:\n",
    "                    current_timestep = current_timestep[None].to(latent_model_input.device)\n",
    "                # Broadcast timestep to batch dimension\n",
    "                # current_timestep shape is (2*orig_batch_size,) or (orig_batch_size,)\n",
    "                current_timestep = current_timestep.expand(latent_model_input.shape[0])\n",
    "\n",
    "                # Predict the noise residual using the transformer\n",
    "                # prompt_embeds shape is (2*orig_batch_size, SeqLen, Dim) or (orig_batch_size, SeqLen, Dim)\n",
    "                # prompt_attention_mask shape is (2*orig_batch_size, SeqLen) or (orig_batch_size, SeqLen)\n",
    "                # added_cond_kwargs has tensors of shape (2*orig_batch_size, ...) or (orig_batch_size, ...)\n",
    "                noise_pred_out = self.transformer(\n",
    "                    latent_model_input,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    encoder_attention_mask=prompt_attention_mask,\n",
    "                    timestep=current_timestep,\n",
    "                    added_cond_kwargs=added_cond_kwargs,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "                # noise_pred_out shape is (2*orig_batch_size, C_out, H, W) or (orig_batch_size, C_out, H, W)\n",
    "\n",
    "                # Perform classifier-free guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred_out.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                else:\n",
    "                    noise_pred = noise_pred_out\n",
    "                # noise_pred shape is (orig_batch_size, C_out, H, W)\n",
    "\n",
    "                # Handle learned sigma prediction if necessary\n",
    "                if self.transformer.config.out_channels // 2 == latent_channels:\n",
    "                    # Assumes the model predicts both noise and variance, we take only the noise\n",
    "                    noise_pred = noise_pred.chunk(2, dim=1)[0]\n",
    "                # noise_pred final shape is (orig_batch_size, C, H, W)\n",
    "\n",
    "                latents_shape = latents.shape\n",
    "                if jacobian:\n",
    "                    latents_for_jacobian = latents.clone().detach().requires_grad_(True)\n",
    "                    # latents_for_jacobian = latents_for_jacobian.reshape(latents_shape[0], -1)\n",
    "                \n",
    "                    def func_for_jacobian(x):\n",
    "                        latent_model_input = torch.cat([x] * 2) if do_classifier_free_guidance else x\n",
    "                        # latent_model_input shape is (2*orig_batch_size, C, H, W) or (orig_batch_size, C, H, W)\n",
    "\n",
    "                        # Scale the latents (scheduler input)\n",
    "                        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                        # Prepare timestep embedding\n",
    "                        current_timestep = t\n",
    "                        if not torch.is_tensor(current_timestep):\n",
    "                            is_mps = latent_model_input.device.type == \"mps\"\n",
    "                            if isinstance(current_timestep, float):\n",
    "                                dtype = torch.float32 if is_mps else torch.float64\n",
    "                            else:\n",
    "                                dtype = torch.int32 if is_mps else torch.int64\n",
    "                            current_timestep = torch.tensor([current_timestep], dtype=dtype, device=latent_model_input.device)\n",
    "                        elif len(current_timestep.shape) == 0:\n",
    "                            current_timestep = current_timestep[None].to(latent_model_input.device)\n",
    "                        # Broadcast timestep to batch dimension\n",
    "                        # current_timestep shape is (2*orig_batch_size,) or (orig_batch_size,)\n",
    "                        current_timestep = current_timestep.expand(latent_model_input.shape[0])\n",
    "\n",
    "                        # Predict the noise residual using the transformer\n",
    "                        # prompt_embeds shape is (2*orig_batch_size, SeqLen, Dim) or (orig_batch_size, SeqLen, Dim)\n",
    "                        # prompt_attention_mask shape is (2*orig_batch_size, SeqLen) or (orig_batch_size, SeqLen)\n",
    "                        # added_cond_kwargs has tensors of shape (2*orig_batch_size, ...) or (orig_batch_size, ...)\n",
    "                        noise_pred_out = self.transformer(\n",
    "                            latent_model_input,\n",
    "                            encoder_hidden_states=prompt_embeds,\n",
    "                            encoder_attention_mask=prompt_attention_mask,\n",
    "                            timestep=current_timestep,\n",
    "                            added_cond_kwargs=added_cond_kwargs,\n",
    "                            return_dict=False,\n",
    "                        )[0]\n",
    "                        # noise_pred_out shape is (2*orig_batch_size, C_out, H, W) or (orig_batch_size, C_out, H, W)\n",
    "\n",
    "                        # Perform classifier-free guidance\n",
    "                        if do_classifier_free_guidance:\n",
    "                            noise_pred_uncond, noise_pred_text = noise_pred_out.chunk(2)\n",
    "                            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                            noise_pred = noise_pred.chunk(2, dim=1)[0]\n",
    "                        else:\n",
    "                            noise_pred = noise_pred_out\n",
    "                        # Flatten the noise prediction\n",
    "                        # return noise_pred.flatten()\n",
    "                        return noise_pred\n",
    "                    \n",
    "                    # Compute the Jacobian of the noise prediction with respect to the latents\n",
    "                    jacobian = torch.autograd.functional.jacobian(func_for_jacobian, latents_for_jacobian)\n",
    "                    jacobian_traj.append(jacobian.detach().cpu())\n",
    "                # jacobian = jacobian.reshape(latents_shape[0], -1, latents_shape[1], latents_shape[2], latents_shape[3])\n",
    "\n",
    "                # Store trajectories (use original latents, not the one requiring grad)\n",
    "                latents_traj.append(latents)\n",
    "                pred_traj.append(noise_pred.cpu()) # Store the final noise prediction used for the step\n",
    "\n",
    "                # Compute the previous noisy sample x_t -> x_t-1 using the scheduler\n",
    "                # Use the original `latents` tensor, which does not require grad here.\n",
    "                if num_inference_steps == 1:\n",
    "                    # Special case for single-step sampling (e.g., DMD)\n",
    "                    latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).pred_original_sample\n",
    "                else:\n",
    "                    # Standard scheduler step\n",
    "                    latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "                \n",
    "                # Store timestep\n",
    "                t_traj.append(t)\n",
    "\n",
    "                # Call the callback, if provided, and update progress bar\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                        callback(step_idx, t, latents)\n",
    "\n",
    "        latents_traj.append(latents)\n",
    "        \n",
    "        images = []\n",
    "        if not output_type == \"latent\":\n",
    "            images.append(latents_traj[-1])\n",
    "            # for i in range(len(latents_traj)):\n",
    "            #     image = self.vae.decode(latents_traj[i] / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "            #     image = self.image_processor.postprocess(image.detach(), output_type=output_type)\n",
    "                # images.append(image)\n",
    "                # if use_resolution_binning:\n",
    "            #     image = self.image_processor.resize_and_crop_tensor(image, orig_width, orig_height)\n",
    "        else:\n",
    "            for i in range(len(latents_traj)):\n",
    "                image = self.vae.decode(latents_traj[i].to(weight_dtype) / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "                image = self.image_processor.postprocess(image.detach(), output_type=\"pil\")\n",
    "                images.append(image)\n",
    "            # images = [ImagePipelineOutput(images=image) for image in images]\n",
    "\n",
    "        # if not output_type == \"latent\":\n",
    "        #     image = self.image_processor.postprocess(image.detach(), output_type=output_type)\n",
    "\n",
    "        # Offload all models\n",
    "        self.maybe_free_model_hooks()\n",
    "        latents_traj = torch.stack(latents_traj)\n",
    "        pred_traj = torch.stack(pred_traj)\n",
    "        output_dict = {}\n",
    "        output_dict['images'] = images\n",
    "        output_dict['pred_traj'] = pred_traj\n",
    "        output_dict['latents_traj'] = [latent.cpu() for latent in latents_traj]\n",
    "        output_dict['t_traj'] = t_traj\n",
    "        if jacobian:\n",
    "            jacobian_traj = torch.stack(jacobian_traj)\n",
    "            output_dict['jacobian_traj'] = jacobian_traj\n",
    "        \n",
    "        # t_traj = torch.stack(t_traj) # Stacking list of tensors might be inefficient if they are scalars\n",
    "        if not return_dict:\n",
    "            return (images,)\n",
    "        if return_sample_pred_traj:\n",
    "            # Convert t_traj list of tensors to a single tensor if needed, or return as list\n",
    "            return output_dict\n",
    "        return images\n",
    "    \n",
    "    \n",
    "    \n",
    "def visualize_prompts_with_traj_jacobian(pipeline, validation_prompts, prompt_cache_dir, max_length=120, weight_dtype=torch.float16,\n",
    "                   num_inference_steps=14, guidance_scale=4.5, num_images_per_prompt=1, device=\"cuda\", random_seed=0, return_sample_pred_traj=True, jacobian=False): # Default num_images_per_prompt=1 for Jacobian\n",
    "    # logger.info(\"Running validation... \")\n",
    "    # device = accelerator.device\n",
    "    # model = accelerator.unwrap_model(model)\n",
    "    if validation_prompts is None:\n",
    "        validation_prompts = [\n",
    "            \"triangle is to the upper left of square\", \n",
    "            \"blue triangle is to the upper left of red square\", \n",
    "            \"triangle is above and to the right of square\", \n",
    "            \"blue circle is above and to the right of blue square\", \n",
    "            \"triangle is to the left of square\", \n",
    "            \"triangle is to the left of triangle\", \n",
    "            \"circle is below red square\",\n",
    "            \"red circle is to the left of blue square\",\n",
    "            \"blue square is to the right of red circle\",\n",
    "            \"red circle is above square\",\n",
    "            \"triangle is above red circle\",\n",
    "            \"red is above blue\",\n",
    "            \"red is to the left of red\",\n",
    "            \"blue triangle is above red triangle\", \n",
    "            \"blue circle is above blue square\", \n",
    "        ]\n",
    "    pipeline = pipeline.to(device)\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "    if random_seed is None:\n",
    "        generator = None\n",
    "    else:\n",
    "        generator = torch.Generator(device=device).manual_seed(random_seed)\n",
    "    image_logs = []\n",
    "    images = []\n",
    "    latents = []\n",
    "    pred_traj_list = []\n",
    "    latents_traj_list = []\n",
    "    t_traj_list = []\n",
    "    jacobian_traj_list = []\n",
    "    images_list = []\n",
    "    uncond_data = torch.load(f'{prompt_cache_dir}/uncond_{max_length}token.pth', map_location='cpu')\n",
    "    uncond_prompt_embeds = uncond_data['caption_embeds'].to(device)\n",
    "    uncond_prompt_attention_mask = uncond_data['emb_mask'].to(device)\n",
    "    visualized_prompts = []\n",
    "    for _, prompt in enumerate(validation_prompts):\n",
    "        if not os.path.exists(f'{prompt_cache_dir}/{prompt}_{max_length}token.pth'):\n",
    "            continue\n",
    "        embed = torch.load(f'{prompt_cache_dir}/{prompt}_{max_length}token.pth', map_location='cpu')\n",
    "        caption_embs, emb_masks = embed['caption_embeds'].to(device), embed['emb_mask'].to(device)\n",
    "        \n",
    "        # Ensure num_images_per_prompt=1 if calculating Jacobian for simplicity\n",
    "        # The Jacobian code assumes orig_batch_size corresponds to prompt batch size\n",
    "        current_num_images_per_prompt = 1 if return_sample_pred_traj else num_images_per_prompt \n",
    "\n",
    "        output = pipeline(\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            num_images_per_prompt=current_num_images_per_prompt, \n",
    "            generator=generator,\n",
    "            guidance_scale=guidance_scale,\n",
    "            prompt_embeds=caption_embs,\n",
    "            prompt_attention_mask=emb_masks,\n",
    "            negative_prompt=None,\n",
    "            negative_prompt_embeds=uncond_prompt_embeds,\n",
    "            negative_prompt_attention_mask=uncond_prompt_attention_mask,\n",
    "            use_resolution_binning=False, # need this for smaller images like ours. \n",
    "            return_sample_pred_traj=True, # Always true to get trajectories\n",
    "            output_type=\"latent\",\n",
    "            weight_dtype=weight_dtype,\n",
    "            jacobian=jacobian,\n",
    "        )\n",
    "        images_list.append(output['images'])\n",
    "        pred_traj_list.append(output['pred_traj'])\n",
    "        latents_traj_list.append(output['latents_traj'])\n",
    "        t_traj_list.append(output['t_traj'])\n",
    "        if jacobian:\n",
    "            jacobian_traj_list.append(output['jacobian_traj'])\n",
    "        visualized_prompts.append(prompt)\n",
    "  \n",
    "    return images_list, latents_traj_list, pred_traj_list, t_traj_list, jacobian_traj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|ââââââââââ| 3/3 [00:00<00:00, 36.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_prompts:  ['triangle is to the upper left of square', 'blue triangle is to the upper left of red square', 'triangle is above and to the right of square', 'blue circle is above and to the right of blue square', 'triangle is to the left of square', 'triangle is to the left of triangle', 'circle is below red square', 'red circle is to the left of blue square', 'blue square is to the right of red circle', 'red circle is above square', 'triangle is above red circle', 'red is above blue', 'red is to the left of red', 'blue triangle is above red triangle', 'blue circle is above blue square']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/holylabs/LABS/sompolinsky_lab/Users/xupan/envs/pixart/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "savedir = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/results/objrel_rndembdposemb_DiT_B_pilot\"\n",
    "ckptdir = join(savedir, \"checkpoints\")\n",
    "config = read_config(join(savedir, 'config.py'))\n",
    "weight_dtype = torch.float32\n",
    "if config.mixed_precision == \"fp16\": # accelerator.\n",
    "    weight_dtype = torch.float16\n",
    "elif config.mixed_precision == \"bf16\": # accelerator.\n",
    "    weight_dtype = torch.bfloat16\n",
    "    \n",
    "pipeline = construct_diffuser_pipeline_from_config(config, pipeline_class=PixArtAlphaPipeline_custom_jacobian)\n",
    "ckpt = torch.load(join(ckptdir, \"epoch_4000_step_160000.pth\"))\n",
    "pipeline.transformer.load_state_dict(state_dict_convert(ckpt['state_dict_ema']))\n",
    "pipeline = pipeline.to(\"cuda\")\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "validation_prompts = config.validation_prompts\n",
    "print(\"validation_prompts: \", validation_prompts)\n",
    "\n",
    "prompt_cache_dir = config.prompt_cache_dir\n",
    "\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "T5_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_path, )#subfolder=\"tokenizer\")\n",
    "\n",
    "pipeline.transformer = replace_attn_processor(pipeline.transformer)\n",
    "\n",
    "image_logs, latents_traj_list, pred_traj_list, t_traj_list, jacobian_traj= visualize_prompts_with_traj_jacobian(\\\n",
    "                                                                                        pipeline, \n",
    "                                                                                        validation_prompts[7:8], \n",
    "                                                                                        prompt_cache_dir, \n",
    "                                                                                        max_length=config.model_max_length, \n",
    "                                                                                        weight_dtype=weight_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmcolors\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m jacobian_np \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian_traj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      5\u001b[0m n_plots \u001b[38;5;241m=\u001b[39m jacobian_np\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m ncols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(np\u001b[38;5;241m.\u001b[39msqrt(n_plots)))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "jacobian_np = jacobian_traj[0].numpy()\n",
    "\n",
    "n_plots = jacobian_np.shape[0]\n",
    "ncols = int(np.ceil(np.sqrt(n_plots)))\n",
    "nrows = int(np.ceil(n_plots / ncols))\n",
    "\n",
    "x = 9\n",
    "y = 12\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 3, nrows * 3)) # Adjust figsize as needed\n",
    "axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "for t in range(n_plots):\n",
    "    mean_jacobian = np.mean(jacobian_np[t,0,:,y,x,0,:,:,:], axis=(0,1))\n",
    "    # Use SymLogNorm for log scaling while handling positive/negative values and zero\n",
    "    \n",
    "    # Adjust linthresh based on the data range near zero you want linear\n",
    "    # Using the previous vmin/vmax for clipping within the norm\n",
    "    norm = mcolors.SymLogNorm(linthresh=0.001, linscale=1.0, vmin=-0.01, vmax=0.01, base=10)\n",
    "    im = axes[t].imshow(mean_jacobian, norm=norm, cmap='coolwarm') # Apply the norm\n",
    "    axes[t].set_title(f\"t={t}\")\n",
    "    axes[t].axis('off') # Hide axes ticks and labels\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(n_plots, nrows * ncols):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "axes[-1].imshow(image_logs[0]['images'][0])\n",
    "\n",
    "# Add a single colorbar for the entire figure\n",
    "# fig.colorbar(im, ax=axes[:n_plots], shrink=0.6) # Adjust shrink factor as needed\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

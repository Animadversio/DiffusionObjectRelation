{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5O8tQblzOVHu"
   },
   "source": [
    "# Training a basic SAE with SAELens\n",
    "\n",
    "This tutorial demonstrates training a simple, relatively small Sparse Autoencoder, specifically on the tiny-stories-1L-21M model. \n",
    "\n",
    "As the SAELens library is under active development, please open an issue if this tutorial is stale [here](https://github.com/jbloomAus/SAELens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shAFb9-lOVHu"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uy-b3CcSOVHu",
    "outputId": "58ce28d0-f91f-436d-cf87-76bb26e2ecaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home03/fjxdaisy/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: '/n/home03/fjxdaisy/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "#import transformer-lens\n",
    "sys.path.append(\"/n/netscratch/konkle_lab/Everyone/Jingxuan/DiffusionObjectRelation\")\n",
    "#import sae_lens\n",
    "#from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "from sae_lens.sae_training_runner_custom import SAETrainingRunner\n",
    "from sae_lens.config_custom import LanguageModelSAERunnerConfig\n",
    "import pickle\n",
    "import numpy as np\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "saveroot = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/results/objrel_rndembdposemb_DiT_B_pilot/latent_store\"\n",
    "\n",
    "latent_file = \"red_blue_8_pos_rndembposemb_img_latent_residual_prompt9_seed7.pkl\"\n",
    "#latent_file = \"red_blue_8_pos_rndembposemb_repr_vector_dataset_all.pkl\"\n",
    "latent_path = os.path.join(saveroot, latent_file)\n",
    "\n",
    "with open(latent_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_reshape_latent(pkl_path, time_step):\n",
    "    \"\"\"\n",
    "    Loads a latent tensor from a .pkl file, extracts the block_11_residual_spatial_state_traj,\n",
    "    selects a specific time step, and reshapes the tensor to batch x sequence length x feature dimension.\n",
    "\n",
    "    Parameters:\n",
    "        pkl_path (str): Path to the .pkl file.\n",
    "        time_step (int): The specific time step to extract from the latent tensor (first dimension).\n",
    "\n",
    "    Returns:\n",
    "        reshaped_tensor (numpy.ndarray): Reshaped tensor of shape [batch_size, sequence_len, feature_dim].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the .pkl file\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        # Extract the latent tensor\n",
    "        latent = data['block_11_residual_spatial_state_traj']  # Shape: [time_steps, batch, 8, 8, feature_dim]\n",
    "\n",
    "        # Check if the time step is valid\n",
    "        if time_step >= latent.shape[0]:\n",
    "            raise ValueError(f\"Invalid time_step {time_step}. Maximum allowed is {latent.shape[0] - 1}.\")\n",
    "\n",
    "        # Extract the tensor for the given time step\n",
    "        time_step_tensor = latent[time_step]  # Shape: [batch, 8, 8, feature_dim]\n",
    "\n",
    "        # Reshape the tensor to [batch, sequence_len (8*8), feature_dim]\n",
    "        batch_size, height, width, feature_dim = time_step_tensor.shape\n",
    "        sequence_len = height * width\n",
    "        reshaped_tensor = time_step_tensor.reshape(batch_size, sequence_len, feature_dim)\n",
    "\n",
    "        return reshaped_tensor\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file at {pkl_path} was not found.\")\n",
    "    except KeyError:\n",
    "        raise KeyError(\"The required key 'block_11_residual_spatial_state_traj' is missing in the .pkl file.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"An error occurred while processing the file: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = load_and_reshape_latent(latent_path, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 64, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "er3H1TDoOVHw"
   },
   "source": [
    "# Training an SAE\n",
    "\n",
    "Now we're ready to train out SAE. We'll make a runner config, instantiate the runner and the rest is taken care of for us!\n",
    "\n",
    "During training, you use weights and biases to check key metrics which indicate how well we are able to optimize the variables we care about.\n",
    "\n",
    "To get a better sense of which variables to look at, you can read my (Joseph's) post [here](https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream) and especially look at my weights and biases report [here](https://links-cdn.wandb.ai/wandb-public-images/links/jbloom/uue9i416.html).\n",
    "\n",
    "A few tips:\n",
    "- Feel free to reorganize your wandb dashboard to put L0, CE_Loss_score, explained variance and other key metrics in one section at the top.\n",
    "- Make a [run comparer](https://docs.wandb.ai/guides/app/features/panels/run-comparer) when tuning hyperparameters.\n",
    "- You can download the resulting sparse autoencoder / sparsity estimate from wandb and upload them to huggingface if you want to share your SAE with other.\n",
    "    - cfg.json (training config)\n",
    "    - sae_weight.safetensors (model weights)\n",
    "    - sparsity.safetensors (sparsity estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oAsZCAdJOVHw"
   },
   "outputs": [],
   "source": [
    "total_training_steps = 10  # probably we should do more\n",
    "batch_size = 128\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "l1_warm_up_steps = total_training_steps // 20  # 5% of training\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    d_in=768,  # the width of the mlp output.\n",
    "    # SAE Parameters\n",
    "    architecture=\"standard\",\n",
    "    mse_loss_normalization=None,  # We won't normalize the mse loss,\n",
    "    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.\n",
    "    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n",
    "    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=True,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    normalize_activations=\"expected_average_only_in\",\n",
    "    # Training Parameters\n",
    "    lr=5e-5,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n",
    "    l1_coefficient=5,  # will control how sparse the feature activations are\n",
    "    l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n",
    "    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    store_batch_size_prompts=16,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,  # we don't use ghost grads anymore.\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    # WANDB\n",
    "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"sae_lens_tutorial\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "#sparse_autoencoder = SAETrainingRunner(cfg).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "sparse_autoencoder = SAETrainingRunner(cfg).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption_feature_wmask  images\t\t\t       partition\n",
      "captions\t       img_vae_features_128resolution\n"
     ]
    }
   ],
   "source": [
    "!ls /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!cp -r /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2 \\\n",
    "       /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_rndemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 tokenization + T5 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from os.path import join\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from transformers import T5Tokenizer, T5EncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRelSingle_pilot1\"\n",
    "caption_dir = join(dataset_root, \"captions\")\n",
    "image_dir = join(dataset_root, \"images\")\n",
    "img_feat_dir = join(dataset_root, \"img_vae_features_128resolution\")\n",
    "text_feat_dir = join(dataset_root, \"caption_feature_wmask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcd8ecc29b746a9900e5f49ff45fe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T5_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_path, )#subfolder=\"tokenizer\")\n",
    "encoder = T5EncoderModel.from_pretrained(T5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_captions(caption_dir, tokenizer, num_captions=10000, model_max_length=20):\n",
    "    \"\"\"Tokenize captions from text files using T5 tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        caption_dir: Directory containing caption text files\n",
    "        tokenizer: T5 tokenizer instance\n",
    "        num_captions: Number of captions to process\n",
    "        model_max_length: Max sequence length for tokenization\n",
    "        \n",
    "    Returns:\n",
    "        input_ids_tsr: Tensor of tokenized input IDs\n",
    "        attention_mask_col: List of attention masks\n",
    "    \"\"\"\n",
    "    input_ids_col = []\n",
    "    attention_mask_col = []\n",
    "    \n",
    "    for i in trange(num_captions):\n",
    "        text = open(join(caption_dir, f\"{i}.txt\")).read()\n",
    "        text_tokens_and_mask = tokenizer(\n",
    "            text,\n",
    "            max_length=model_max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids_col.append(text_tokens_and_mask['input_ids'])\n",
    "        attention_mask_col.append(text_tokens_and_mask['attention_mask'])\n",
    "\n",
    "    input_ids_tsr = th.cat(input_ids_col, dim=0)\n",
    "    return input_ids_tsr, attention_mask_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8654f26e68f4d0d9fef14b7bf58a36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_max_length = 20\n",
    "# use T5 tokenizer \n",
    "corpus = []\n",
    "input_ids_col = []\n",
    "attention_mask_col = []\n",
    "for i in trange(10000):\n",
    "    text = open(join(caption_dir, f\"{i}.txt\")).read()\n",
    "    text_tokens_and_mask = tokenizer(\n",
    "        text,\n",
    "        max_length=model_max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids_col.append(text_tokens_and_mask['input_ids'])\n",
    "    attention_mask_col.append(text_tokens_and_mask['attention_mask'])\n",
    "    # break\n",
    "    # corpus.append(text)\n",
    "\n",
    "input_ids_tsr = th.cat(input_ids_col, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    3,     9,  1131, 19938,     1,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "tensor([    3,     9,  1131, 19938,     1,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# find unique input ids\n",
    "embed_dim = 4096 \n",
    "unique_input_ids, unique_indices = th.unique(input_ids_tsr, return_inverse=True)\n",
    "# create random word embeddings\n",
    "embedding_dict = th.randn(len(unique_input_ids), embed_dim, \n",
    "                          generator=th.Generator().manual_seed(42)) \\\n",
    "                            / np.sqrt(embed_dim) * 7.5\n",
    "input_ids2dict_ids = {idx.item(): id for id, idx in enumerate(unique_input_ids)}\n",
    "dict_ids2input_ids = {id: idx.item() for id, idx in enumerate(unique_input_ids)}\n",
    "# check if the unique input ids are the same as the input ids \n",
    "print(input_ids_tsr[0])\n",
    "print(unique_input_ids[unique_indices[0]])\n",
    "assert (input_ids_tsr == unique_input_ids[unique_indices]).all()\n",
    "th.save({\"unique_input_ids\": unique_input_ids, \n",
    "         \"embedding_dict\": embedding_dict,\n",
    "         \"input_ids2dict_ids\": input_ids2dict_ids, \n",
    "         \"dict_ids2input_ids\": dict_ids2input_ids}, \n",
    "         join(text_feat_dir, \"word_embedding_dict.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6b79cc178b440286a424c8297e2168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# map the input_ids to the word embeddings \n",
    "for sample_idx in trange(10000):\n",
    "    input_ids = input_ids_col[sample_idx]\n",
    "    embeddings = embedding_dict[unique_indices[sample_idx]]\n",
    "    embeddings = embeddings.unsqueeze(0)\n",
    "    if sample_idx % 1000 == 0:\n",
    "        # check if the embeddings shpae and date type are the same as the original ones \n",
    "        data = np.load(join(text_feat_dir, f\"{sample_idx}.npz\"))\n",
    "        assert (data[\"caption_feature\"].shape == embeddings.numpy().shape)\n",
    "        assert(data[\"caption_feature\"].dtype == embeddings.numpy().dtype)\n",
    "        assert (data[\"attention_mask\"] == attention_mask_col[sample_idx].numpy()).all()\n",
    "        assert(data[\"attention_mask\"].dtype == attention_mask_col[sample_idx].numpy().dtype)\n",
    "\n",
    "    np.savez(join(text_feat_dir, f\"{sample_idx}.npz\"), caption_feature=embeddings.numpy(), attention_mask=attention_mask_col[sample_idx].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "@torch.no_grad()\n",
    "def save_prompt_embeddings(tokenizer, text_encoder, validation_prompts, prompt_cache_dir=\"output/tmp/prompt_cache\", \n",
    "                           device=\"cuda\", max_length=20, t5_path=None, recompute=False):\n",
    "    \"\"\"Save T5 text embeddings for a list of prompts to cache directory.\n",
    "    \n",
    "    Args:\n",
    "        validation_prompts (list): List of text prompts to encode\n",
    "        prompt_cache_dir (str): Directory to save embeddings\n",
    "        device (str): Device to run encoding on\n",
    "        max_length (int): Max sequence length for tokenization\n",
    "        t5_path (str): Path to T5 model. If None, uses default path\n",
    "    \"\"\"\n",
    "    if t5_path is None:\n",
    "        t5_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\"\n",
    "    \n",
    "    result_col = []\n",
    "    os.makedirs(prompt_cache_dir, exist_ok=True)\n",
    "\n",
    "    # Load models\n",
    "    print(f\"Loading text encoder and tokenizer from {t5_path} ...\")\n",
    "    # tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "    # text_encoder = T5EncoderModel.from_pretrained(t5_path).to(device)\n",
    "    # text_encoder = text_encoder.to(device)\n",
    "\n",
    "    # Save unconditioned embedding\n",
    "    uncond = tokenizer(\"\", max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "    uncond_prompt_embeds = text_encoder(uncond.input_ids, attention_mask=uncond.attention_mask)[0]\n",
    "    torch.save({'caption_embeds': uncond_prompt_embeds, 'emb_mask': uncond.attention_mask, 'prompt': ''}, \n",
    "               join(prompt_cache_dir,f'uncond_{max_length}token.pth'))\n",
    "    result_col.append({'prompt': '', 'caption_embeds': uncond_prompt_embeds, 'emb_mask': uncond.attention_mask})\n",
    "\n",
    "    print(\"Preparing Visualization prompt embeddings...\")\n",
    "    print(f\"Saving visualizate prompt text embedding at {prompt_cache_dir}\")\n",
    "    \n",
    "    for prompt in validation_prompts:\n",
    "        if os.path.exists(join(prompt_cache_dir,f'{prompt}_{max_length}token.pth')) and not recompute:\n",
    "            result_col.append(torch.load(join(prompt_cache_dir,f'{prompt}_{max_length}token.pth')))\n",
    "            continue\n",
    "        print(f\"Mapping {prompt}...\")\n",
    "        caption_token = tokenizer(prompt, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "        caption_emb = text_encoder(caption_token.input_ids, attention_mask=caption_token.attention_mask)[0]\n",
    "        torch.save({'caption_embeds': caption_emb, 'emb_mask': caption_token.attention_mask, 'prompt': prompt}, \n",
    "                    join(prompt_cache_dir,f'{prompt}_{max_length}token.pth'))\n",
    "        result_col.append({'prompt': prompt, 'caption_embeds': caption_emb, 'emb_mask': caption_token.attention_mask})\n",
    "    print(\"Done!\")\n",
    "    # garbage collection\n",
    "    del tokenizer, text_encoder\n",
    "    torch.cuda.empty_cache()\n",
    "    return result_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5EmbeddingEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"t5-base\", device=\"cuda\", ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.encoder = T5EncoderModel.from_pretrained(model_name, ).to(torch.bfloat16)\n",
    "        self.encoder.eval()\n",
    "        self.device = device\n",
    "        self.encoder.to(device)\n",
    "\n",
    "    def __call__(self, input_ids, attention_mask=None):\n",
    "        return self.encode(input_ids, attention_mask)\n",
    "\n",
    "    def encode(self, input_ids, attention_mask=None):\n",
    "        if isinstance(input_ids, list) and isinstance(input_ids[0], str):\n",
    "            # assume input_ids is raw text prompts\n",
    "            tokens = self.tokenizer(input_ids, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = tokens[\"input_ids\"].to(self.device)\n",
    "            attention_mask = tokens[\"attention_mask\"].to(self.device)\n",
    "        else:\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            if attention_mask is None:\n",
    "                attention_mask = (input_ids != self.tokenizer.pad_token_id).long().to(self.device)\n",
    "            else:\n",
    "                attention_mask = attention_mask.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n",
    "        return embeddings, attention_mask\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        self.encoder.to(device)\n",
    "        return self\n",
    "\n",
    "text_encoder = T5EmbeddingEncoder().to(\"cuda\")\n",
    "text_emb =  text_encoder(input_ids_tsr[0:1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0060,  0.0064,  0.0045,  ...,  0.0049, -0.0029,  0.0041],\n",
       "         [-0.5352, -0.0132,  0.0356,  ...,  0.1289,  0.3086, -0.5703],\n",
       "         [-0.0815, -0.0069, -0.1289,  ..., -0.1123,  0.2930, -0.2500],\n",
       "         ...,\n",
       "         [-0.2441, -0.2676,  0.0574,  ...,  0.0977, -0.0337, -0.4316],\n",
       "         [-0.2559, -0.2969,  0.0396,  ...,  0.0957, -0.0234, -0.4414],\n",
       "         [-0.2754, -0.3301,  0.0544,  ...,  0.0811, -0.0155, -0.4355]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models//t5_ckpts/t5-v1_1-xxl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eaa9703f1e0413eafa63b05012ab144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/n/home12/hjkim/Github/DiffusionObjectRelation/PixArt-alpha\")\n",
    "from diffusion.model.t5 import T5Embedder\n",
    "\n",
    "\n",
    "pretrain_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/\"\n",
    "t5 = T5Embedder(device=\"cuda\", local_cache=True, cache_dir=f'{pretrain_path}/t5_ckpts', model_max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(tensor([[[-5.4688e-02, -4.1016e-02, -4.5410e-02,  ...,  7.7637e-02,\n",
      "          -3.3936e-02,  7.7148e-02],\n",
      "         [-2.0312e-01,  2.5146e-02, -1.5039e-01,  ..., -9.1797e-02,\n",
      "           9.0820e-02, -1.7773e-01],\n",
      "         [-4.5166e-03, -9.1553e-03,  4.0054e-04,  ..., -3.6926e-03,\n",
      "           6.5918e-03,  1.9150e-03],\n",
      "         ...,\n",
      "         [-7.5195e-02, -1.0559e-02, -9.3750e-02,  ...,  1.9336e-01,\n",
      "           8.4961e-02,  4.2725e-02],\n",
      "         [ 1.2988e-01,  1.0889e-01,  1.2988e-01,  ...,  3.3008e-01,\n",
      "           7.8125e-02,  1.7188e-01],\n",
      "         [ 1.2988e-01,  1.0889e-01,  1.2988e-01,  ...,  3.3008e-01,\n",
      "           7.8125e-02,  1.7188e-01]],\n",
      "\n",
      "        [[-5.8350e-02, -1.8433e-02, -4.5654e-02,  ...,  8.3496e-02,\n",
      "          -5.2979e-02,  4.2480e-02],\n",
      "         [-1.4551e-01,  1.9531e-02, -2.2168e-01,  ...,  5.0781e-02,\n",
      "           1.9922e-01, -1.6992e-01],\n",
      "         [-4.6082e-03, -8.4839e-03,  9.6893e-04,  ..., -3.5553e-03,\n",
      "           6.2866e-03,  2.5330e-03],\n",
      "         ...,\n",
      "         [ 8.9722e-03, -1.3428e-02, -4.1809e-03,  ...,  1.3965e-01,\n",
      "           2.3438e-02, -2.1240e-02],\n",
      "         [-1.9409e-02, -1.3574e-01, -1.7700e-02,  ...,  1.6504e-01,\n",
      "          -3.5400e-02, -9.9121e-02],\n",
      "         [-1.9409e-02, -1.3574e-01, -1.7700e-02,  ...,  1.6504e-01,\n",
      "          -3.5400e-02, -9.9121e-02]],\n",
      "\n",
      "        [[-6.4941e-02, -7.3242e-02, -5.8350e-02,  ...,  8.7402e-02,\n",
      "          -8.1543e-02,  8.9355e-02],\n",
      "         [-1.7871e-01,  2.1362e-02, -1.5820e-01,  ..., -1.4648e-01,\n",
      "           1.2109e-01, -1.5137e-01],\n",
      "         [-4.5166e-03, -8.3618e-03,  4.5395e-04,  ..., -2.8381e-03,\n",
      "           6.5918e-03,  2.4719e-03],\n",
      "         ...,\n",
      "         [ 1.5137e-01,  1.6235e-02, -2.1484e-01,  ...,  2.9297e-01,\n",
      "           1.0986e-01, -4.0771e-02],\n",
      "         [ 9.4238e-02,  6.2866e-03, -6.4941e-02,  ...,  3.2812e-01,\n",
      "           5.6641e-02,  1.4648e-02],\n",
      "         [ 9.4238e-02,  6.2866e-03, -6.4941e-02,  ...,  3.2812e-01,\n",
      "           5.6641e-02,  1.4648e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.9121e-02, -5.4443e-02, -7.1777e-02,  ...,  2.2339e-02,\n",
      "          -5.4688e-02,  7.6660e-02],\n",
      "         [-6.8359e-02, -2.1606e-02,  1.5488e-03,  ..., -1.0938e-01,\n",
      "           1.7578e-01, -1.7383e-01],\n",
      "         [-4.4250e-03, -7.1716e-03,  1.2970e-04,  ..., -2.9755e-03,\n",
      "           5.2490e-03,  2.1362e-03],\n",
      "         ...,\n",
      "         [-5.7373e-02, -1.0254e-01,  9.7656e-03,  ...,  3.4961e-01,\n",
      "          -9.0820e-02, -4.6875e-02],\n",
      "         [-9.9609e-02, -2.0752e-02,  1.2695e-01,  ...,  3.2227e-01,\n",
      "          -1.2988e-01,  7.8125e-02],\n",
      "         [-9.9609e-02, -2.0752e-02,  1.2695e-01,  ...,  3.2227e-01,\n",
      "          -1.2988e-01,  7.8125e-02]],\n",
      "\n",
      "        [[ 1.5015e-02, -8.0566e-03, -4.6631e-02,  ...,  4.7607e-02,\n",
      "          -2.9297e-02,  5.2490e-02],\n",
      "         [-1.4160e-01, -1.5320e-02, -1.6406e-01,  ..., -1.5625e-01,\n",
      "           1.6895e-01, -4.9805e-02],\n",
      "         [-4.4250e-03, -8.7891e-03,  3.6621e-04,  ..., -3.2196e-03,\n",
      "           6.2866e-03,  2.7313e-03],\n",
      "         ...,\n",
      "         [ 1.2988e-01,  2.4536e-02, -7.7820e-03,  ...,  6.9824e-02,\n",
      "          -1.2988e-01, -1.1133e-01],\n",
      "         [ 2.6172e-01,  2.2949e-02,  2.9907e-02,  ...,  2.1387e-01,\n",
      "           9.0942e-03, -7.9590e-02],\n",
      "         [ 2.6172e-01,  2.2949e-02,  2.9907e-02,  ...,  2.1387e-01,\n",
      "           9.0942e-03, -7.9590e-02]],\n",
      "\n",
      "        [[-5.1270e-02, -7.9102e-02, -3.6133e-02,  ...,  7.9102e-02,\n",
      "          -8.5938e-02,  8.1055e-02],\n",
      "         [-1.8359e-01,  3.5400e-02, -1.9336e-01,  ...,  4.3457e-02,\n",
      "           2.4805e-01, -2.9102e-01],\n",
      "         [-4.3030e-03, -8.3618e-03,  3.6430e-04,  ..., -3.9978e-03,\n",
      "           7.0190e-03,  2.6093e-03],\n",
      "         ...,\n",
      "         [-1.3184e-02,  7.7637e-02, -3.2617e-01,  ...,  2.4512e-01,\n",
      "          -4.9561e-02, -6.1279e-02],\n",
      "         [ 6.2256e-03,  2.2070e-01, -2.3730e-01,  ...,  2.1582e-01,\n",
      "           1.4832e-02, -3.3203e-02],\n",
      "         [ 6.2256e-03,  2.2070e-01, -2.3730e-01,  ...,  2.1582e-01,\n",
      "           1.4832e-02, -3.3203e-02]]], device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "1\n",
      "(tensor([[[-1.5259e-02, -6.5430e-02, -4.2969e-02,  ...,  9.7168e-02,\n",
      "          -9.5703e-02,  1.2012e-01],\n",
      "         [-2.2754e-01, -6.5613e-04, -1.4941e-01,  ..., -8.3984e-02,\n",
      "           1.9531e-01, -9.1309e-02],\n",
      "         [-4.2419e-03, -7.9346e-03,  7.3242e-04,  ..., -3.7079e-03,\n",
      "           7.1106e-03,  2.5330e-03],\n",
      "         ...,\n",
      "         [-5.0049e-02,  5.6152e-02, -1.9824e-01,  ...,  2.5586e-01,\n",
      "          -1.1377e-01, -3.4180e-02],\n",
      "         [ 9.1797e-02,  3.3936e-02, -9.8633e-02,  ...,  2.3145e-01,\n",
      "          -1.1749e-03, -2.2339e-02],\n",
      "         [ 9.1797e-02,  3.3936e-02, -9.8633e-02,  ...,  2.3145e-01,\n",
      "          -1.1749e-03, -2.2339e-02]],\n",
      "\n",
      "        [[ 1.4648e-02, -7.1716e-03, -4.4922e-02,  ...,  5.1025e-02,\n",
      "          -2.9419e-02,  5.4443e-02],\n",
      "         [-1.4258e-01, -1.6113e-02, -1.6504e-01,  ..., -1.5527e-01,\n",
      "           1.6797e-01, -4.8096e-02],\n",
      "         [-4.4556e-03, -8.8501e-03,  3.6240e-04,  ..., -3.2196e-03,\n",
      "           6.2561e-03,  2.7161e-03],\n",
      "         ...,\n",
      "         [ 1.2500e-01,  1.5503e-02, -1.6602e-02,  ...,  7.7148e-02,\n",
      "          -1.3281e-01, -1.1230e-01],\n",
      "         [ 2.7539e-01, -3.6469e-03,  2.6978e-02,  ...,  2.1191e-01,\n",
      "           5.0354e-03, -7.3242e-02],\n",
      "         [ 2.7539e-01, -3.6469e-03,  2.6978e-02,  ...,  2.1191e-01,\n",
      "           5.0354e-03, -7.3242e-02]],\n",
      "\n",
      "        [[-5.4199e-02, -9.1797e-02, -5.3223e-02,  ...,  1.4160e-01,\n",
      "          -9.1797e-02,  8.2520e-02],\n",
      "         [-1.7285e-01,  5.2246e-02, -9.7168e-02,  ...,  5.6885e-02,\n",
      "           1.2695e-01, -1.8652e-01],\n",
      "         [-4.3030e-03, -8.4839e-03,  1.2207e-03,  ..., -3.3417e-03,\n",
      "           6.7749e-03,  2.7618e-03],\n",
      "         ...,\n",
      "         [ 2.5195e-01,  4.5898e-02, -1.8945e-01,  ...,  2.4805e-01,\n",
      "           6.3477e-02, -3.0640e-02],\n",
      "         [ 2.4609e-01,  1.7383e-01, -7.5378e-03,  ...,  3.1836e-01,\n",
      "          -3.9551e-02, -1.6211e-01],\n",
      "         [ 2.4609e-01,  1.7383e-01, -7.5378e-03,  ...,  3.1836e-01,\n",
      "          -3.9551e-02, -1.6211e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0107e-01, -5.2979e-02, -7.1289e-02,  ...,  2.1240e-02,\n",
      "          -5.1758e-02,  7.6660e-02],\n",
      "         [-6.8359e-02, -1.9409e-02, -8.8120e-04,  ..., -1.1230e-01,\n",
      "           1.7773e-01, -1.7383e-01],\n",
      "         [-4.4556e-03, -7.2327e-03,  1.2875e-04,  ..., -2.8839e-03,\n",
      "           5.1880e-03,  2.1210e-03],\n",
      "         ...,\n",
      "         [-6.2500e-02, -8.8867e-02,  4.0588e-03,  ...,  3.4570e-01,\n",
      "          -8.1055e-02, -5.2002e-02],\n",
      "         [-1.1523e-01, -2.1240e-02,  1.1768e-01,  ...,  3.3008e-01,\n",
      "          -1.2402e-01,  7.1289e-02],\n",
      "         [-1.1523e-01, -2.1240e-02,  1.1768e-01,  ...,  3.3008e-01,\n",
      "          -1.2402e-01,  7.1289e-02]],\n",
      "\n",
      "        [[ 1.4648e-02, -7.1716e-03, -4.4922e-02,  ...,  5.1025e-02,\n",
      "          -2.9419e-02,  5.4443e-02],\n",
      "         [-1.4258e-01, -1.6113e-02, -1.6504e-01,  ..., -1.5527e-01,\n",
      "           1.6797e-01, -4.8096e-02],\n",
      "         [-4.4556e-03, -8.8501e-03,  3.6240e-04,  ..., -3.2196e-03,\n",
      "           6.2561e-03,  2.7161e-03],\n",
      "         ...,\n",
      "         [ 1.2500e-01,  1.5503e-02, -1.6602e-02,  ...,  7.7148e-02,\n",
      "          -1.3281e-01, -1.1230e-01],\n",
      "         [ 2.7539e-01, -3.6469e-03,  2.6978e-02,  ...,  2.1191e-01,\n",
      "           5.0354e-03, -7.3242e-02],\n",
      "         [ 2.7539e-01, -3.6469e-03,  2.6978e-02,  ...,  2.1191e-01,\n",
      "           5.0354e-03, -7.3242e-02]],\n",
      "\n",
      "        [[-5.2979e-02, -7.9102e-02, -3.6133e-02,  ...,  7.7148e-02,\n",
      "          -8.5449e-02,  8.2031e-02],\n",
      "         [-1.8359e-01,  3.4424e-02, -1.9238e-01,  ...,  4.3457e-02,\n",
      "           2.4512e-01, -2.9102e-01],\n",
      "         [-4.3335e-03, -8.3008e-03,  3.6621e-04,  ..., -3.9978e-03,\n",
      "           7.0190e-03,  2.6398e-03],\n",
      "         ...,\n",
      "         [-1.2756e-02,  7.6660e-02, -3.2617e-01,  ...,  2.4219e-01,\n",
      "          -5.0781e-02, -6.3965e-02],\n",
      "         [ 6.5918e-03,  2.2461e-01, -2.3535e-01,  ...,  2.1680e-01,\n",
      "           1.0254e-02, -3.4180e-02],\n",
      "         [ 6.5918e-03,  2.2461e-01, -2.3535e-01,  ...,  2.1680e-01,\n",
      "           1.0254e-02, -3.4180e-02]]], device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "2\n",
      "(tensor([[[-0.0569, -0.0181, -0.0481,  ...,  0.0811, -0.0503,  0.0415],\n",
      "         [-0.1465,  0.0172, -0.2207,  ...,  0.0500,  0.2012, -0.1660],\n",
      "         [-0.0047, -0.0085,  0.0010,  ..., -0.0036,  0.0063,  0.0025],\n",
      "         ...,\n",
      "         [ 0.0088, -0.0126, -0.0093,  ...,  0.1367,  0.0131, -0.0139],\n",
      "         [-0.0234, -0.1455, -0.0129,  ...,  0.1631, -0.0449, -0.1006],\n",
      "         [-0.0234, -0.1455, -0.0129,  ...,  0.1631, -0.0449, -0.1006]],\n",
      "\n",
      "        [[-0.0542, -0.0776, -0.0356,  ...,  0.0776, -0.0859,  0.0815],\n",
      "         [-0.1855,  0.0327, -0.1934,  ...,  0.0437,  0.2480, -0.2910],\n",
      "         [-0.0043, -0.0083,  0.0004,  ..., -0.0040,  0.0070,  0.0027],\n",
      "         ...,\n",
      "         [-0.0129,  0.0708, -0.3301,  ...,  0.2471, -0.0508, -0.0664],\n",
      "         [ 0.0075,  0.2197, -0.2412,  ...,  0.2178,  0.0150, -0.0344],\n",
      "         [ 0.0075,  0.2197, -0.2412,  ...,  0.2178,  0.0150, -0.0344]],\n",
      "\n",
      "        [[-0.0320, -0.0811, -0.0282,  ...,  0.0311, -0.0649,  0.0879],\n",
      "         [-0.2295, -0.0210, -0.1816,  ..., -0.0645,  0.2109, -0.0908],\n",
      "         [-0.0044, -0.0092,  0.0005,  ..., -0.0036,  0.0070,  0.0025],\n",
      "         ...,\n",
      "         [-0.1680,  0.1299, -0.0835,  ...,  0.0913, -0.0481, -0.0635],\n",
      "         [-0.3555,  0.1572, -0.0762,  ...,  0.0527, -0.1035, -0.1040],\n",
      "         [-0.3555,  0.1572, -0.0762,  ...,  0.0527, -0.1035, -0.1040]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0415, -0.0703, -0.0845,  ...,  0.0588, -0.0894,  0.0464],\n",
      "         [-0.1211, -0.0317,  0.1230,  ...,  0.1064,  0.0674, -0.2871],\n",
      "         [-0.0035, -0.0074, -0.0005,  ..., -0.0034,  0.0066,  0.0015],\n",
      "         ...,\n",
      "         [ 0.0767, -0.0820, -0.3262,  ...,  0.1387,  0.0045,  0.2852],\n",
      "         [-0.0481, -0.0054, -0.1992,  ...,  0.2715, -0.0140,  0.2256],\n",
      "         [-0.0481, -0.0054, -0.1992,  ...,  0.2715, -0.0140,  0.2256]],\n",
      "\n",
      "        [[-0.0569, -0.0181, -0.0481,  ...,  0.0811, -0.0503,  0.0415],\n",
      "         [-0.1465,  0.0172, -0.2207,  ...,  0.0500,  0.2012, -0.1660],\n",
      "         [-0.0047, -0.0085,  0.0010,  ..., -0.0036,  0.0063,  0.0025],\n",
      "         ...,\n",
      "         [ 0.0088, -0.0126, -0.0093,  ...,  0.1367,  0.0131, -0.0139],\n",
      "         [-0.0234, -0.1455, -0.0129,  ...,  0.1631, -0.0449, -0.1006],\n",
      "         [-0.0234, -0.1455, -0.0129,  ...,  0.1631, -0.0449, -0.1006]],\n",
      "\n",
      "        [[-0.0542, -0.0776, -0.0356,  ...,  0.0776, -0.0859,  0.0815],\n",
      "         [-0.1855,  0.0327, -0.1934,  ...,  0.0437,  0.2480, -0.2910],\n",
      "         [-0.0043, -0.0083,  0.0004,  ..., -0.0040,  0.0070,  0.0027],\n",
      "         ...,\n",
      "         [-0.0129,  0.0708, -0.3301,  ...,  0.2471, -0.0508, -0.0664],\n",
      "         [ 0.0075,  0.2197, -0.2412,  ...,  0.2178,  0.0150, -0.0344],\n",
      "         [ 0.0075,  0.2197, -0.2412,  ...,  0.2178,  0.0150, -0.0344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "3\n",
      "(tensor([[[-7.2754e-02, -1.3794e-02, -5.6396e-02,  ...,  6.4941e-02,\n",
      "          -7.8613e-02,  1.1865e-01],\n",
      "         [-1.1914e-01, -3.6377e-02,  2.2461e-02,  ..., -1.0938e-01,\n",
      "           1.8359e-01, -2.2461e-01],\n",
      "         [-5.7068e-03, -8.3618e-03, -6.9141e-05,  ..., -3.7079e-03,\n",
      "           6.0120e-03,  2.4567e-03],\n",
      "         ...,\n",
      "         [ 2.5146e-02, -3.8757e-03, -1.8164e-01,  ...,  2.1094e-01,\n",
      "          -9.1309e-02, -4.2969e-02],\n",
      "         [-8.7891e-02,  1.2158e-01, -1.7480e-01,  ...,  8.5938e-02,\n",
      "          -4.1016e-02, -1.0620e-02],\n",
      "         [-8.7891e-02,  1.2158e-01, -1.7480e-01,  ...,  8.5938e-02,\n",
      "          -4.1016e-02, -1.0620e-02]],\n",
      "\n",
      "        [[-1.0156e-01, -6.5430e-02, -2.7954e-02,  ...,  8.9844e-02,\n",
      "          -6.7383e-02,  1.7676e-01],\n",
      "         [-1.3477e-01,  1.3086e-01, -6.1035e-02,  ...,  8.3008e-02,\n",
      "           8.1543e-02, -9.9121e-02],\n",
      "         [-4.1809e-03, -7.8125e-03,  1.0147e-03,  ..., -4.7302e-03,\n",
      "           5.4626e-03,  2.3499e-03],\n",
      "         ...,\n",
      "         [ 1.8164e-01, -7.5195e-02, -1.1963e-01,  ...,  1.7188e-01,\n",
      "           2.2217e-02,  8.2520e-02],\n",
      "         [ 5.1025e-02,  4.1992e-02,  1.8799e-02,  ...,  2.0215e-01,\n",
      "          -5.5908e-02, -9.0820e-02],\n",
      "         [ 5.1025e-02,  4.1992e-02,  1.8799e-02,  ...,  2.0215e-01,\n",
      "          -5.5908e-02, -9.0820e-02]],\n",
      "\n",
      "        [[-5.2979e-02, -9.2773e-02, -5.4443e-02,  ...,  1.4355e-01,\n",
      "          -9.0820e-02,  8.1543e-02],\n",
      "         [-1.7285e-01,  5.3223e-02, -9.7656e-02,  ...,  5.8350e-02,\n",
      "           1.2988e-01, -1.8750e-01],\n",
      "         [-4.3030e-03, -8.4839e-03,  1.2207e-03,  ..., -3.3417e-03,\n",
      "           6.8359e-03,  2.7161e-03],\n",
      "         ...,\n",
      "         [ 2.5195e-01,  4.8584e-02, -1.9141e-01,  ...,  2.5000e-01,\n",
      "           6.2012e-02, -2.8198e-02],\n",
      "         [ 2.3926e-01,  1.7578e-01, -5.7373e-03,  ...,  3.2227e-01,\n",
      "          -3.1250e-02, -1.6797e-01],\n",
      "         [ 2.3926e-01,  1.7578e-01, -5.7373e-03,  ...,  3.2227e-01,\n",
      "          -3.1250e-02, -1.6797e-01]],\n",
      "\n",
      "        [[-4.0283e-02, -7.1289e-02, -8.3496e-02,  ...,  5.8838e-02,\n",
      "          -8.8867e-02,  4.6631e-02],\n",
      "         [-1.1719e-01, -3.0396e-02,  1.2061e-01,  ...,  1.0645e-01,\n",
      "           6.7383e-02, -2.8711e-01],\n",
      "         [-3.4943e-03, -7.3242e-03, -5.5313e-04,  ..., -3.4027e-03,\n",
      "           6.5918e-03,  1.4801e-03],\n",
      "         ...,\n",
      "         [ 8.4473e-02, -8.3984e-02, -3.2812e-01,  ...,  1.5723e-01,\n",
      "           0.0000e+00,  2.8125e-01],\n",
      "         [-3.8086e-02, -4.2114e-03, -1.9531e-01,  ...,  2.7148e-01,\n",
      "          -1.3977e-02,  2.1484e-01],\n",
      "         [-3.8086e-02, -4.2114e-03, -1.9531e-01,  ...,  2.7148e-01,\n",
      "          -1.3977e-02,  2.1484e-01]],\n",
      "\n",
      "        [[-5.6396e-02, -1.9287e-02, -4.5410e-02,  ...,  8.3008e-02,\n",
      "          -5.1758e-02,  4.2969e-02],\n",
      "         [-1.4258e-01,  1.9043e-02, -2.2070e-01,  ...,  4.8340e-02,\n",
      "           1.9922e-01, -1.6992e-01],\n",
      "         [-4.6692e-03, -8.4839e-03,  9.8419e-04,  ..., -3.5858e-03,\n",
      "           6.2866e-03,  2.5024e-03],\n",
      "         ...,\n",
      "         [ 6.7444e-03, -1.1719e-02, -1.1230e-02,  ...,  1.3574e-01,\n",
      "           1.7090e-02, -1.1475e-02],\n",
      "         [-1.7944e-02, -1.3867e-01, -1.5503e-02,  ...,  1.6406e-01,\n",
      "          -3.9307e-02, -9.7168e-02],\n",
      "         [-1.7944e-02, -1.3867e-01, -1.5503e-02,  ...,  1.6406e-01,\n",
      "          -3.9307e-02, -9.7168e-02]],\n",
      "\n",
      "        [[-5.2246e-02, -7.8125e-02, -3.4912e-02,  ...,  7.8613e-02,\n",
      "          -8.5938e-02,  8.1543e-02],\n",
      "         [-1.8555e-01,  3.4424e-02, -1.9531e-01,  ...,  4.5654e-02,\n",
      "           2.5391e-01, -2.8906e-01],\n",
      "         [-4.3335e-03, -8.3008e-03,  3.8910e-04,  ..., -3.9978e-03,\n",
      "           7.0190e-03,  2.6703e-03],\n",
      "         ...,\n",
      "         [-1.3306e-02,  7.1777e-02, -3.2812e-01,  ...,  2.3340e-01,\n",
      "          -4.9316e-02, -6.4941e-02],\n",
      "         [ 8.0566e-03,  2.2070e-01, -2.4219e-01,  ...,  2.1094e-01,\n",
      "           1.0681e-02, -3.2715e-02],\n",
      "         [ 8.0566e-03,  2.2070e-01, -2.4219e-01,  ...,  2.1094e-01,\n",
      "           1.0681e-02, -3.2715e-02]]], device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "4\n",
      "(tensor([[[-5.2734e-02, -2.8931e-02, -4.5166e-02,  ...,  5.1270e-02,\n",
      "          -1.0986e-01,  1.4160e-01],\n",
      "         [-1.8457e-01,  3.8818e-02, -6.6895e-02,  ..., -5.4199e-02,\n",
      "           7.1289e-02, -1.4258e-01],\n",
      "         [-4.3945e-03, -7.7209e-03,  4.3678e-04,  ..., -4.1809e-03,\n",
      "           6.8359e-03,  2.4567e-03],\n",
      "         ...,\n",
      "         [ 1.5015e-02, -1.7578e-02, -8.1543e-02,  ...,  3.6719e-01,\n",
      "          -1.2354e-01, -5.5664e-02],\n",
      "         [-4.0527e-02,  6.3782e-03, -6.9824e-02,  ...,  3.4766e-01,\n",
      "          -2.0312e-01, -8.5938e-02],\n",
      "         [-4.0527e-02,  6.3782e-03, -6.9824e-02,  ...,  3.4766e-01,\n",
      "          -2.0312e-01, -8.5938e-02]],\n",
      "\n",
      "        [[-6.2988e-02, -7.4219e-02, -5.8838e-02,  ...,  8.8867e-02,\n",
      "          -8.1543e-02,  9.0332e-02],\n",
      "         [-1.7969e-01,  2.0142e-02, -1.5918e-01,  ..., -1.4648e-01,\n",
      "           1.2158e-01, -1.4941e-01],\n",
      "         [-4.5166e-03, -8.3618e-03,  4.5586e-04,  ..., -2.8229e-03,\n",
      "           6.5918e-03,  2.4567e-03],\n",
      "         ...,\n",
      "         [ 1.5234e-01,  1.7700e-02, -2.1777e-01,  ...,  2.9492e-01,\n",
      "           1.1230e-01, -3.6865e-02],\n",
      "         [ 9.1797e-02,  9.7046e-03, -5.6396e-02,  ...,  3.3008e-01,\n",
      "           5.7373e-02,  1.7090e-02],\n",
      "         [ 9.1797e-02,  9.7046e-03, -5.6396e-02,  ...,  3.3008e-01,\n",
      "           5.7373e-02,  1.7090e-02]],\n",
      "\n",
      "        [[-5.6396e-02, -1.9287e-02, -4.5410e-02,  ...,  8.3008e-02,\n",
      "          -5.1758e-02,  4.2969e-02],\n",
      "         [-1.4258e-01,  1.9043e-02, -2.2070e-01,  ...,  4.8340e-02,\n",
      "           1.9922e-01, -1.6992e-01],\n",
      "         [-4.6692e-03, -8.4839e-03,  9.8419e-04,  ..., -3.5858e-03,\n",
      "           6.2866e-03,  2.5024e-03],\n",
      "         ...,\n",
      "         [ 6.7444e-03, -1.1719e-02, -1.1230e-02,  ...,  1.3574e-01,\n",
      "           1.7090e-02, -1.1475e-02],\n",
      "         [-1.7944e-02, -1.3867e-01, -1.5503e-02,  ...,  1.6406e-01,\n",
      "          -3.9307e-02, -9.7168e-02],\n",
      "         [-1.7944e-02, -1.3867e-01, -1.5503e-02,  ...,  1.6406e-01,\n",
      "          -3.9307e-02, -9.7168e-02]],\n",
      "\n",
      "        [[-5.2734e-02, -2.8931e-02, -4.5166e-02,  ...,  5.1270e-02,\n",
      "          -1.0986e-01,  1.4160e-01],\n",
      "         [-1.8457e-01,  3.8818e-02, -6.6895e-02,  ..., -5.4199e-02,\n",
      "           7.1289e-02, -1.4258e-01],\n",
      "         [-4.3945e-03, -7.7209e-03,  4.3678e-04,  ..., -4.1809e-03,\n",
      "           6.8359e-03,  2.4567e-03],\n",
      "         ...,\n",
      "         [ 1.5015e-02, -1.7578e-02, -8.1543e-02,  ...,  3.6719e-01,\n",
      "          -1.2354e-01, -5.5664e-02],\n",
      "         [-4.0527e-02,  6.3782e-03, -6.9824e-02,  ...,  3.4766e-01,\n",
      "          -2.0312e-01, -8.5938e-02],\n",
      "         [-4.0527e-02,  6.3782e-03, -6.9824e-02,  ...,  3.4766e-01,\n",
      "          -2.0312e-01, -8.5938e-02]],\n",
      "\n",
      "        [[ 1.3062e-02, -1.0376e-02, -4.3945e-02,  ...,  5.0537e-02,\n",
      "          -3.0640e-02,  5.4443e-02],\n",
      "         [-1.4355e-01, -1.6235e-02, -1.6602e-01,  ..., -1.5723e-01,\n",
      "           1.6895e-01, -4.9805e-02],\n",
      "         [-4.4556e-03, -8.7891e-03,  3.3569e-04,  ..., -3.2501e-03,\n",
      "           6.2866e-03,  2.7161e-03],\n",
      "         ...,\n",
      "         [ 1.2402e-01,  1.4771e-02, -1.5503e-02,  ...,  7.5195e-02,\n",
      "          -1.3379e-01, -1.1523e-01],\n",
      "         [ 2.7344e-01,  6.0425e-03,  2.8809e-02,  ...,  2.1680e-01,\n",
      "           1.0132e-02, -9.0332e-02],\n",
      "         [ 2.7344e-01,  6.0425e-03,  2.8809e-02,  ...,  2.1680e-01,\n",
      "           1.0132e-02, -9.0332e-02]],\n",
      "\n",
      "        [[-5.2246e-02, -7.8125e-02, -3.4912e-02,  ...,  7.8613e-02,\n",
      "          -8.5938e-02,  8.1543e-02],\n",
      "         [-1.8555e-01,  3.4424e-02, -1.9531e-01,  ...,  4.5654e-02,\n",
      "           2.5391e-01, -2.8906e-01],\n",
      "         [-4.3335e-03, -8.3008e-03,  3.8910e-04,  ..., -3.9978e-03,\n",
      "           7.0190e-03,  2.6703e-03],\n",
      "         ...,\n",
      "         [-1.3306e-02,  7.1777e-02, -3.2812e-01,  ...,  2.3340e-01,\n",
      "          -4.9316e-02, -6.4941e-02],\n",
      "         [ 8.0566e-03,  2.2070e-01, -2.4219e-01,  ...,  2.1094e-01,\n",
      "           1.0681e-02, -3.2715e-02],\n",
      "         [ 8.0566e-03,  2.2070e-01, -2.4219e-01,  ...,  2.1094e-01,\n",
      "           1.0681e-02, -3.2715e-02]]], device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "5\n",
      "(tensor([[[-0.0162, -0.0654, -0.0422,  ...,  0.0967, -0.0938,  0.1201],\n",
      "         [-0.2275, -0.0007, -0.1494,  ..., -0.0835,  0.1953, -0.0908],\n",
      "         [-0.0042, -0.0079,  0.0008,  ..., -0.0037,  0.0071,  0.0025],\n",
      "         ...,\n",
      "         [-0.0459,  0.0486, -0.1982,  ...,  0.2559, -0.1216, -0.0303],\n",
      "         [ 0.0923,  0.0393, -0.0986,  ...,  0.2256, -0.0067, -0.0166],\n",
      "         [ 0.0923,  0.0393, -0.0986,  ...,  0.2256, -0.0067, -0.0166]],\n",
      "\n",
      "        [[ 0.0138, -0.0103, -0.0452,  ...,  0.0515, -0.0317,  0.0557],\n",
      "         [-0.1426, -0.0159, -0.1641,  ..., -0.1582,  0.1689, -0.0488],\n",
      "         [-0.0045, -0.0088,  0.0004,  ..., -0.0033,  0.0063,  0.0028],\n",
      "         ...,\n",
      "         [ 0.1187,  0.0148, -0.0162,  ...,  0.0698, -0.1328, -0.1152],\n",
      "         [ 0.2695,  0.0087,  0.0315,  ...,  0.2148,  0.0064, -0.0825],\n",
      "         [ 0.2695,  0.0087,  0.0315,  ...,  0.2148,  0.0064, -0.0825]],\n",
      "\n",
      "        [[-0.0522, -0.0908, -0.0532,  ...,  0.1416, -0.0908,  0.0806],\n",
      "         [-0.1738,  0.0518, -0.0991,  ...,  0.0588,  0.1279, -0.1865],\n",
      "         [-0.0043, -0.0084,  0.0013,  ..., -0.0033,  0.0068,  0.0027],\n",
      "         ...,\n",
      "         [ 0.2441,  0.0505, -0.1924,  ...,  0.2490,  0.0605, -0.0282],\n",
      "         [ 0.2383,  0.1719, -0.0043,  ...,  0.3164, -0.0332, -0.1719],\n",
      "         [ 0.2383,  0.1719, -0.0043,  ...,  0.3164, -0.0332, -0.1719]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0569, -0.0181, -0.0481,  ...,  0.0811, -0.0503,  0.0415],\n",
      "         [-0.1465,  0.0172, -0.2207,  ...,  0.0500,  0.2012, -0.1660],\n",
      "         [-0.0047, -0.0085,  0.0010,  ..., -0.0036,  0.0063,  0.0025],\n",
      "         ...,\n",
      "         [ 0.0088, -0.0126, -0.0093,  ...,  0.1367,  0.0131, -0.0139],\n",
      "         [-0.0234, -0.1455, -0.0129,  ...,  0.1631, -0.0449, -0.1006],\n",
      "         [-0.0234, -0.1455, -0.0129,  ...,  0.1631, -0.0449, -0.1006]],\n",
      "\n",
      "        [[ 0.0138, -0.0103, -0.0452,  ...,  0.0515, -0.0317,  0.0557],\n",
      "         [-0.1426, -0.0159, -0.1641,  ..., -0.1582,  0.1689, -0.0488],\n",
      "         [-0.0045, -0.0088,  0.0004,  ..., -0.0033,  0.0063,  0.0028],\n",
      "         ...,\n",
      "         [ 0.1187,  0.0148, -0.0162,  ...,  0.0698, -0.1328, -0.1152],\n",
      "         [ 0.2695,  0.0087,  0.0315,  ...,  0.2148,  0.0064, -0.0825],\n",
      "         [ 0.2695,  0.0087,  0.0315,  ...,  0.2148,  0.0064, -0.0825]],\n",
      "\n",
      "        [[-0.0542, -0.0776, -0.0356,  ...,  0.0776, -0.0859,  0.0815],\n",
      "         [-0.1855,  0.0327, -0.1934,  ...,  0.0437,  0.2480, -0.2910],\n",
      "         [-0.0043, -0.0083,  0.0004,  ..., -0.0040,  0.0070,  0.0027],\n",
      "         ...,\n",
      "         [-0.0129,  0.0708, -0.3301,  ...,  0.2471, -0.0508, -0.0664],\n",
      "         [ 0.0075,  0.2197, -0.2412,  ...,  0.2178,  0.0150, -0.0344],\n",
      "         [ 0.0075,  0.2197, -0.2412,  ...,  0.2178,  0.0150, -0.0344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "6\n",
      "(tensor([[[-0.0537, -0.0417, -0.0479,  ...,  0.0811, -0.0342,  0.0771],\n",
      "         [-0.2061,  0.0267, -0.1523,  ..., -0.0908,  0.0894, -0.1777],\n",
      "         [-0.0045, -0.0093,  0.0004,  ..., -0.0036,  0.0066,  0.0019],\n",
      "         ...,\n",
      "         [-0.0781, -0.0135, -0.0962,  ...,  0.1914,  0.0845,  0.0410],\n",
      "         [ 0.1260,  0.1079,  0.1387,  ...,  0.3262,  0.0762,  0.1680],\n",
      "         [ 0.1260,  0.1079,  0.1387,  ...,  0.3262,  0.0762,  0.1680]],\n",
      "\n",
      "        [[-0.0554, -0.0186, -0.0491,  ...,  0.0825, -0.0510,  0.0427],\n",
      "         [-0.1445,  0.0187, -0.2197,  ...,  0.0493,  0.2002, -0.1670],\n",
      "         [-0.0047, -0.0084,  0.0010,  ..., -0.0036,  0.0063,  0.0026],\n",
      "         ...,\n",
      "         [ 0.0107, -0.0160, -0.0095,  ...,  0.1348,  0.0161, -0.0123],\n",
      "         [-0.0159, -0.1299, -0.0134,  ...,  0.1729, -0.0437, -0.1021],\n",
      "         [-0.0159, -0.1299, -0.0134,  ...,  0.1729, -0.0437, -0.1021]],\n",
      "\n",
      "        [[-0.0654, -0.0742, -0.0583,  ...,  0.0869, -0.0796,  0.0894],\n",
      "         [-0.1807,  0.0203, -0.1562,  ..., -0.1465,  0.1201, -0.1514],\n",
      "         [-0.0046, -0.0084,  0.0005,  ..., -0.0029,  0.0066,  0.0025],\n",
      "         ...,\n",
      "         [ 0.1504,  0.0129, -0.2168,  ...,  0.2949,  0.1094, -0.0396],\n",
      "         [ 0.0854,  0.0046, -0.0593,  ...,  0.3301,  0.0530,  0.0168],\n",
      "         [ 0.0854,  0.0046, -0.0593,  ...,  0.3301,  0.0530,  0.0168]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0408, -0.0718, -0.0845,  ...,  0.0601, -0.0889,  0.0461],\n",
      "         [-0.1196, -0.0322,  0.1221,  ...,  0.1094,  0.0679, -0.2852],\n",
      "         [-0.0035, -0.0074, -0.0006,  ..., -0.0034,  0.0066,  0.0015],\n",
      "         ...,\n",
      "         [ 0.0830, -0.0786, -0.3301,  ...,  0.1465, -0.0015,  0.2910],\n",
      "         [-0.0452, -0.0044, -0.1943,  ...,  0.2734, -0.0082,  0.2158],\n",
      "         [-0.0452, -0.0044, -0.1943,  ...,  0.2734, -0.0082,  0.2158]],\n",
      "\n",
      "        [[-0.0554, -0.0186, -0.0491,  ...,  0.0825, -0.0510,  0.0427],\n",
      "         [-0.1445,  0.0187, -0.2197,  ...,  0.0493,  0.2002, -0.1670],\n",
      "         [-0.0047, -0.0084,  0.0010,  ..., -0.0036,  0.0063,  0.0026],\n",
      "         ...,\n",
      "         [ 0.0107, -0.0160, -0.0095,  ...,  0.1348,  0.0161, -0.0123],\n",
      "         [-0.0159, -0.1299, -0.0134,  ...,  0.1729, -0.0437, -0.1021],\n",
      "         [-0.0159, -0.1299, -0.0134,  ...,  0.1729, -0.0437, -0.1021]],\n",
      "\n",
      "        [[-0.0530, -0.0791, -0.0361,  ...,  0.0771, -0.0854,  0.0820],\n",
      "         [-0.1836,  0.0344, -0.1924,  ...,  0.0435,  0.2451, -0.2910],\n",
      "         [-0.0043, -0.0083,  0.0004,  ..., -0.0040,  0.0070,  0.0026],\n",
      "         ...,\n",
      "         [-0.0128,  0.0767, -0.3262,  ...,  0.2422, -0.0508, -0.0640],\n",
      "         [ 0.0066,  0.2246, -0.2354,  ...,  0.2168,  0.0103, -0.0342],\n",
      "         [ 0.0066,  0.2246, -0.2354,  ...,  0.2168,  0.0103, -0.0342]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "7\n",
      "(tensor([[[-0.0527, -0.0430, -0.0469,  ...,  0.0796, -0.0347,  0.0786],\n",
      "         [-0.2051,  0.0275, -0.1514,  ..., -0.0928,  0.0894, -0.1777],\n",
      "         [-0.0045, -0.0093,  0.0004,  ..., -0.0037,  0.0066,  0.0019],\n",
      "         ...,\n",
      "         [-0.0771, -0.0109, -0.0972,  ...,  0.1934,  0.0815,  0.0435],\n",
      "         [ 0.1289,  0.1147,  0.1309,  ...,  0.3359,  0.0762,  0.1670],\n",
      "         [ 0.1289,  0.1147,  0.1309,  ...,  0.3359,  0.0762,  0.1670]],\n",
      "\n",
      "        [[-0.0564, -0.0186, -0.0486,  ...,  0.0840, -0.0503,  0.0410],\n",
      "         [-0.1436,  0.0181, -0.2236,  ...,  0.0481,  0.1992, -0.1660],\n",
      "         [-0.0047, -0.0085,  0.0010,  ..., -0.0036,  0.0063,  0.0025],\n",
      "         ...,\n",
      "         [ 0.0120, -0.0153, -0.0115,  ...,  0.1406,  0.0188, -0.0205],\n",
      "         [-0.0282, -0.1357, -0.0095,  ...,  0.1699, -0.0422, -0.1079],\n",
      "         [-0.0282, -0.1357, -0.0095,  ...,  0.1699, -0.0422, -0.1079]],\n",
      "\n",
      "        [[-0.0645, -0.0732, -0.0603,  ...,  0.0869, -0.0820,  0.0908],\n",
      "         [-0.1807,  0.0179, -0.1562,  ..., -0.1445,  0.1211, -0.1543],\n",
      "         [-0.0046, -0.0084,  0.0005,  ..., -0.0028,  0.0066,  0.0024],\n",
      "         ...,\n",
      "         [ 0.1553,  0.0178, -0.2139,  ...,  0.3008,  0.1055, -0.0354],\n",
      "         [ 0.0942,  0.0084, -0.0605,  ...,  0.3340,  0.0508,  0.0173],\n",
      "         [ 0.0942,  0.0084, -0.0605,  ...,  0.3340,  0.0508,  0.0173]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0405, -0.0718, -0.0840,  ...,  0.0608, -0.0889,  0.0466],\n",
      "         [-0.1196, -0.0310,  0.1211,  ...,  0.1055,  0.0674, -0.2852],\n",
      "         [-0.0035, -0.0073, -0.0005,  ..., -0.0034,  0.0066,  0.0015],\n",
      "         ...,\n",
      "         [ 0.0757, -0.0767, -0.3301,  ...,  0.1387, -0.0045,  0.2891],\n",
      "         [-0.0410, -0.0042, -0.1924,  ...,  0.2715, -0.0145,  0.2275],\n",
      "         [-0.0410, -0.0042, -0.1924,  ...,  0.2715, -0.0145,  0.2275]],\n",
      "\n",
      "        [[-0.0564, -0.0186, -0.0486,  ...,  0.0840, -0.0503,  0.0410],\n",
      "         [-0.1436,  0.0181, -0.2236,  ...,  0.0481,  0.1992, -0.1660],\n",
      "         [-0.0047, -0.0085,  0.0010,  ..., -0.0036,  0.0063,  0.0025],\n",
      "         ...,\n",
      "         [ 0.0120, -0.0153, -0.0115,  ...,  0.1406,  0.0188, -0.0205],\n",
      "         [-0.0282, -0.1357, -0.0095,  ...,  0.1699, -0.0422, -0.1079],\n",
      "         [-0.0282, -0.1357, -0.0095,  ...,  0.1699, -0.0422, -0.1079]],\n",
      "\n",
      "        [[-0.0515, -0.0791, -0.0356,  ...,  0.0776, -0.0874,  0.0806],\n",
      "         [-0.1855,  0.0342, -0.1924,  ...,  0.0466,  0.2471, -0.2891],\n",
      "         [-0.0043, -0.0083,  0.0004,  ..., -0.0040,  0.0070,  0.0027],\n",
      "         ...,\n",
      "         [-0.0135,  0.0757, -0.3262,  ...,  0.2393, -0.0527, -0.0664],\n",
      "         [ 0.0069,  0.2168, -0.2422,  ...,  0.2090,  0.0104, -0.0306],\n",
      "         [ 0.0069,  0.2168, -0.2422,  ...,  0.2090,  0.0104, -0.0306]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "8\n",
      "(tensor([[[-5.2734e-02, -4.2969e-02, -4.6875e-02,  ...,  7.9590e-02,\n",
      "          -3.4668e-02,  7.8613e-02],\n",
      "         [-2.0508e-01,  2.7466e-02, -1.5137e-01,  ..., -9.2773e-02,\n",
      "           8.9355e-02, -1.7773e-01],\n",
      "         [-4.4861e-03, -9.2773e-03,  3.5858e-04,  ..., -3.6926e-03,\n",
      "           6.6223e-03,  1.9455e-03],\n",
      "         ...,\n",
      "         [-7.7148e-02, -1.0925e-02, -9.7168e-02,  ...,  1.9336e-01,\n",
      "           8.1543e-02,  4.3457e-02],\n",
      "         [ 1.2891e-01,  1.1475e-01,  1.3086e-01,  ...,  3.3594e-01,\n",
      "           7.6172e-02,  1.6699e-01],\n",
      "         [ 1.2891e-01,  1.1475e-01,  1.3086e-01,  ...,  3.3594e-01,\n",
      "           7.6172e-02,  1.6699e-01]],\n",
      "\n",
      "        [[-5.6396e-02, -1.8555e-02, -4.8584e-02,  ...,  8.3984e-02,\n",
      "          -5.0293e-02,  4.1016e-02],\n",
      "         [-1.4355e-01,  1.8066e-02, -2.2363e-01,  ...,  4.8096e-02,\n",
      "           1.9922e-01, -1.6602e-01],\n",
      "         [-4.6692e-03, -8.4839e-03,  9.6893e-04,  ..., -3.5858e-03,\n",
      "           6.2866e-03,  2.4719e-03],\n",
      "         ...,\n",
      "         [ 1.2024e-02, -1.5320e-02, -1.1536e-02,  ...,  1.4062e-01,\n",
      "           1.8799e-02, -2.0508e-02],\n",
      "         [-2.8198e-02, -1.3574e-01, -9.5215e-03,  ...,  1.6992e-01,\n",
      "          -4.2236e-02, -1.0791e-01],\n",
      "         [-2.8198e-02, -1.3574e-01, -9.5215e-03,  ...,  1.6992e-01,\n",
      "          -4.2236e-02, -1.0791e-01]],\n",
      "\n",
      "        [[-6.4453e-02, -7.3242e-02, -6.0303e-02,  ...,  8.6914e-02,\n",
      "          -8.2031e-02,  9.0820e-02],\n",
      "         [-1.8066e-01,  1.7944e-02, -1.5625e-01,  ..., -1.4453e-01,\n",
      "           1.2109e-01, -1.5430e-01],\n",
      "         [-4.5776e-03, -8.4229e-03,  4.6730e-04,  ..., -2.8381e-03,\n",
      "           6.5918e-03,  2.4414e-03],\n",
      "         ...,\n",
      "         [ 1.5527e-01,  1.7822e-02, -2.1387e-01,  ...,  3.0078e-01,\n",
      "           1.0547e-01, -3.5400e-02],\n",
      "         [ 9.4238e-02,  8.4229e-03, -6.0547e-02,  ...,  3.3398e-01,\n",
      "           5.0781e-02,  1.7334e-02],\n",
      "         [ 9.4238e-02,  8.4229e-03, -6.0547e-02,  ...,  3.3398e-01,\n",
      "           5.0781e-02,  1.7334e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.8633e-02, -5.3467e-02, -7.2754e-02,  ...,  2.1240e-02,\n",
      "          -5.3711e-02,  7.5195e-02],\n",
      "         [-6.6406e-02, -2.2339e-02,  1.3199e-03,  ..., -1.1035e-01,\n",
      "           1.7578e-01, -1.7676e-01],\n",
      "         [-4.4250e-03, -7.2327e-03,  1.5259e-04,  ..., -2.9297e-03,\n",
      "           5.2185e-03,  2.0752e-03],\n",
      "         ...,\n",
      "         [-5.9082e-02, -9.7168e-02,  2.0142e-03,  ...,  3.3789e-01,\n",
      "          -8.7402e-02, -5.3223e-02],\n",
      "         [-9.9121e-02, -1.5442e-02,  1.2061e-01,  ...,  3.2031e-01,\n",
      "          -1.2695e-01,  7.5684e-02],\n",
      "         [-9.9121e-02, -1.5442e-02,  1.2061e-01,  ...,  3.2031e-01,\n",
      "          -1.2695e-01,  7.5684e-02]],\n",
      "\n",
      "        [[ 1.4404e-02, -9.9487e-03, -4.4922e-02,  ...,  5.0537e-02,\n",
      "          -2.8931e-02,  5.4932e-02],\n",
      "         [-1.4258e-01, -1.5747e-02, -1.6602e-01,  ..., -1.5723e-01,\n",
      "           1.6797e-01, -5.3223e-02],\n",
      "         [-4.4556e-03, -8.8501e-03,  3.8338e-04,  ..., -3.2349e-03,\n",
      "           6.2866e-03,  2.7466e-03],\n",
      "         ...,\n",
      "         [ 1.2988e-01,  1.7822e-02, -1.7090e-02,  ...,  7.0801e-02,\n",
      "          -1.3184e-01, -1.1475e-01],\n",
      "         [ 2.7148e-01,  4.9133e-03,  3.1128e-02,  ...,  2.1094e-01,\n",
      "           7.7209e-03, -8.0566e-02],\n",
      "         [ 2.7148e-01,  4.9133e-03,  3.1128e-02,  ...,  2.1094e-01,\n",
      "           7.7209e-03, -8.0566e-02]],\n",
      "\n",
      "        [[-5.1514e-02, -7.9102e-02, -3.5645e-02,  ...,  7.7637e-02,\n",
      "          -8.7402e-02,  8.0566e-02],\n",
      "         [-1.8555e-01,  3.4180e-02, -1.9238e-01,  ...,  4.6631e-02,\n",
      "           2.4707e-01, -2.8906e-01],\n",
      "         [-4.3030e-03, -8.3008e-03,  3.7384e-04,  ..., -3.9978e-03,\n",
      "           7.0190e-03,  2.6703e-03],\n",
      "         ...,\n",
      "         [-1.3489e-02,  7.5684e-02, -3.2617e-01,  ...,  2.3926e-01,\n",
      "          -5.2734e-02, -6.6406e-02],\n",
      "         [ 6.9275e-03,  2.1680e-01, -2.4219e-01,  ...,  2.0898e-01,\n",
      "           1.0437e-02, -3.0640e-02],\n",
      "         [ 6.9275e-03,  2.1680e-01, -2.4219e-01,  ...,  2.0898e-01,\n",
      "           1.0437e-02, -3.0640e-02]]], device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "9\n",
      "(tensor([[[-5.2490e-02, -2.9785e-02, -4.4678e-02,  ...,  5.1758e-02,\n",
      "          -1.0938e-01,  1.4160e-01],\n",
      "         [-1.8359e-01,  3.5645e-02, -7.0801e-02,  ..., -5.4688e-02,\n",
      "           7.1289e-02, -1.4062e-01],\n",
      "         [-4.4250e-03, -7.7209e-03,  4.2534e-04,  ..., -4.1504e-03,\n",
      "           6.8359e-03,  2.4414e-03],\n",
      "         ...,\n",
      "         [ 1.3428e-02, -1.6357e-02, -8.6426e-02,  ...,  3.7305e-01,\n",
      "          -1.2695e-01, -5.4932e-02],\n",
      "         [-3.6621e-02,  4.8828e-03, -6.6406e-02,  ...,  3.4570e-01,\n",
      "          -2.0605e-01, -8.9844e-02],\n",
      "         [-3.6621e-02,  4.8828e-03, -6.6406e-02,  ...,  3.4570e-01,\n",
      "          -2.0605e-01, -8.9844e-02]],\n",
      "\n",
      "        [[-6.5430e-02, -7.4219e-02, -5.9814e-02,  ...,  8.6426e-02,\n",
      "          -7.9590e-02,  8.8867e-02],\n",
      "         [-1.7773e-01,  2.1362e-02, -1.5723e-01,  ..., -1.4355e-01,\n",
      "           1.2109e-01, -1.5137e-01],\n",
      "         [-4.5471e-03, -8.3618e-03,  4.7112e-04,  ..., -2.8381e-03,\n",
      "           6.6223e-03,  2.4719e-03],\n",
      "         ...,\n",
      "         [ 1.5527e-01,  1.5198e-02, -2.1484e-01,  ...,  2.9492e-01,\n",
      "           1.1182e-01, -3.6133e-02],\n",
      "         [ 8.5449e-02,  7.7515e-03, -5.8350e-02,  ...,  3.2617e-01,\n",
      "           5.3467e-02,  1.5991e-02],\n",
      "         [ 8.5449e-02,  7.7515e-03, -5.8350e-02,  ...,  3.2617e-01,\n",
      "           5.3467e-02,  1.5991e-02]],\n",
      "\n",
      "        [[-5.7617e-02, -1.9775e-02, -4.6387e-02,  ...,  8.3496e-02,\n",
      "          -5.4199e-02,  4.1748e-02],\n",
      "         [-1.4453e-01,  1.8066e-02, -2.2168e-01,  ...,  5.3223e-02,\n",
      "           2.0020e-01, -1.6699e-01],\n",
      "         [-4.7607e-03, -8.4839e-03,  1.0223e-03,  ..., -3.5706e-03,\n",
      "           6.3477e-03,  2.5024e-03],\n",
      "         ...,\n",
      "         [ 9.8877e-03, -1.7456e-02, -1.5137e-02,  ...,  1.3867e-01,\n",
      "           2.3682e-02, -2.0630e-02],\n",
      "         [-2.3071e-02, -1.3574e-01, -1.7456e-02,  ...,  1.6504e-01,\n",
      "          -3.7598e-02, -9.3750e-02],\n",
      "         [-2.3071e-02, -1.3574e-01, -1.7456e-02,  ...,  1.6504e-01,\n",
      "          -3.7598e-02, -9.3750e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.0771e-02, -7.0801e-02, -8.4961e-02,  ...,  6.0791e-02,\n",
      "          -8.9355e-02,  4.6143e-02],\n",
      "         [-1.1865e-01, -3.1006e-02,  1.2207e-01,  ...,  1.0596e-01,\n",
      "           6.7383e-02, -2.8711e-01],\n",
      "         [-3.4637e-03, -7.3242e-03, -5.6076e-04,  ..., -3.4485e-03,\n",
      "           6.5918e-03,  1.4801e-03],\n",
      "         ...,\n",
      "         [ 8.0078e-02, -8.0566e-02, -3.2812e-01,  ...,  1.4941e-01,\n",
      "          -2.6093e-03,  2.8516e-01],\n",
      "         [-5.1270e-02,  8.3923e-04, -1.9531e-01,  ...,  2.7734e-01,\n",
      "          -1.5991e-02,  2.1094e-01],\n",
      "         [-5.1270e-02,  8.3923e-04, -1.9531e-01,  ...,  2.7734e-01,\n",
      "          -1.5991e-02,  2.1094e-01]],\n",
      "\n",
      "        [[-5.7617e-02, -1.9775e-02, -4.6387e-02,  ...,  8.3496e-02,\n",
      "          -5.4199e-02,  4.1748e-02],\n",
      "         [-1.4453e-01,  1.8066e-02, -2.2168e-01,  ...,  5.3223e-02,\n",
      "           2.0020e-01, -1.6699e-01],\n",
      "         [-4.7607e-03, -8.4839e-03,  1.0223e-03,  ..., -3.5706e-03,\n",
      "           6.3477e-03,  2.5024e-03],\n",
      "         ...,\n",
      "         [ 9.8877e-03, -1.7456e-02, -1.5137e-02,  ...,  1.3867e-01,\n",
      "           2.3682e-02, -2.0630e-02],\n",
      "         [-2.3071e-02, -1.3574e-01, -1.7456e-02,  ...,  1.6504e-01,\n",
      "          -3.7598e-02, -9.3750e-02],\n",
      "         [-2.3071e-02, -1.3574e-01, -1.7456e-02,  ...,  1.6504e-01,\n",
      "          -3.7598e-02, -9.3750e-02]],\n",
      "\n",
      "        [[-5.1025e-02, -7.8613e-02, -3.5889e-02,  ...,  7.8613e-02,\n",
      "          -8.5449e-02,  8.1055e-02],\n",
      "         [-1.8359e-01,  3.5645e-02, -1.9043e-01,  ...,  4.3457e-02,\n",
      "           2.4805e-01, -2.9102e-01],\n",
      "         [-4.3030e-03, -8.3618e-03,  3.6049e-04,  ..., -4.0283e-03,\n",
      "           7.0801e-03,  2.7161e-03],\n",
      "         ...,\n",
      "         [-1.1047e-02,  7.7148e-02, -3.3008e-01,  ...,  2.3926e-01,\n",
      "          -4.9316e-02, -6.1279e-02],\n",
      "         [ 9.3994e-03,  2.2266e-01, -2.4121e-01,  ...,  2.1094e-01,\n",
      "           8.9111e-03, -3.1982e-02],\n",
      "         [ 9.3994e-03,  2.2266e-01, -2.4121e-01,  ...,  2.1094e-01,\n",
      "           8.9111e-03, -3.1982e-02]]], device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "10\n",
      "(tensor([[[-0.0554, -0.0186, -0.0491,  ...,  0.0825, -0.0510,  0.0427],\n",
      "         [-0.1445,  0.0187, -0.2197,  ...,  0.0493,  0.2002, -0.1670],\n",
      "         [-0.0047, -0.0084,  0.0010,  ..., -0.0036,  0.0063,  0.0026],\n",
      "         ...,\n",
      "         [ 0.0107, -0.0160, -0.0095,  ...,  0.1348,  0.0161, -0.0123],\n",
      "         [-0.0159, -0.1299, -0.0134,  ...,  0.1729, -0.0437, -0.1021],\n",
      "         [-0.0159, -0.1299, -0.0134,  ...,  0.1729, -0.0437, -0.1021]],\n",
      "\n",
      "        [[-0.0530, -0.0791, -0.0361,  ...,  0.0771, -0.0854,  0.0820],\n",
      "         [-0.1836,  0.0344, -0.1924,  ...,  0.0435,  0.2451, -0.2910],\n",
      "         [-0.0043, -0.0083,  0.0004,  ..., -0.0040,  0.0070,  0.0026],\n",
      "         ...,\n",
      "         [-0.0128,  0.0767, -0.3262,  ...,  0.2422, -0.0508, -0.0640],\n",
      "         [ 0.0066,  0.2246, -0.2354,  ...,  0.2168,  0.0103, -0.0342],\n",
      "         [ 0.0066,  0.2246, -0.2354,  ...,  0.2168,  0.0103, -0.0342]],\n",
      "\n",
      "        [[-0.0332, -0.0796, -0.0288,  ...,  0.0303, -0.0654,  0.0874],\n",
      "         [-0.2305, -0.0215, -0.1826,  ..., -0.0630,  0.2109, -0.0918],\n",
      "         [-0.0044, -0.0092,  0.0005,  ..., -0.0037,  0.0071,  0.0025],\n",
      "         ...,\n",
      "         [-0.1631,  0.1289, -0.0796,  ...,  0.0947, -0.0510, -0.0684],\n",
      "         [-0.3516,  0.1465, -0.0723,  ...,  0.0562, -0.0962, -0.1001],\n",
      "         [-0.3516,  0.1465, -0.0723,  ...,  0.0562, -0.0962, -0.1001]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0408, -0.0718, -0.0845,  ...,  0.0601, -0.0889,  0.0461],\n",
      "         [-0.1196, -0.0322,  0.1221,  ...,  0.1094,  0.0679, -0.2852],\n",
      "         [-0.0035, -0.0074, -0.0006,  ..., -0.0034,  0.0066,  0.0015],\n",
      "         ...,\n",
      "         [ 0.0830, -0.0786, -0.3301,  ...,  0.1465, -0.0015,  0.2910],\n",
      "         [-0.0452, -0.0044, -0.1943,  ...,  0.2734, -0.0082,  0.2158],\n",
      "         [-0.0452, -0.0044, -0.1943,  ...,  0.2734, -0.0082,  0.2158]],\n",
      "\n",
      "        [[-0.0554, -0.0186, -0.0491,  ...,  0.0825, -0.0510,  0.0427],\n",
      "         [-0.1445,  0.0187, -0.2197,  ...,  0.0493,  0.2002, -0.1670],\n",
      "         [-0.0047, -0.0084,  0.0010,  ..., -0.0036,  0.0063,  0.0026],\n",
      "         ...,\n",
      "         [ 0.0107, -0.0160, -0.0095,  ...,  0.1348,  0.0161, -0.0123],\n",
      "         [-0.0159, -0.1299, -0.0134,  ...,  0.1729, -0.0437, -0.1021],\n",
      "         [-0.0159, -0.1299, -0.0134,  ...,  0.1729, -0.0437, -0.1021]],\n",
      "\n",
      "        [[-0.0530, -0.0791, -0.0361,  ...,  0.0771, -0.0854,  0.0820],\n",
      "         [-0.1836,  0.0344, -0.1924,  ...,  0.0435,  0.2451, -0.2910],\n",
      "         [-0.0043, -0.0083,  0.0004,  ..., -0.0040,  0.0070,  0.0026],\n",
      "         ...,\n",
      "         [-0.0128,  0.0767, -0.3262,  ...,  0.2422, -0.0508, -0.0640],\n",
      "         [ 0.0066,  0.2246, -0.2354,  ...,  0.2168,  0.0103, -0.0342],\n",
      "         [ 0.0066,  0.2246, -0.2354,  ...,  0.2168,  0.0103, -0.0342]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "11\n",
      "(tensor([[[-1.5259e-02, -6.5430e-02, -4.2969e-02,  ...,  9.7168e-02,\n",
      "          -9.5703e-02,  1.2012e-01],\n",
      "         [-2.2754e-01, -6.5613e-04, -1.4941e-01,  ..., -8.3984e-02,\n",
      "           1.9531e-01, -9.1309e-02],\n",
      "         [-4.2419e-03, -7.9346e-03,  7.3242e-04,  ..., -3.7079e-03,\n",
      "           7.1106e-03,  2.5330e-03],\n",
      "         ...,\n",
      "         [-5.0049e-02,  5.6152e-02, -1.9824e-01,  ...,  2.5586e-01,\n",
      "          -1.1377e-01, -3.4180e-02],\n",
      "         [ 9.1797e-02,  3.3936e-02, -9.8633e-02,  ...,  2.3145e-01,\n",
      "          -1.1749e-03, -2.2339e-02],\n",
      "         [ 9.1797e-02,  3.3936e-02, -9.8633e-02,  ...,  2.3145e-01,\n",
      "          -1.1749e-03, -2.2339e-02]],\n",
      "\n",
      "        [[ 1.4648e-02, -7.1716e-03, -4.4922e-02,  ...,  5.1025e-02,\n",
      "          -2.9419e-02,  5.4443e-02],\n",
      "         [-1.4258e-01, -1.6113e-02, -1.6504e-01,  ..., -1.5527e-01,\n",
      "           1.6797e-01, -4.8096e-02],\n",
      "         [-4.4556e-03, -8.8501e-03,  3.6240e-04,  ..., -3.2196e-03,\n",
      "           6.2561e-03,  2.7161e-03],\n",
      "         ...,\n",
      "         [ 1.2500e-01,  1.5503e-02, -1.6602e-02,  ...,  7.7148e-02,\n",
      "          -1.3281e-01, -1.1230e-01],\n",
      "         [ 2.7539e-01, -3.6469e-03,  2.6978e-02,  ...,  2.1191e-01,\n",
      "           5.0354e-03, -7.3242e-02],\n",
      "         [ 2.7539e-01, -3.6469e-03,  2.6978e-02,  ...,  2.1191e-01,\n",
      "           5.0354e-03, -7.3242e-02]],\n",
      "\n",
      "        [[-5.4199e-02, -9.1797e-02, -5.3223e-02,  ...,  1.4160e-01,\n",
      "          -9.1797e-02,  8.2520e-02],\n",
      "         [-1.7285e-01,  5.2246e-02, -9.7168e-02,  ...,  5.6885e-02,\n",
      "           1.2695e-01, -1.8652e-01],\n",
      "         [-4.3030e-03, -8.4839e-03,  1.2207e-03,  ..., -3.3417e-03,\n",
      "           6.7749e-03,  2.7618e-03],\n",
      "         ...,\n",
      "         [ 2.5195e-01,  4.5898e-02, -1.8945e-01,  ...,  2.4805e-01,\n",
      "           6.3477e-02, -3.0640e-02],\n",
      "         [ 2.4609e-01,  1.7383e-01, -7.5378e-03,  ...,  3.1836e-01,\n",
      "          -3.9551e-02, -1.6211e-01],\n",
      "         [ 2.4609e-01,  1.7383e-01, -7.5378e-03,  ...,  3.1836e-01,\n",
      "          -3.9551e-02, -1.6211e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-5.2246e-02, -2.8320e-02, -4.5654e-02,  ...,  5.0781e-02,\n",
      "          -1.1084e-01,  1.4160e-01],\n",
      "         [-1.8555e-01,  3.5889e-02, -6.8359e-02,  ..., -5.4688e-02,\n",
      "           7.2266e-02, -1.4258e-01],\n",
      "         [-4.3945e-03, -7.7209e-03,  4.2152e-04,  ..., -4.1504e-03,\n",
      "           6.8359e-03,  2.3804e-03],\n",
      "         ...,\n",
      "         [ 1.4587e-02, -2.1851e-02, -8.5938e-02,  ...,  3.7109e-01,\n",
      "          -1.2256e-01, -5.1758e-02],\n",
      "         [-3.7354e-02,  6.0120e-03, -7.3730e-02,  ...,  3.4570e-01,\n",
      "          -2.1387e-01, -9.1797e-02],\n",
      "         [-3.7354e-02,  6.0120e-03, -7.3730e-02,  ...,  3.4570e-01,\n",
      "          -2.1387e-01, -9.1797e-02]],\n",
      "\n",
      "        [[ 1.4648e-02, -7.1716e-03, -4.4922e-02,  ...,  5.1025e-02,\n",
      "          -2.9419e-02,  5.4443e-02],\n",
      "         [-1.4258e-01, -1.6113e-02, -1.6504e-01,  ..., -1.5527e-01,\n",
      "           1.6797e-01, -4.8096e-02],\n",
      "         [-4.4556e-03, -8.8501e-03,  3.6240e-04,  ..., -3.2196e-03,\n",
      "           6.2561e-03,  2.7161e-03],\n",
      "         ...,\n",
      "         [ 1.2500e-01,  1.5503e-02, -1.6602e-02,  ...,  7.7148e-02,\n",
      "          -1.3281e-01, -1.1230e-01],\n",
      "         [ 2.7539e-01, -3.6469e-03,  2.6978e-02,  ...,  2.1191e-01,\n",
      "           5.0354e-03, -7.3242e-02],\n",
      "         [ 2.7539e-01, -3.6469e-03,  2.6978e-02,  ...,  2.1191e-01,\n",
      "           5.0354e-03, -7.3242e-02]],\n",
      "\n",
      "        [[-5.2979e-02, -7.9102e-02, -3.6133e-02,  ...,  7.7148e-02,\n",
      "          -8.5449e-02,  8.2031e-02],\n",
      "         [-1.8359e-01,  3.4424e-02, -1.9238e-01,  ...,  4.3457e-02,\n",
      "           2.4512e-01, -2.9102e-01],\n",
      "         [-4.3335e-03, -8.3008e-03,  3.6621e-04,  ..., -3.9978e-03,\n",
      "           7.0190e-03,  2.6398e-03],\n",
      "         ...,\n",
      "         [-1.2756e-02,  7.6660e-02, -3.2617e-01,  ...,  2.4219e-01,\n",
      "          -5.0781e-02, -6.3965e-02],\n",
      "         [ 6.5918e-03,  2.2461e-01, -2.3535e-01,  ...,  2.1680e-01,\n",
      "           1.0254e-02, -3.4180e-02],\n",
      "         [ 6.5918e-03,  2.2461e-01, -2.3535e-01,  ...,  2.1680e-01,\n",
      "           1.0254e-02, -3.4180e-02]]], device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "12\n",
      "(tensor([[[-0.0576, -0.0198, -0.0464,  ...,  0.0835, -0.0542,  0.0417],\n",
      "         [-0.1445,  0.0181, -0.2217,  ...,  0.0532,  0.2002, -0.1670],\n",
      "         [-0.0048, -0.0085,  0.0010,  ..., -0.0036,  0.0063,  0.0025],\n",
      "         ...,\n",
      "         [ 0.0099, -0.0175, -0.0151,  ...,  0.1387,  0.0237, -0.0206],\n",
      "         [-0.0231, -0.1357, -0.0175,  ...,  0.1650, -0.0376, -0.0938],\n",
      "         [-0.0231, -0.1357, -0.0175,  ...,  0.1650, -0.0376, -0.0938]],\n",
      "\n",
      "        [[-0.0510, -0.0786, -0.0359,  ...,  0.0786, -0.0854,  0.0811],\n",
      "         [-0.1836,  0.0356, -0.1904,  ...,  0.0435,  0.2480, -0.2910],\n",
      "         [-0.0043, -0.0084,  0.0004,  ..., -0.0040,  0.0071,  0.0027],\n",
      "         ...,\n",
      "         [-0.0110,  0.0771, -0.3301,  ...,  0.2393, -0.0493, -0.0613],\n",
      "         [ 0.0094,  0.2227, -0.2412,  ...,  0.2109,  0.0089, -0.0320],\n",
      "         [ 0.0094,  0.2227, -0.2412,  ...,  0.2109,  0.0089, -0.0320]],\n",
      "\n",
      "        [[-0.0327, -0.0801, -0.0272,  ...,  0.0308, -0.0654,  0.0884],\n",
      "         [-0.2295, -0.0206, -0.1836,  ..., -0.0659,  0.2061, -0.0898],\n",
      "         [-0.0044, -0.0091,  0.0005,  ..., -0.0036,  0.0070,  0.0025],\n",
      "         ...,\n",
      "         [-0.1631,  0.1289, -0.0811,  ...,  0.0942, -0.0518, -0.0605],\n",
      "         [-0.3574,  0.1553, -0.0713,  ...,  0.0557, -0.1001, -0.1030],\n",
      "         [-0.3574,  0.1553, -0.0713,  ...,  0.0557, -0.1001, -0.1030]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0408, -0.0708, -0.0850,  ...,  0.0608, -0.0894,  0.0461],\n",
      "         [-0.1187, -0.0310,  0.1221,  ...,  0.1060,  0.0674, -0.2871],\n",
      "         [-0.0035, -0.0073, -0.0006,  ..., -0.0034,  0.0066,  0.0015],\n",
      "         ...,\n",
      "         [ 0.0801, -0.0806, -0.3281,  ...,  0.1494, -0.0026,  0.2852],\n",
      "         [-0.0513,  0.0008, -0.1953,  ...,  0.2773, -0.0160,  0.2109],\n",
      "         [-0.0513,  0.0008, -0.1953,  ...,  0.2773, -0.0160,  0.2109]],\n",
      "\n",
      "        [[-0.0576, -0.0198, -0.0464,  ...,  0.0835, -0.0542,  0.0417],\n",
      "         [-0.1445,  0.0181, -0.2217,  ...,  0.0532,  0.2002, -0.1670],\n",
      "         [-0.0048, -0.0085,  0.0010,  ..., -0.0036,  0.0063,  0.0025],\n",
      "         ...,\n",
      "         [ 0.0099, -0.0175, -0.0151,  ...,  0.1387,  0.0237, -0.0206],\n",
      "         [-0.0231, -0.1357, -0.0175,  ...,  0.1650, -0.0376, -0.0938],\n",
      "         [-0.0231, -0.1357, -0.0175,  ...,  0.1650, -0.0376, -0.0938]],\n",
      "\n",
      "        [[-0.0510, -0.0786, -0.0359,  ...,  0.0786, -0.0854,  0.0811],\n",
      "         [-0.1836,  0.0356, -0.1904,  ...,  0.0435,  0.2480, -0.2910],\n",
      "         [-0.0043, -0.0084,  0.0004,  ..., -0.0040,  0.0071,  0.0027],\n",
      "         ...,\n",
      "         [-0.0110,  0.0771, -0.3301,  ...,  0.2393, -0.0493, -0.0613],\n",
      "         [ 0.0094,  0.2227, -0.2412,  ...,  0.2109,  0.0089, -0.0320],\n",
      "         [ 0.0094,  0.2227, -0.2412,  ...,  0.2109,  0.0089, -0.0320]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n",
      "13\n",
      "(tensor([[[-5.3467e-02, -4.0527e-02, -4.5654e-02,  ...,  8.1543e-02,\n",
      "          -3.4912e-02,  7.7637e-02],\n",
      "         [-2.0605e-01,  2.5024e-02, -1.5332e-01,  ..., -9.1797e-02,\n",
      "           8.9844e-02, -1.7871e-01],\n",
      "         [-4.4861e-03, -9.2163e-03,  4.1580e-04,  ..., -3.6621e-03,\n",
      "           6.6223e-03,  1.9302e-03],\n",
      "         ...,\n",
      "         [-7.5195e-02, -1.1047e-02, -9.5703e-02,  ...,  1.9336e-01,\n",
      "           8.5938e-02,  4.5410e-02],\n",
      "         [ 1.2695e-01,  1.1133e-01,  1.3477e-01,  ...,  3.3008e-01,\n",
      "           7.7637e-02,  1.7090e-01],\n",
      "         [ 1.2695e-01,  1.1133e-01,  1.3477e-01,  ...,  3.3008e-01,\n",
      "           7.7637e-02,  1.7090e-01]],\n",
      "\n",
      "        [[-5.7617e-02, -1.9775e-02, -4.6387e-02,  ...,  8.3496e-02,\n",
      "          -5.4199e-02,  4.1748e-02],\n",
      "         [-1.4453e-01,  1.8066e-02, -2.2168e-01,  ...,  5.3223e-02,\n",
      "           2.0020e-01, -1.6699e-01],\n",
      "         [-4.7607e-03, -8.4839e-03,  1.0223e-03,  ..., -3.5706e-03,\n",
      "           6.3477e-03,  2.5024e-03],\n",
      "         ...,\n",
      "         [ 9.8877e-03, -1.7456e-02, -1.5137e-02,  ...,  1.3867e-01,\n",
      "           2.3682e-02, -2.0630e-02],\n",
      "         [-2.3071e-02, -1.3574e-01, -1.7456e-02,  ...,  1.6504e-01,\n",
      "          -3.7598e-02, -9.3750e-02],\n",
      "         [-2.3071e-02, -1.3574e-01, -1.7456e-02,  ...,  1.6504e-01,\n",
      "          -3.7598e-02, -9.3750e-02]],\n",
      "\n",
      "        [[-6.5430e-02, -7.4219e-02, -5.9814e-02,  ...,  8.6426e-02,\n",
      "          -7.9590e-02,  8.8867e-02],\n",
      "         [-1.7773e-01,  2.1362e-02, -1.5723e-01,  ..., -1.4355e-01,\n",
      "           1.2109e-01, -1.5137e-01],\n",
      "         [-4.5471e-03, -8.3618e-03,  4.7112e-04,  ..., -2.8381e-03,\n",
      "           6.6223e-03,  2.4719e-03],\n",
      "         ...,\n",
      "         [ 1.5527e-01,  1.5198e-02, -2.1484e-01,  ...,  2.9492e-01,\n",
      "           1.1182e-01, -3.6133e-02],\n",
      "         [ 8.5449e-02,  7.7515e-03, -5.8350e-02,  ...,  3.2617e-01,\n",
      "           5.3467e-02,  1.5991e-02],\n",
      "         [ 8.5449e-02,  7.7515e-03, -5.8350e-02,  ...,  3.2617e-01,\n",
      "           5.3467e-02,  1.5991e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-5.2490e-02, -2.9785e-02, -4.4678e-02,  ...,  5.1758e-02,\n",
      "          -1.0938e-01,  1.4160e-01],\n",
      "         [-1.8359e-01,  3.5645e-02, -7.0801e-02,  ..., -5.4688e-02,\n",
      "           7.1289e-02, -1.4062e-01],\n",
      "         [-4.4250e-03, -7.7209e-03,  4.2534e-04,  ..., -4.1504e-03,\n",
      "           6.8359e-03,  2.4414e-03],\n",
      "         ...,\n",
      "         [ 1.3428e-02, -1.6357e-02, -8.6426e-02,  ...,  3.7305e-01,\n",
      "          -1.2695e-01, -5.4932e-02],\n",
      "         [-3.6621e-02,  4.8828e-03, -6.6406e-02,  ...,  3.4570e-01,\n",
      "          -2.0605e-01, -8.9844e-02],\n",
      "         [-3.6621e-02,  4.8828e-03, -6.6406e-02,  ...,  3.4570e-01,\n",
      "          -2.0605e-01, -8.9844e-02]],\n",
      "\n",
      "        [[ 1.5503e-02, -9.8877e-03, -4.3945e-02,  ...,  5.0537e-02,\n",
      "          -3.0273e-02,  5.4932e-02],\n",
      "         [-1.4258e-01, -1.5747e-02, -1.6699e-01,  ..., -1.5527e-01,\n",
      "           1.6895e-01, -5.1758e-02],\n",
      "         [-4.4250e-03, -8.7280e-03,  3.6240e-04,  ..., -3.2043e-03,\n",
      "           6.2866e-03,  2.7466e-03],\n",
      "         ...,\n",
      "         [ 1.2598e-01,  1.4465e-02, -1.7700e-02,  ...,  6.8848e-02,\n",
      "          -1.3379e-01, -1.1621e-01],\n",
      "         [ 2.6758e-01,  6.5308e-03,  3.4424e-02,  ...,  2.1387e-01,\n",
      "           3.3569e-03, -8.1055e-02],\n",
      "         [ 2.6758e-01,  6.5308e-03,  3.4424e-02,  ...,  2.1387e-01,\n",
      "           3.3569e-03, -8.1055e-02]],\n",
      "\n",
      "        [[-5.1025e-02, -7.8613e-02, -3.5889e-02,  ...,  7.8613e-02,\n",
      "          -8.5449e-02,  8.1055e-02],\n",
      "         [-1.8359e-01,  3.5645e-02, -1.9043e-01,  ...,  4.3457e-02,\n",
      "           2.4805e-01, -2.9102e-01],\n",
      "         [-4.3030e-03, -8.3618e-03,  3.6049e-04,  ..., -4.0283e-03,\n",
      "           7.0801e-03,  2.7161e-03],\n",
      "         ...,\n",
      "         [-1.1047e-02,  7.7148e-02, -3.3008e-01,  ...,  2.3926e-01,\n",
      "          -4.9316e-02, -6.1279e-02],\n",
      "         [ 9.3994e-03,  2.2266e-01, -2.4121e-01,  ...,  2.1094e-01,\n",
      "           8.9111e-03, -3.1982e-02],\n",
      "         [ 9.3994e-03,  2.2266e-01, -2.4121e-01,  ...,  2.1094e-01,\n",
      "           8.9111e-03, -3.1982e-02]]], device='cuda:0', dtype=torch.bfloat16), tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# # visualize prompts \n",
    "# visualize_prompts = [\n",
    "#     \"triangle\",\n",
    "#     \"blue triangle\",\n",
    "#     \"red square\",\n",
    "#     \"square\",\n",
    "#     \"circle\",\n",
    "#     \"blue cirle\",\n",
    "#     \"triangle is to the upper left of square\", \n",
    "#     \"triangle is to the left of square\", \n",
    "#     \"triangle is to the left of triangle\", \n",
    "#     \"circle is below red square\",\n",
    "#     \"red circle is to the left of blue square\",\n",
    "#     \"blue square is to the right of red circle\",\n",
    "#     \"red circle is above square\",\n",
    "#     \"triangle is above red circle\",\n",
    "# ]\n",
    "# prompt_cache_dir = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/prompt_cache_t5emb\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(T5_path)\n",
    "\n",
    "# # t5_text_encoder = T5EmbeddingEncoder(model_name=T5_path, device=\"cuda\")\n",
    "\n",
    "# caption_embeddings = []\n",
    "\n",
    "# for prompt in visualize_prompts:\n",
    "#     caption_embeddings.append(t5.get_text_embeddings(prompt))\n",
    "\n",
    "# # caption_embeddings = save_prompt_embeddings(tokenizer, t5, \n",
    "# #     visualize_prompts, prompt_cache_dir, device=\"cuda\", max_length=20, t5_path=T5_path, recompute=True)\n",
    "# for i, embedding in enumerate(caption_embeddings):\n",
    "#     print(i)\n",
    "#     print(embedding)\n",
    "#     # print(f\"{i}: {embedding['prompt']} | token num:{embedding['emb_mask'].sum()}\")\n",
    "\n",
    "# torch.save(caption_embeddings, join(prompt_cache_dir, \"caption_embeddings_list.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = [\"triangle is above red circle\"]\n",
    "caption_emb, emb_mask = t5.get_text_embeddings(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models//t5_ckpts/t5-v1_1-xxl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52d1ac8641043f9a42c0e9b68c17129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Visualization prompt embeddings...\n",
      "Saving visualizate prompt text embedding at /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/prompt_cache_t5emb\n",
      "Mapping triangle...\n",
      "Mapping blue triangle...\n",
      "Mapping red square...\n",
      "Mapping square...\n",
      "Mapping circle...\n",
      "Mapping blue cirle...\n",
      "Mapping triangle is to the upper left of square...\n",
      "Mapping triangle is to the left of square...\n",
      "Mapping triangle is to the left of triangle...\n",
      "Mapping circle is below red square...\n",
      "Mapping red circle is to the left of blue square...\n",
      "Mapping blue square is to the right of red circle...\n",
      "Mapping red circle is above square...\n",
      "Mapping triangle is above red circle...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# @torch.no_grad()\n",
    "# def save_prompt_embeddings(tokenizer, text_encoder, validation_prompts, prompt_cache_dir=\"output/tmp/prompt_cache\", \n",
    "#                            device=\"cuda\", max_length=20, t5_path=None, recompute=False):\n",
    "#     \"\"\"Save T5 text embeddings for a list of prompts to cache directory.\n",
    "    \n",
    "#     Args:\n",
    "#         validation_prompts (list): List of text prompts to encode\n",
    "#         prompt_cache_dir (str): Directory to save embeddings\n",
    "#         device (str): Device to run encoding on\n",
    "#         max_length (int): Max sequence length for tokenization\n",
    "#         t5_path (str): Path to T5 model. If None, uses default path\n",
    "#     \"\"\"\n",
    "#     if t5_path is None:\n",
    "#         t5_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\"\n",
    "\n",
    "\n",
    "\n",
    "validation_prompts = [\n",
    "    \"triangle\",\n",
    "    \"blue triangle\",\n",
    "    \"red square\",\n",
    "    \"square\",\n",
    "    \"circle\",\n",
    "    \"blue cirle\",\n",
    "    \"triangle is to the upper left of square\", \n",
    "    \"triangle is to the left of square\", \n",
    "    \"triangle is to the left of triangle\", \n",
    "    \"circle is below red square\",\n",
    "    \"red circle is to the left of blue square\",\n",
    "    \"blue square is to the right of red circle\",\n",
    "    \"red circle is above square\",\n",
    "    \"triangle is above red circle\",\n",
    "    ]\n",
    "max_length = 20\n",
    "result_col = []\n",
    "recompute = True \n",
    "device = \"cuda\"\n",
    "prompt_cache_dir = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/prompt_cache_t5emb\"\n",
    "os.makedirs(prompt_cache_dir, exist_ok=True)\n",
    "\n",
    "pretrain_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/\"\n",
    "t5 = T5Embedder(device=\"cuda\", local_cache=True, cache_dir=f'{pretrain_path}/t5_ckpts', model_max_length=max_length)\n",
    "    # Load models\n",
    "# print(f\"Loading text encoder and tokenizer from {t5_path} ...\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "# text_encoder = T5EncoderModel.from_pretrained(t5_path).to(device)\n",
    "# text_encoder = text_encoder.to(device)\n",
    "\n",
    "# Save unconditioned embedding\n",
    "uncond_prompt_embeds, uncond_attention_mask = t5.get_text_embeddings([\"\"], )\n",
    "torch.save({'caption_embeds': uncond_prompt_embeds, 'emb_mask': uncond_attention_mask, 'prompt': ''}, \n",
    "            join(prompt_cache_dir,f'uncond_{max_length}token.pth'))\n",
    "result_col.append({'prompt': '', 'caption_embeds': uncond_prompt_embeds, 'emb_mask': uncond_attention_mask})\n",
    "\n",
    "print(\"Preparing Visualization prompt embeddings...\")\n",
    "print(f\"Saving visualizate prompt text embedding at {prompt_cache_dir}\")\n",
    "\n",
    "for prompt in validation_prompts:\n",
    "    if os.path.exists(join(prompt_cache_dir,f'{prompt}_{max_length}token.pth')) and not recompute:\n",
    "        result_col.append(torch.load(join(prompt_cache_dir,f'{prompt}_{max_length}token.pth')))\n",
    "        continue\n",
    "    print(f\"Mapping {prompt}...\")\n",
    "    caption_emb, caption_token_attention_mask =  t5.get_text_embeddings([prompt], )\n",
    "    torch.save({'caption_embeds': caption_emb, 'emb_mask': caption_token_attention_mask, 'prompt': prompt}, \n",
    "                join(prompt_cache_dir,f'{prompt}_{max_length}token.pth'))\n",
    "    result_col.append({'prompt': prompt, 'caption_embeds': caption_emb, 'emb_mask': caption_token_attention_mask})\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 4096])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 4096])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[\"caption_embeds\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0356, -0.0014,  0.0184,  ...,  0.2559, -0.0903,  0.0052],\n",
       "         [ 0.0723,  0.0737,  0.0354,  ...,  0.0579, -0.1177, -0.0776],\n",
       "         [ 0.0845, -0.0737, -0.1328,  ...,  0.2051, -0.2041,  0.1089],\n",
       "         ...,\n",
       "         [-0.1084,  0.2100, -0.0830,  ..., -0.0542,  0.0327,  0.0664],\n",
       "         [-0.1348,  0.1826, -0.1064,  ..., -0.0049,  0.0618, -0.0110],\n",
       "         [-0.0752,  0.2207, -0.1543,  ..., -0.0413,  0.0056, -0.0293]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.allclose(caption_emb, embedding[ 'caption_embeds'], atol=1E-1, rtol=1E-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'triangle is above red circle',\n",
       " 'caption_embeds': tensor([[[ 0.0386,  0.0007,  0.0167,  ...,  0.2578, -0.0903,  0.0042],\n",
       "          [ 0.0723,  0.0742,  0.0349,  ...,  0.0586, -0.1196, -0.0781],\n",
       "          [ 0.0859, -0.0747, -0.1338,  ...,  0.2041, -0.2061,  0.1104],\n",
       "          ...,\n",
       "          [-0.1069,  0.2070, -0.0820,  ..., -0.0559,  0.0347,  0.0674],\n",
       "          [-0.1299,  0.1836, -0.1099,  ..., -0.0120,  0.0574, -0.0200],\n",
       "          [-0.0781,  0.2246, -0.1523,  ..., -0.0378,  0.0069, -0.0378]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'emb_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[\"emb_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1442556/440429035.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  caption_embeddings = torch.load(join(prompt_cache_dir1, \"caption_embeddings_list.pth\"))\n"
     ]
    }
   ],
   "source": [
    "prompt_cache_dir1 = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/prompt_cache_t5emb\"\n",
    "caption_embeddings = torch.load(join(prompt_cache_dir1, \"caption_embeddings_list.pth\"))\n",
    "# for i, embedding in enumerate(caption_embeddings):\n",
    "#     print(f\"{i}: {embedding['prompt']} | token num:{embedding['emb_mask'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

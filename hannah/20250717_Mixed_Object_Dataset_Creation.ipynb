{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dataset that combines images with 2 objects and 1 object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from diffusers import AutoencoderKL\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from PIL import Image, ImageDraw\n",
    "from os.path import join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer to the double object and single object classes made previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SingleShapeDataset(Dataset):\n",
    "    def __init__(self, num_images, resolution=64, radius=8, transform=None):\n",
    "        self.num_images = num_images\n",
    "        self.shapes = ['triangle', 'circle', 'square']\n",
    "        self.colors = ['red', 'blue']\n",
    "        self.articles = ['a', 'an', 'the', 'or', '']\n",
    "        self.canvas_size = resolution\n",
    "        self.radius = radius\n",
    "        self.transform = transform \n",
    "        self.shape_to_idx = {'triangle': 0, 'circle': 1, 'square': 2}\n",
    "        self.color_to_rgb = {'red': 'red', 'blue': 'blue'}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "    \n",
    "    def draw_shape_on_image(self, img, shape, location, color):\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        x, y = location\n",
    "        \n",
    "        if shape == 'circle':\n",
    "            r = self.radius\n",
    "            draw.ellipse([(x - r, y - r), (x + r, y + r)], fill=color)\n",
    "        elif shape == 'square':\n",
    "            s = self.radius * 2\n",
    "            draw.rectangle([(x - s//2, y - s//2), (x + s//2, y + s//2)], fill=color)\n",
    "        elif shape == 'triangle':\n",
    "            s = self.radius * 2\n",
    "            h = s * (3 ** 0.5) / 2\n",
    "            point1 = (x, y - h / 3)\n",
    "            point2 = (x - s / 2, y + h * 2 / 3)\n",
    "            point3 = (x + s / 2, y + h * 2 / 3)\n",
    "            draw.polygon([point1, point2, point3], fill=color)\n",
    "\n",
    "        return img\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Blank image\n",
    "        img = Image.new('RGB', (self.canvas_size, self.canvas_size), 'gray')\n",
    "\n",
    "        # Randomly select shape and color\n",
    "        include_color = random.random() < 0.8\n",
    "        shape = random.choice(self.shapes)\n",
    "        color = random.choice(self.colors)\n",
    "        article = random.choice(self.articles)\n",
    "\n",
    "        components = [article, color, shape]\n",
    "        caption = f\"{article} {color if include_color else ''} {shape.lstrip()}\".strip()\n",
    "\n",
    "        # Random location\n",
    "        x = random.randint(self.radius + 1, self.canvas_size - self.radius - 1)\n",
    "        y = random.randint(self.radius + 1, self.canvas_size - self.radius - 1)\n",
    "\n",
    "        # Draw the shape\n",
    "        img = self.draw_shape_on_image(img, shape, (x, y), color=self.color_to_rgb[color])\n",
    "\n",
    "        # Convert image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = transforms.ToTensor()(img)\n",
    "\n",
    "        labels = {\n",
    "            'shape': self.shape_to_idx[shape],\n",
    "            'location': torch.tensor([x, y], dtype=torch.float32),\n",
    "            'caption': caption\n",
    "        }\n",
    "\n",
    "        return img, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleShapeDataset(Dataset):\n",
    "    def __init__(self, num_images, resolution=64, radius=8, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - num_images: Integer specifying the number of images in the dataset.\n",
    "        - transform: Optional torchvision transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        self.num_images = num_images\n",
    "        self.shapes = ['triangle', 'circle', 'square']\n",
    "        self.canvas_size = resolution\n",
    "        self.transform = transform\n",
    "        self.shape_to_idx = {'triangle': 0, 'circle': 1, 'square': 2}\n",
    "        self.idx_to_shape = {0: 'triangle', 1: 'circle', 2: 'square'}\n",
    "        self.radius = radius\n",
    "\n",
    "        # Spatial relationship phrases for variety\n",
    "        self.spatial_phrases = {\n",
    "            'upper_left': ['to the upper left of', 'above and to the left of', 'diagonally up and left from'],\n",
    "            'upper_right': ['to the upper right of', 'above and to the right of', 'diagonally up and right from'],\n",
    "            'lower_left': ['to the lower left of', 'below and to the left of', 'diagonally down and left from'],\n",
    "            'lower_right': ['to the lower right of', 'below and to the right of', 'diagonally down and right from'],\n",
    "            'above': ['above', 'directly above', 'higher than'],\n",
    "            'below': ['below', 'directly below', 'lower than'],\n",
    "            'left': ['to the left of', 'left of', 'left'],\n",
    "            'right': ['to the right of', 'right of', 'right']\n",
    "        }\n",
    "\n",
    "    def generate_caption(self, shape1_idx, shape2_idx, loc1, loc2):\n",
    "        \"\"\"\n",
    "        Generates a natural language caption describing the spatial relationship between two shapes.\n",
    "        \n",
    "        Parameters:\n",
    "        - shape1_idx: Index of first shape\n",
    "        - shape2_idx: Index of second shape\n",
    "        - loc1: Coordinates of first shape (x, y)\n",
    "        - loc2: Coordinates of second shape (x, y)\n",
    "        \n",
    "        Returns:\n",
    "        - string: A natural language caption describing the scene\n",
    "        \"\"\"\n",
    "        # Get shape names\n",
    "        shape1_name = self.idx_to_shape[shape1_idx]\n",
    "        shape2_name = self.idx_to_shape[shape2_idx]\n",
    "        \n",
    "        # Get coordinates\n",
    "        x1, y1 = loc1\n",
    "        x2, y2 = loc2\n",
    "        \n",
    "        # Calculate position differences\n",
    "        dx = x1 - x2  # Positive means shape1 is to the right\n",
    "        dy = y1 - y2  # Positive means shape1 is lower\n",
    "        \n",
    "        # Define thresholds for \"directly\" above/below/left/right\n",
    "        threshold = 5  # pixels\n",
    "        \n",
    "        # Determine spatial relationship\n",
    "        if abs(dx) <= threshold:  # Roughly aligned vertically\n",
    "            if dy < 0:\n",
    "                relation = random.choice(self.spatial_phrases['above'])\n",
    "            else:\n",
    "                relation = random.choice(self.spatial_phrases['below'])\n",
    "        elif abs(dy) <= threshold:  # Roughly aligned horizontally\n",
    "            if dx < 0:\n",
    "                relation = random.choice(self.spatial_phrases['left'])\n",
    "            else:\n",
    "                relation = random.choice(self.spatial_phrases['right'])\n",
    "        else:  # Diagonal relationship\n",
    "            if dx < 0 and dy < 0:\n",
    "                relation = random.choice(self.spatial_phrases['upper_left'])\n",
    "            elif dx < 0 and dy > 0:\n",
    "                relation = random.choice(self.spatial_phrases['lower_left'])\n",
    "            elif dx > 0 and dy < 0:\n",
    "                relation = random.choice(self.spatial_phrases['upper_right'])\n",
    "            else:  # dx > 0 and dy > 0\n",
    "                relation = random.choice(self.spatial_phrases['lower_right'])\n",
    "        \n",
    "        # Construct caption\n",
    "        caption = f\"{shape1_name} is {relation} {shape2_name}\"\n",
    "        return caption\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def draw_shape_on_image(self, img, shape, location, color='black'):\n",
    "        \"\"\"\n",
    "        Draws a specified shape at a given location on the provided image.\n",
    "\n",
    "        Parameters:\n",
    "        - img: PIL Image object to draw on.\n",
    "        - shape: String specifying the shape ('triangle', 'circle', 'square').\n",
    "        - location: Tuple (x, y) specifying the location of the shape's center.\n",
    "\n",
    "        Returns:\n",
    "        - img: PIL Image object with the shape drawn on it.\n",
    "        \"\"\"\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        x, y = location\n",
    "\n",
    "        if shape == 'circle':\n",
    "            r = self.radius  # Radius\n",
    "            leftUpPoint = (x - r, y - r)\n",
    "            rightDownPoint = (x + r, y + r)\n",
    "            draw.ellipse([leftUpPoint, rightDownPoint], fill=color)\n",
    "\n",
    "        elif shape == 'square':\n",
    "            s = self.radius * 2  # Side length\n",
    "            leftUpPoint = (x - s // 2, y - s // 2)\n",
    "            rightDownPoint = (x + s // 2, y + s // 2)\n",
    "            draw.rectangle([leftUpPoint, rightDownPoint], fill=color)\n",
    "\n",
    "        elif shape == 'triangle':\n",
    "            s = self.radius * 2  # Side length\n",
    "            h = s * (3 ** 0.5) / 2  # Height of the equilateral triangle\n",
    "            point1 = (x, y - h / 3)\n",
    "            point2 = (x - s / 2, y + h * 2 / 3)\n",
    "            point3 = (x + s / 2, y + h * 2 / 3)\n",
    "            draw.polygon([point1, point2, point3], fill=color)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Shape must be 'triangle', 'circle', or 'square'.\")\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one image and its labels.\n",
    "\n",
    "        Parameters:\n",
    "        - idx: Index of the image (not used as images are generated on-the-fly).\n",
    "\n",
    "        Returns:\n",
    "        - img: Tensor representing the image.\n",
    "        - labels: Dictionary containing the shapes and locations of the objects.\n",
    "        \"\"\"\n",
    "        # Create a blank image\n",
    "        img = Image.new('RGB', (self.canvas_size, self.canvas_size), 'gray')\n",
    "\n",
    "        # Randomly select two shapes, make sure they are different\n",
    "        shape1 = random.choice(self.shapes)\n",
    "        while True:\n",
    "            shape2 = random.choice(self.shapes)\n",
    "            if shape1 != shape2:\n",
    "                break\n",
    "\n",
    "        # Randomly select locations\n",
    "        x1 = random.randint(self.radius + 1, self.canvas_size - self.radius - 1)\n",
    "        y1 = random.randint(self.radius + 1, self.canvas_size - self.radius - 1)\n",
    "        x2 = random.randint(self.radius + 1, self.canvas_size - self.radius - 1)\n",
    "        y2 = random.randint(self.radius + 1, self.canvas_size - self.radius - 1)\n",
    "\n",
    "        # Randomly decide drawing order to allow overlapping\n",
    "        if random.random() < 0.5:\n",
    "            img = self.draw_shape_on_image(img, shape1, (x1, y1), color=\"red\")\n",
    "            img = self.draw_shape_on_image(img, shape2, (x2, y2), color=\"blue\")\n",
    "            shapes_order = [shape1, shape2]\n",
    "            locations_order = [(x1, y1), (x2, y2)]\n",
    "        else:\n",
    "            img = self.draw_shape_on_image(img, shape2, (x2, y2), color=\"blue\")\n",
    "            img = self.draw_shape_on_image(img, shape1, (x1, y1), color=\"red\")\n",
    "            shapes_order = [shape2, shape1]\n",
    "            locations_order = [(x2, y2), (x1, y1)]\n",
    "\n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = transforms.ToTensor()(img)\n",
    "\n",
    "        # Encode labels and generate caption\n",
    "        shape1_idx = self.shape_to_idx[shapes_order[0]]\n",
    "        shape2_idx = self.shape_to_idx[shapes_order[1]]\n",
    "        location1 = torch.tensor(locations_order[0], dtype=torch.float32)\n",
    "        location2 = torch.tensor(locations_order[1], dtype=torch.float32)\n",
    "        \n",
    "        caption = self.generate_caption(shape1_idx, shape2_idx, locations_order[0], locations_order[1])\n",
    "\n",
    "        labels = {\n",
    "            'shape1': shape1_idx,\n",
    "            'location1': location1,\n",
    "            'shape2': shape2_idx,\n",
    "            'location2': location2,\n",
    "            'caption': caption\n",
    "        }\n",
    "\n",
    "        return img, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Mixed Object Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedShapesDataset(Dataset):\n",
    "    def __init__(self, single_dataset, double_dataset, single_ratio=0.3, total_length=None):\n",
    "        \"\"\" \n",
    "        \"\"\"\n",
    "\n",
    "        self.single_dataset = single_dataset\n",
    "        self.double_dataset = double_dataset\n",
    "        self.single_ratio = single_ratio \n",
    "\n",
    "        if total_length is None:\n",
    "            self.length = len(single_dataset) + len(double_dataset)\n",
    "        else:\n",
    "            self.length = total_length \n",
    "\n",
    "        num_single = int(self.length * self.single_ratio)\n",
    "        num_double = self.length - num_single\n",
    "        self.sample_types = ['single'] * num_single + ['double'] * num_double\n",
    "        random.shuffle(self.sample_types)\n",
    "\n",
    "        self.single_indices = list(range(len(single_dataset)))\n",
    "        self.double_indices = list(range(len(double_dataset)))\n",
    "        random.shuffle(self.single_indices)\n",
    "        random.shuffle(self.double_dataset)\n",
    "\n",
    "        self.single_ptr = 0\n",
    "        self.double_ptr = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_type = self.sample_types[idx]\n",
    "        if sample_type == 'single':\n",
    "            if self.single_ptr >= len(self.single_indices):\n",
    "                self.single_ptr = 0\n",
    "                random.shuffle(self.single_indices)\n",
    "            img, labels = self.single_dataset[self.single_indices[self.single_ptr]]\n",
    "            self.single_ptr += 1\n",
    "\n",
    "         # Convert to double-object format for consistency\n",
    "            new_labels = {\n",
    "                'shape1': labels['shape'],\n",
    "                'location1': labels['location'],\n",
    "                'shape2': None,\n",
    "                'location2': None,\n",
    "                'caption': labels['caption'],\n",
    "                'type': 'single'\n",
    "            }\n",
    "            return img, new_labels\n",
    "        else:\n",
    "            if self.double_ptr >= len(self.double_indices):\n",
    "                self.double_ptr = 0\n",
    "                random.shuffle(self.double_indices)\n",
    "            img, labels = self.double_dataset[self.double_indices[self.double_ptr]]\n",
    "            self.double_ptr += 1\n",
    "\n",
    "            #Adding type field for consistency \n",
    "            labels['type'] = 'double'\n",
    "            return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset into a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/home12/hjkim/Github/DiffusionObjectRelation/PixArt-alpha\n",
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:22<00:00, 11.27s/it]\n",
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 944046.46it/s]\n",
      "Extracting Single Image Resolution 128\n",
      "An error occurred while trying to fetch /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/sd-vae-ft-ema: Error no file named diffusion_pytorch_model.safetensors found in directory /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/sd-vae-ft-ema.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "100%|████████████████████████████████████| 10000/10000 [00:59<00:00, 167.01it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd ../PixArt-alpha\n",
    "!python ~/Github/DiffusionObjectRelation/PixArt-alpha/tools/extract_features.py \\\n",
    "    --img_size 128 \\\n",
    "    --max_tokens 20 \\\n",
    "    --dataset_root \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRelSingle_pilot1\" \\\n",
    "    --json_path \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRelSingle_pilot1/partition/data_info.json\" \\\n",
    "    --t5_save_root \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRelSingle_pilot1/caption_feature_wmask\" \\\n",
    "    --vae_save_root \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRelSingle_pilot1/img_vae_features\" \\\n",
    "    --pretrained_models_dir \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1G\t/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2/caption_feature_wmask\n",
      "40M\t/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2/captions\n",
      "40M\t/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2/images\n",
      "118M\t/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2/img_vae_features_128resolution\n",
      "1.2M\t/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2/partition\n"
     ]
    }
   ],
   "source": [
    "!du -sh /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/n/home12/hjkim/.conda/envs/torch2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:16<00:00,  8.12s/it]\n",
      "100%|█████████████████████████████████| 10001/10001 [00:00<00:00, 963573.25it/s]\n",
      "Extracting Single Image Resolution 128\n",
      "An error occurred while trying to fetch /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/sd-vae-ft-ema: Error no file named diffusion_pytorch_model.safetensors found in directory /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/sd-vae-ft-ema.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "100%|████████████████████████████████████| 10001/10001 [00:23<00:00, 428.91it/s]\n"
     ]
    }
   ],
   "source": [
    "!python ~/Github/DiffusionObjectRelation/PixArt-alpha/tools/extract_features.py \\\n",
    "    --img_size 128 \\\n",
    "    --dataset_root \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot\" \\\n",
    "    --json_path \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot/partition/data_info.json\" \\\n",
    "    --t5_save_root \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot/caption_feature_wmask\" \\\n",
    "    --vae_save_root \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot/img_vae_features\" \\\n",
    "    --pretrained_models_dir \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixArt-XL-2-1024-MS.pth  PixArt-XL-2-512x512.pth  sd-vae-ft-ema  t5_ckpts\n"
     ]
    }
   ],
   "source": [
    "!ls /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

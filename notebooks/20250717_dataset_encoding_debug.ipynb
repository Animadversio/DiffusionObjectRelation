{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion.model.t5 import T5Embedder\n",
    "from os.path import join\n",
    "import torch as th\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_models_dir = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models\"\n",
    "t5_embedder = T5Embedder(device=\"cuda\", local_cache=True, cache_dir=f'{pretrained_models_dir}/t5_ckpts', model_max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6234b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking images...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.png', '1.png', '10.png', '100.png', '1000.png', '1001.png', '1002.png', '1003.png', '1004.png', '1005.png']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ 0.png:  content matches (hash: 3083128f...)\n",
      "✓ 1.png:  content matches (hash: 003b9ece...)\n",
      "✓ 10.png:  content matches (hash: f9e1efdb...)\n",
      "✓ 100.png:  content matches (hash: 3e255d52...)\n",
      "✓ 1000.png:  content matches (hash: 57109c7f...)\n",
      "\n",
      "Checking captions...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.txt', '1.txt', '10.txt', '100.txt', '1000.txt', '1001.txt', '1002.txt', '1003.txt', '1004.txt', '1005.txt']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ 0.txt:  content matches (hash: a1a35875...)\n",
      "✓ 1.txt:  content matches (hash: 41fef664...)\n",
      "✓ 10.txt:  content matches (hash: 7dbc8465...)\n",
      "✓ 100.txt:  content matches (hash: 600791fe...)\n",
      "✓ 1000.txt:  content matches (hash: 03993e85...)\n",
      "\n",
      "Checking img_vae_features_128resolution/noflip...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.npy', '1.npy', '10.npy', '100.npy', '1000.npy', '1001.npy', '1002.npy', '1003.npy', '1004.npy', '1005.npy']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ 0.npy:  content matches (hash: cbd685f2...)\n",
      "✓ 1.npy:  content matches (hash: 89655baf...)\n",
      "✓ 10.npy:  content matches (hash: 97b845af...)\n",
      "✓ 100.npy:  content matches (hash: 551642c0...)\n",
      "✓ 1000.npy:  content matches (hash: 4e74379b...)\n",
      "\n",
      "Checking partition...\n",
      "First 10 files in both datasets are identical:\n",
      "['data_info.json']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ data_info.json:  content matches (hash: 9c4787a7...)\n",
      "\n",
      "Checking caption_feature_wmask...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.npz', '1.npz', '10.npz', '100.npz', '1000.npz', '1001.npz', '1002.npz', '1003.npz', '1004.npz', '1005.npz']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✗ 0.npz:  content differs\n",
      "  Dataset1 hash: 428ce1cfe342df0c61e09c0f60929012\n",
      "  Dataset2 hash: 2deb3f08c64154b27220348d62b86ca6\n",
      "✗ 1.npz:  content differs\n",
      "  Dataset1 hash: dd509fa69a313a06b66ea39ab55351c7\n",
      "  Dataset2 hash: b55f0d807ea4b06602cfcc0b491a5961\n",
      "✗ 10.npz:  content differs\n",
      "  Dataset1 hash: 32246addcc407083c8cb4580ce5f0ebf\n",
      "  Dataset2 hash: 52155f1a7d8f77bd3cf976c43b7a9a2c\n",
      "✗ 100.npz:  content differs\n",
      "  Dataset1 hash: be3131ca073e76cc61ba6f82982692df\n",
      "  Dataset2 hash: 0907fcf2ad11d279321be44a9367e643\n",
      "✗ 1000.npz:  content differs\n",
      "  Dataset1 hash: e9eebbe859a67015747342c5704f2f39\n",
      "  Dataset2 hash: b2eb221098cd3c92995eca02760c85fd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "dataset_root1 = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2\"\n",
    "dataset_root2 = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_rndembposemb\"\n",
    "\n",
    "subfolder = \"img_vae_features_128resolution/noflip\" #\"images\"\n",
    "for subfolder in [\"images\",\n",
    "                  \"captions\",\n",
    "                  \"img_vae_features_128resolution/noflip\",\n",
    "                  \"partition\",\n",
    "                  \"caption_feature_wmask\"]:\n",
    "    print(f\"\\nChecking {subfolder}...\")\n",
    "    # List files in images subfolder for both datasets\n",
    "    files1 = sorted(os.listdir(join(dataset_root1, subfolder)))\n",
    "    files2 = sorted(os.listdir(join(dataset_root2, subfolder)))\n",
    "\n",
    "    # Sort the lists to ensure consistent ordering\n",
    "    files1.sort()\n",
    "    files2.sort()\n",
    "    # Assert that the first few files are the same\n",
    "    assert files1[:10] == files2[:10], \"First 10 files in  subfolder are not the same\"\n",
    "    print(\"First 10 files in both datasets are identical:\")\n",
    "    print(files1[:10])\n",
    "    # Check if the content of corresponding image files matches using hash comparison\n",
    "    print(\"\\nChecking content of first 5  files...\")\n",
    "    for i in range(min(5, len(files1), len(files2))):\n",
    "        file1_path = join(dataset_root1, subfolder, files1[i])\n",
    "        file2_path = join(dataset_root2, subfolder, files2[i])\n",
    "        \n",
    "        # Read files in binary mode and compute hashes\n",
    "        with open(file1_path, 'rb') as f1:\n",
    "            hash1 = hashlib.md5(f1.read()).hexdigest()\n",
    "        \n",
    "        with open(file2_path, 'rb') as f2:\n",
    "            hash2 = hashlib.md5(f2.read()).hexdigest()\n",
    "        \n",
    "        if hash1 == hash2:\n",
    "            print(f\"✓ {files1[i]}:  content matches (hash: {hash1[:8]}...)\")\n",
    "        else:\n",
    "            print(f\"✗ {files1[i]}:  content differs\")\n",
    "            print(f\"  Dataset1 hash: {hash1}\")\n",
    "            print(f\"  Dataset2 hash: {hash2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking images...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.png', '1.png', '10.png', '100.png', '1000.png', '1001.png', '1002.png', '1003.png', '1004.png', '1005.png']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ 0.png:  content matches (hash: 3083128f...)\n",
      "✓ 1.png:  content matches (hash: 003b9ece...)\n",
      "✓ 10.png:  content matches (hash: f9e1efdb...)\n",
      "✓ 100.png:  content matches (hash: 3e255d52...)\n",
      "✓ 1000.png:  content matches (hash: 57109c7f...)\n",
      "\n",
      "Checking captions...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.txt', '1.txt', '10.txt', '100.txt', '1000.txt', '1001.txt', '1002.txt', '1003.txt', '1004.txt', '1005.txt']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ 0.txt:  content matches (hash: a1a35875...)\n",
      "✓ 1.txt:  content matches (hash: 41fef664...)\n",
      "✓ 10.txt:  content matches (hash: 7dbc8465...)\n",
      "✓ 100.txt:  content matches (hash: 600791fe...)\n",
      "✓ 1000.txt:  content matches (hash: 03993e85...)\n",
      "\n",
      "Checking img_vae_features_128resolution/noflip...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.npy', '1.npy', '10.npy', '100.npy', '1000.npy', '1001.npy', '1002.npy', '1003.npy', '1004.npy', '1005.npy']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ 0.npy:  content matches (hash: cbd685f2...)\n",
      "✓ 1.npy:  content matches (hash: 89655baf...)\n",
      "✓ 10.npy:  content matches (hash: 97b845af...)\n",
      "✓ 100.npy:  content matches (hash: 551642c0...)\n",
      "✓ 1000.npy:  content matches (hash: 4e74379b...)\n",
      "\n",
      "Checking partition...\n",
      "First 10 files in both datasets are identical:\n",
      "['data_info.json']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ data_info.json:  content matches (hash: 9c4787a7...)\n",
      "\n",
      "Checking caption_feature_wmask...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.npz', '1.npz', '10.npz', '100.npz', '1000.npz', '1001.npz', '1002.npz', '1003.npz', '1004.npz', '1005.npz']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ 0.npz:  content matches (hash: 428ce1cf...)\n",
      "✓ 1.npz:  content matches (hash: dd509fa6...)\n",
      "✓ 10.npz:  content matches (hash: 32246add...)\n",
      "✓ 100.npz:  content matches (hash: be3131ca...)\n",
      "✓ 1000.npz:  content matches (hash: e9eebbe8...)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "dataset_root1 = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2\"\n",
    "dataset_root2 = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_rndemb\"\n",
    "\n",
    "subfolder = \"img_vae_features_128resolution/noflip\" #\"images\"\n",
    "for subfolder in [\"images\",\n",
    "                  \"captions\",\n",
    "                  \"img_vae_features_128resolution/noflip\",\n",
    "                  \"partition\",\n",
    "                  \"caption_feature_wmask\"]:\n",
    "    print(f\"\\nChecking {subfolder}...\")\n",
    "    # List files in images subfolder for both datasets\n",
    "    files1 = sorted(os.listdir(join(dataset_root1, subfolder)))\n",
    "    files2 = sorted(os.listdir(join(dataset_root2, subfolder)))\n",
    "\n",
    "    # Sort the lists to ensure consistent ordering\n",
    "    files1.sort()\n",
    "    files2.sort()\n",
    "    # Assert that the first few files are the same\n",
    "    assert files1[:10] == files2[:10], \"First 10 files in  subfolder are not the same\"\n",
    "    print(\"First 10 files in both datasets are identical:\")\n",
    "    print(files1[:10])\n",
    "    # Check if the content of corresponding image files matches using hash comparison\n",
    "    print(\"\\nChecking content of first 5  files...\")\n",
    "    for i in range(min(5, len(files1), len(files2))):\n",
    "        file1_path = join(dataset_root1, subfolder, files1[i])\n",
    "        file2_path = join(dataset_root2, subfolder, files2[i])\n",
    "        \n",
    "        # Read files in binary mode and compute hashes\n",
    "        with open(file1_path, 'rb') as f1:\n",
    "            hash1 = hashlib.md5(f1.read()).hexdigest()\n",
    "        \n",
    "        with open(file2_path, 'rb') as f2:\n",
    "            hash2 = hashlib.md5(f2.read()).hexdigest()\n",
    "        \n",
    "        if hash1 == hash2:\n",
    "            print(f\"✓ {files1[i]}:  content matches (hash: {hash1[:8]}...)\")\n",
    "        else:\n",
    "            print(f\"✗ {files1[i]}:  content differs\")\n",
    "            print(f\"  Dataset1 hash: {hash1}\")\n",
    "            print(f\"  Dataset2 hash: {hash2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root1 = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_T5\"\n",
    "with open(join(dataset_root1, \"captions\", \"100.txt\"), 'r') as f:\n",
    "    text = f.read()\n",
    "data = np.load(join(dataset_root1, \"caption_feature_wmask\", \"100.npz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca6497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"caption_feature\"]\n",
    "data[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_emb3, emb_mask3 = t5_embedder.get_text_embeddings([text.strip()])\n",
    "assert caption_emb3.shape == (1, 20, 4096)\n",
    "assert emb_mask3.shape == (1, 20)\n",
    "assert np.allclose(caption_emb3.float().cpu().data.numpy(), data[\"caption_feature\"])\n",
    "assert np.allclose(emb_mask3.cpu().data.numpy(), data[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcda572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking images...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.png', '1.png', '10.png', '100.png', '1000.png', '1001.png', '1002.png', '1003.png', '1004.png', '1005.png']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ 0.png:  content matches (hash: 3083128f...)\n",
      "✓ 1.png:  content matches (hash: 003b9ece...)\n",
      "✓ 10.png:  content matches (hash: f9e1efdb...)\n",
      "✓ 100.png:  content matches (hash: 3e255d52...)\n",
      "✓ 1000.png:  content matches (hash: 57109c7f...)\n",
      "\n",
      "Checking captions...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.txt', '1.txt', '10.txt', '100.txt', '1000.txt', '1001.txt', '1002.txt', '1003.txt', '1004.txt', '1005.txt']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ 0.txt:  content matches (hash: a1a35875...)\n",
      "✓ 1.txt:  content matches (hash: 41fef664...)\n",
      "✓ 10.txt:  content matches (hash: 7dbc8465...)\n",
      "✓ 100.txt:  content matches (hash: 600791fe...)\n",
      "✓ 1000.txt:  content matches (hash: 03993e85...)\n",
      "\n",
      "Checking img_vae_features_128resolution/noflip...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.npy', '1.npy', '10.npy', '100.npy', '1000.npy', '1001.npy', '1002.npy', '1003.npy', '1004.npy', '1005.npy']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ 0.npy:  content matches (hash: cbd685f2...)\n",
      "✓ 1.npy:  content matches (hash: 89655baf...)\n",
      "✓ 10.npy:  content matches (hash: 97b845af...)\n",
      "✓ 100.npy:  content matches (hash: 551642c0...)\n",
      "✓ 1000.npy:  content matches (hash: 4e74379b...)\n",
      "\n",
      "Checking partition...\n",
      "First 10 files in both datasets are identical:\n",
      "['data_info.json']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ data_info.json:  content matches (hash: 9c4787a7...)\n",
      "\n",
      "Checking caption_feature_wmask...\n",
      "First 10 files in both datasets are identical:\n",
      "['0.npz', '1.npz', '10.npz', '100.npz', '1000.npz', '1001.npz', '1002.npz', '1003.npz', '1004.npz', '1005.npz']\n",
      "\n",
      "Checking content of first 5  files...\n",
      "✓ 0.npz:  content matches (hash: 2deb3f08...)\n",
      "✓ 1.npz:  content matches (hash: b55f0d80...)\n",
      "✓ 10.npz:  content matches (hash: 52155f1a...)\n",
      "✓ 100.npz:  content matches (hash: 0907fcf2...)\n",
      "✓ 1000.npz:  content matches (hash: b2eb2210...)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "dataset_root1 = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_T5\"\n",
    "dataset_root2 = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_rndembposemb\"\n",
    "\n",
    "subfolder = \"img_vae_features_128resolution/noflip\" #\"images\"\n",
    "for subfolder in [\"images\",\n",
    "                  \"captions\",\n",
    "                  \"img_vae_features_128resolution/noflip\",\n",
    "                  \"partition\",\n",
    "                  \"caption_feature_wmask\"]:\n",
    "    print(f\"\\nChecking {subfolder}...\")\n",
    "    # List files in images subfolder for both datasets\n",
    "    files1 = sorted(os.listdir(join(dataset_root1, subfolder)))\n",
    "    files2 = sorted(os.listdir(join(dataset_root2, subfolder)))\n",
    "\n",
    "    # Sort the lists to ensure consistent ordering\n",
    "    files1.sort()\n",
    "    files2.sort()\n",
    "    # Assert that the first few files are the same\n",
    "    assert files1[:10] == files2[:10], \"First 10 files in  subfolder are not the same\"\n",
    "    print(\"First 10 files in both datasets are identical:\")\n",
    "    print(files1[:10])\n",
    "    # Check if the content of corresponding image files matches using hash comparison\n",
    "    print(\"\\nChecking content of first 5  files...\")\n",
    "    for i in range(min(5, len(files1), len(files2))):\n",
    "        file1_path = join(dataset_root1, subfolder, files1[i])\n",
    "        file2_path = join(dataset_root2, subfolder, files2[i])\n",
    "        \n",
    "        # Read files in binary mode and compute hashes\n",
    "        with open(file1_path, 'rb') as f1:\n",
    "            hash1 = hashlib.md5(f1.read()).hexdigest()\n",
    "        \n",
    "        with open(file2_path, 'rb') as f2:\n",
    "            hash2 = hashlib.md5(f2.read()).hexdigest()\n",
    "        \n",
    "        if hash1 == hash2:\n",
    "            print(f\"✓ {files1[i]}:  content matches (hash: {hash1[:8]}...)\")\n",
    "        else:\n",
    "            print(f\"✗ {files1[i]}:  content differs\")\n",
    "            print(f\"  Dataset1 hash: {hash1}\")\n",
    "            print(f\"  Dataset2 hash: {hash2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d42bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.1510793 , -0.00401133, -0.03006095, ...,  0.02540384,\n",
       "          0.13742766,  0.07499641],\n",
       "        [ 0.21941254, -0.04772367, -0.03246624, ...,  0.19155434,\n",
       "          0.20113255,  0.06196149],\n",
       "        [-0.15288968,  0.17000431,  0.02862656, ...,  0.29570985,\n",
       "          0.16181944,  0.05648008],\n",
       "        ...,\n",
       "        [ 0.22581038,  0.1742911 ,  0.1055528 , ..., -0.01644292,\n",
       "          0.11034603, -0.00138784],\n",
       "        [ 0.22581038,  0.1742911 ,  0.1055528 , ..., -0.01644292,\n",
       "          0.11034603, -0.00138784],\n",
       "        [ 0.22581038,  0.1742911 ,  0.1055528 , ..., -0.01644292,\n",
       "          0.11034603, -0.00138784]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"caption_feature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.08056641, -0.16210938,  0.06542969, ...,  0.01361084,\n",
       "          0.00848389,  0.00994873],\n",
       "        [ 0.03930664,  0.01806641, -0.19628906, ...,  0.00415039,\n",
       "         -0.12109375,  0.01409912],\n",
       "        [ 0.00077438, -0.00254822, -0.203125  , ..., -0.05322266,\n",
       "         -0.01989746,  0.15917969],\n",
       "        ...,\n",
       "        [ 0.37695312, -0.07666016,  0.04931641, ..., -0.14453125,\n",
       "          0.04467773,  0.21582031],\n",
       "        [ 0.10449219, -0.04614258, -0.00549316, ..., -0.10888672,\n",
       "          0.09765625,  0.23242188],\n",
       "        [ 0.13671875,  0.00570679, -0.15234375, ..., -0.19921875,\n",
       "         -0.01416016,  0.14550781]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "caption_emb3.float().cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"caption_feature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01280f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.08056641, -0.16210938,  0.06542969, ...,  0.01361084,\n",
       "          0.00848389,  0.00994873],\n",
       "        [ 0.03930664,  0.01806641, -0.19628906, ...,  0.00415039,\n",
       "         -0.12109375,  0.01409912],\n",
       "        [ 0.00077438, -0.00254822, -0.203125  , ..., -0.05322266,\n",
       "         -0.01989746,  0.15917969],\n",
       "        ...,\n",
       "        [ 0.37695312, -0.07666016,  0.04931641, ..., -0.14453125,\n",
       "          0.04467773,  0.21582031],\n",
       "        [ 0.10449219, -0.04614258, -0.00549316, ..., -0.10888672,\n",
       "          0.09765625,  0.23242188],\n",
       "        [ 0.13671875,  0.00570679, -0.15234375, ..., -0.19921875,\n",
       "         -0.01416016,  0.14550781]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "caption_emb3.float().cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef63e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.02177385, -0.0469401 ,  0.05691458, ..., -0.1136976 ,\n",
       "         -0.24676234,  0.10760345],\n",
       "        [ 0.177893  ,  0.06684501, -0.04189382, ...,  0.09979867,\n",
       "         -0.13998131, -0.01908986],\n",
       "        [ 0.21941254, -0.04772367, -0.03246624, ...,  0.19155434,\n",
       "          0.20113255,  0.06196149],\n",
       "        ...,\n",
       "        [ 0.22581038,  0.1742911 ,  0.1055528 , ..., -0.01644292,\n",
       "          0.11034603, -0.00138784],\n",
       "        [ 0.22581038,  0.1742911 ,  0.1055528 , ..., -0.01644292,\n",
       "          0.11034603, -0.00138784],\n",
       "        [ 0.22581038,  0.1742911 ,  0.1055528 , ..., -0.01644292,\n",
       "          0.11034603, -0.00138784]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"caption_feature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62459082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.6894531e-01, -7.7819824e-04, -5.4016113e-03, ...,\n",
       "          7.4707031e-02, -1.7871094e-01, -4.2724609e-02],\n",
       "        [-1.2084961e-02,  4.1503906e-03,  8.8867188e-02, ...,\n",
       "          1.1718750e-01, -1.2011719e-01, -8.9355469e-02],\n",
       "        [ 3.5644531e-02,  5.8349609e-02, -6.4453125e-02, ...,\n",
       "          3.8574219e-02, -1.4453125e-01, -9.5367432e-05],\n",
       "        ...,\n",
       "        [ 1.7968750e-01,  9.4726562e-02, -2.4902344e-01, ...,\n",
       "          7.0312500e-02, -1.0449219e-01,  1.7285156e-01],\n",
       "        [ 2.6953125e-01,  9.6679688e-02, -3.4960938e-01, ...,\n",
       "          6.7382812e-02, -4.4555664e-03,  2.1289062e-01],\n",
       "        [ 2.1777344e-01,  1.3867188e-01, -3.8867188e-01, ...,\n",
       "          4.1015625e-02, -1.3061523e-02,  2.3925781e-01]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb3.cpu().float().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e67d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r objectRel_pilot_rndembposemb objectRel_pilot_T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ffc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ~/Github/DiffusionObjectRelation/PixArt-alpha/tools/extract_features.py \\\n",
    "    --img_size 128 \\\n",
    "    --max_tokens 20 \\\n",
    "    --dataset_root $STORE_DIR\"/DL_Projects/PixArt/objectRel_pilot_T5\" \\\n",
    "    --json_path $STORE_DIR\"/DL_Projects/PixArt/objectRel_pilot_T5/partition/data_info.json\" \\\n",
    "    --t5_save_root $STORE_DIR\"/DL_Projects/PixArt/objectRel_pilot_T5/caption_feature_wmask\" \\\n",
    "    --vae_save_root $STORE_DIR\"/DL_Projects/PixArt/objectRel_pilot_T5/img_vae_features\" \\\n",
    "    --pretrained_models_dir $STORE_DIR\"/DL_Projects/PixArt/output/pretrained_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captions  images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_T5"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

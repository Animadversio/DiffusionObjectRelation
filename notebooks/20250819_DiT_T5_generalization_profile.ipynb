{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e22e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b7c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/xformers/ops/swiglu_op.py:128: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(cls, ctx, x, w1, b1, w2, b2, w3, b3):\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/xformers/ops/swiglu_op.py:149: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(cls, ctx, dx5):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "import torch as th\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.auto import trange\n",
    "from contextlib import redirect_stdout\n",
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/DiffusionObjectRelation/PixArt-alpha\")\n",
    "from diffusion.model.builder import build_model\n",
    "from diffusion.utils.misc import set_random_seed, read_config, init_random_seed, DebugUnderflowOverflow\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/DiffusionObjectRelation\")\n",
    "from utils.pixart_sampling_utils import pipeline_inference_custom, \\\n",
    "    PixArtAlphaPipeline_custom\n",
    "from utils.pixart_utils import state_dict_convert\n",
    "from utils.pixart_utils import construct_diffuser_pipeline_from_config\n",
    "from utils.cv2_eval_utils import find_classify_objects, evaluate_parametric_relation, eval_func_factory, scene_info_collection\n",
    "from utils.cv2_eval_utils import evaluate_pipeline_on_prompts, print_evaluation_summary\n",
    "from utils.relation_shape_dataset_lib import ShapesDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458e698",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b150ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run_name = \"objrel_T5_DiT_B_pilot\" # \"objrel_rndembdposemb_DiT_B_pilot\" \n",
    "# model_run_name = \"objrel_T5_DiT_mini_pilot_WDecay\" # \"objrel_rndembdposemb_DiT_B_pilot\" \n",
    "# ckpt_name = \"epoch_4000_step_160000.pth\" # \"epoch_4000_step_160000.pth\" \n",
    "ckpt_name = \"epoch_1500_step_60000.pth\" # \"epoch_4000_step_160000.pth\" \n",
    "text_encoder_type = \"T5\" \n",
    "suffix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc6aba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f83f07f15f4ec6b18bb020ebdcd638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected types for transformer: (<class 'diffusers.models.transformers.pixart_transformer_2d.PixArtTransformer2DModel'>,), got <class 'diffusers.models.transformers.transformer_2d.Transformer2DModel'>.\n"
     ]
    }
   ],
   "source": [
    "savedir = f\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/results/{model_run_name}\"\n",
    "# savedir = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/results/objrel_rndembdposemb_DiT_B_pilot\"\n",
    "config = read_config(join(savedir, 'config.py'))\n",
    "# Build pipeline from config\n",
    "pipeline = construct_diffuser_pipeline_from_config(config, pipeline_class=PixArtAlphaPipeline_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd38809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3823d66bf69420b8e8c7d1df2042c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_feat_dir_old = '/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/training_datasets/random_emd_dictionary'\n",
    "T5_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_path, )#subfolder=\"tokenizer\")\n",
    "if text_encoder_type == \"T5\":\n",
    "    text_encoder = T5EncoderModel.from_pretrained(T5_path, load_in_8bit=False, torch_dtype=torch.bfloat16, )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bce592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_750701/2026684612.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(join(ckptdir, ckpt_name))\n"
     ]
    }
   ],
   "source": [
    "ckptdir = join(savedir, \"checkpoints\")\n",
    "ckpt_name = \"epoch_1500_step_60000.pth\" # \"epoch_4000_step_160000.pth\" \n",
    "ckpt = torch.load(join(ckptdir, ckpt_name))\n",
    "pipeline.transformer.load_state_dict(state_dict_convert(ckpt['state_dict_ema']))\n",
    "# pipeline.transformer.load_state_dict(state_dict_convert(ckpt['state_dict']))\n",
    "pipeline.tokenizer = tokenizer\n",
    "pipeline.to(device=\"cuda\", dtype=torch.float16);\n",
    "# pipeline.to(device=\"cuda\", dtype=torch.bfloat16);\n",
    "pipeline.text_encoder = text_encoder.to(device=\"cuda\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3acbf",
   "metadata": {},
   "source": [
    "### Generate training set prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0586685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tmp = ShapesDataset(num_images=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4102bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_collection(spatial_phrases, \n",
    "                               prompt_template=\"{color1} {shape1} is {rel_text} {color2} {shape2}\",\n",
    "                               color1=\"blue\", shape1=\"circle\", color2=\"red\", shape2=\"square\"):\n",
    "    prompt_collection = []\n",
    "    scene_info_collection = []\n",
    "    for spatial_relationship, rel_text_collection in spatial_phrases.items():\n",
    "        if spatial_relationship in [\"in_front\", \"behind\"]:\n",
    "            continue\n",
    "        for rel_text in rel_text_collection:\n",
    "            prompt = prompt_template.format(color1=color1, shape1=shape1, rel_text=rel_text, color2=color2, shape2=shape2)\n",
    "            scene_info = {\n",
    "                \"color1\": color1,\n",
    "                \"shape1\": shape1,\n",
    "                \"color2\": color2,\n",
    "                \"shape2\": shape2,\n",
    "                \"spatial_relationship\": spatial_relationship\n",
    "            }\n",
    "            prompt_collection.append(prompt)\n",
    "            scene_info_collection.append(scene_info)\n",
    "    return prompt_collection, scene_info_collection\n",
    "\n",
    "prompt_collection, scene_info_collection = generate_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"{color1} {shape1} is {rel_text} {color2} {shape2}\",\n",
    "                                color1=\"blue\", shape1=\"circle\", color2=\"red\", shape2=\"square\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "127a43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def generate_all_prompt_collection(spatial_phrases, \n",
    "                               prompt_template=\"{color1} {shape1} is {rel_text} {color2} {shape2}\",):\n",
    "    color_list = ['red', 'blue']\n",
    "    shape_list = ['square', 'triangle', 'circle']\n",
    "    prompt_collection = []\n",
    "    scene_info_collection = []\n",
    "    for color1, color2 in product(color_list, color_list):\n",
    "        if color1 == color2:      # skip sameâ€color pairs\n",
    "            continue\n",
    "        for shape1, shape2 in product(shape_list, shape_list):\n",
    "            if shape1 == shape2:\n",
    "                continue\n",
    "            for spatial_relationship, rel_text_collection in spatial_phrases.items():\n",
    "                if spatial_relationship in [\"in_front\", \"behind\"]:\n",
    "                    continue\n",
    "                for rel_text in rel_text_collection:\n",
    "                    prompt = prompt_template.format(color1=color1, shape1=shape1, rel_text=rel_text, color2=color2, shape2=shape2)\n",
    "                    scene_info = {\n",
    "                        \"color1\": color1,\n",
    "                        \"shape1\": shape1,\n",
    "                        \"color2\": color2,\n",
    "                        \"shape2\": shape2,\n",
    "                        \"spatial_relationship\": spatial_relationship\n",
    "                    }\n",
    "                    prompt_collection.append(prompt)\n",
    "                    scene_info_collection.append(scene_info)\n",
    "    return prompt_collection, scene_info_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_collection, scene_info_collection = generate_all_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"{color1} {shape1} is {rel_text} {color2} {shape2}\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6151f856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffcce79",
   "metadata": {},
   "source": [
    "### Epoch 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b90e0a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_750701/21473023.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(join(ckptdir, ckpt_name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_name = \"epoch_1500_step_60000.pth\" # \"epoch_4000_step_160000.pth\" \n",
    "ckpt = torch.load(join(ckptdir, ckpt_name))\n",
    "pipeline.transformer.load_state_dict(state_dict_convert(ckpt['state_dict_ema']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847559e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a0d31a933e419482d0f85cb0f8dc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating prompts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "=== EVALUATION SUMMARY ===\n",
      "Total samples evaluated: 1078\n",
      "Number of prompts: 22\n",
      "\n",
      "--- Overall Performance ---\n",
      "overall                  : 0.734\n",
      "overall_loose            : 0.919\n",
      "shape                    : 0.944\n",
      "color                    : 0.968\n",
      "exist_binding            : 0.932\n",
      "unique_binding           : 0.929\n",
      "spatial_relationship     : 0.734\n",
      "spatial_relationship_loose: 0.919\n",
      "Dx                       : -0.990\n",
      "Dy                       : 0.994\n",
      "x1                       : 66.122\n",
      "y1                       : 60.787\n",
      "x2                       : 67.112\n",
      "y2                       : 59.793\n"
     ]
    }
   ],
   "source": [
    "prompt_collection, scene_info_collection = generate_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"{color1} {shape1} is {rel_text} {color2} {shape2}\",\n",
    "                                color1=\"blue\", shape1=\"circle\", color2=\"red\", shape2=\"square\")\n",
    "\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "eval_df_syn, object_df_syn = evaluate_pipeline_on_prompts(pipeline, prompt_collection, scene_info_collection,\n",
    "                                                          num_images=49, num_inference_steps=14, guidance_scale=4.5,\n",
    "                                                          max_sequence_length=20, generator_seed=42, prompt_dtype=torch.float16,\n",
    "                                                          color_margin=25, spatial_threshold=5, device=\"cuda\")\n",
    "\n",
    "print_evaluation_summary(eval_df_syn, group_by_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "623582e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e12b08d63bb476a8ab3ed3f267d97f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating prompts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "=== EVALUATION SUMMARY ===\n",
      "Total samples evaluated: 1078\n",
      "Number of prompts: 22\n",
      "\n",
      "--- Overall Performance ---\n",
      "overall                  : 0.417\n",
      "overall_loose            : 0.593\n",
      "shape                    : 0.902\n",
      "color                    : 0.912\n",
      "exist_binding            : 0.873\n",
      "unique_binding           : 0.871\n",
      "spatial_relationship     : 0.417\n",
      "spatial_relationship_loose: 0.593\n",
      "Dx                       : -18.688\n",
      "Dy                       : -17.751\n",
      "x1                       : 60.605\n",
      "y1                       : 54.722\n",
      "x2                       : 79.293\n",
      "y2                       : 72.473\n"
     ]
    }
   ],
   "source": [
    "prompt_collection, scene_info_collection = generate_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"{color1} {shape1} {rel_text} {color2} {shape2}\",\n",
    "                                color1=\"blue\", shape1=\"circle\", color2=\"red\", shape2=\"square\")\n",
    "\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "eval_df_syn, object_df_syn = evaluate_pipeline_on_prompts(pipeline, prompt_collection, scene_info_collection,\n",
    "                                                          num_images=49, num_inference_steps=14, guidance_scale=4.5,\n",
    "                                                          max_sequence_length=20, generator_seed=42, prompt_dtype=torch.float16,\n",
    "                                                          color_margin=25, spatial_threshold=5, device=\"cuda\")\n",
    "\n",
    "print_evaluation_summary(eval_df_syn, group_by_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8af9edc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52561db68504e83bb8dd36c31168789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating prompts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "=== EVALUATION SUMMARY ===\n",
      "Total samples evaluated: 1078\n",
      "Number of prompts: 22\n",
      "\n",
      "--- Overall Performance ---\n",
      "overall                  : 0.163\n",
      "overall_loose            : 0.327\n",
      "shape                    : 0.888\n",
      "color                    : 0.999\n",
      "exist_binding            : 0.888\n",
      "unique_binding           : 0.887\n",
      "spatial_relationship     : 0.163\n",
      "spatial_relationship_loose: 0.327\n",
      "Dx                       : -45.138\n",
      "Dy                       : -40.601\n",
      "x1                       : 50.142\n",
      "y1                       : 39.026\n",
      "x2                       : 95.280\n",
      "y2                       : 79.628\n"
     ]
    }
   ],
   "source": [
    "prompt_collection, scene_info_collection = generate_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"{color1} {shape1} {rel_text} the {color2} {shape2}\",\n",
    "                                color1=\"blue\", shape1=\"circle\", color2=\"red\", shape2=\"square\")\n",
    "\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "eval_df_syn, object_df_syn = evaluate_pipeline_on_prompts(pipeline, prompt_collection, scene_info_collection,\n",
    "                                                          num_images=49, num_inference_steps=14, guidance_scale=4.5,\n",
    "                                                          max_sequence_length=20, generator_seed=42, prompt_dtype=torch.float16,\n",
    "                                                          color_margin=25, spatial_threshold=5, device=\"cuda\")\n",
    "\n",
    "print_evaluation_summary(eval_df_syn, group_by_prompt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ff065",
   "metadata": {},
   "source": [
    "### Epoch 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29413c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_750701/187202419.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(join(ckptdir, ckpt_name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_name = \"epoch_4000_step_160000.pth\"  # \"epoch_4000_step_160000.pth\" \n",
    "ckpt = torch.load(join(ckptdir, ckpt_name))\n",
    "pipeline.transformer.load_state_dict(state_dict_convert(ckpt['state_dict_ema']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d516502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7cb1431b144bd7a7ad9122389b15ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating prompts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "=== EVALUATION SUMMARY ===\n",
      "Total samples evaluated: 1078\n",
      "Number of prompts: 22\n",
      "\n",
      "--- Overall Performance ---\n",
      "overall                  : 0.894\n",
      "overall_loose            : 0.926\n",
      "shape                    : 0.951\n",
      "color                    : 0.948\n",
      "exist_binding            : 0.931\n",
      "unique_binding           : 0.930\n",
      "spatial_relationship     : 0.894\n",
      "spatial_relationship_loose: 0.926\n",
      "Dx                       : -2.693\n",
      "Dy                       : 0.900\n",
      "x1                       : 63.305\n",
      "y1                       : 61.045\n",
      "x2                       : 65.998\n",
      "y2                       : 60.145\n"
     ]
    }
   ],
   "source": [
    "prompt_collection, scene_info_collection = generate_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"{color1} {shape1} is {rel_text} {color2} {shape2}\",\n",
    "                                color1=\"blue\", shape1=\"circle\", color2=\"red\", shape2=\"square\")\n",
    "\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "eval_df_syn, object_df_syn = evaluate_pipeline_on_prompts(pipeline, prompt_collection, scene_info_collection,\n",
    "                                                          num_images=49, num_inference_steps=14, guidance_scale=4.5,\n",
    "                                                          max_sequence_length=20, generator_seed=42, prompt_dtype=torch.float16,\n",
    "                                                          color_margin=25, spatial_threshold=5, device=\"cuda\")\n",
    "\n",
    "print_evaluation_summary(eval_df_syn, group_by_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d923d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648d0abc77b9490db186a9cf765f3062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating prompts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "=== EVALUATION SUMMARY ===\n",
      "Total samples evaluated: 1078\n",
      "Number of prompts: 22\n",
      "\n",
      "--- Overall Performance ---\n",
      "overall                  : 0.608\n",
      "overall_loose            : 0.711\n",
      "shape                    : 0.963\n",
      "color                    : 0.964\n",
      "exist_binding            : 0.949\n",
      "unique_binding           : 0.949\n",
      "spatial_relationship     : 0.608\n",
      "spatial_relationship_loose: 0.711\n",
      "Dx                       : -12.358\n",
      "Dy                       : -13.795\n",
      "x1                       : 64.102\n",
      "y1                       : 56.080\n",
      "x2                       : 76.459\n",
      "y2                       : 69.875\n"
     ]
    }
   ],
   "source": [
    "prompt_collection, scene_info_collection = generate_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"{color1} {shape1} {rel_text} {color2} {shape2}\",\n",
    "                                color1=\"blue\", shape1=\"circle\", color2=\"red\", shape2=\"square\")\n",
    "\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "eval_df_syn, object_df_syn = evaluate_pipeline_on_prompts(pipeline, prompt_collection, scene_info_collection,\n",
    "                                                          num_images=49, num_inference_steps=14, guidance_scale=4.5,\n",
    "                                                          max_sequence_length=20, generator_seed=42, prompt_dtype=torch.float16,\n",
    "                                                          color_margin=25, spatial_threshold=5, device=\"cuda\")\n",
    "\n",
    "print_evaluation_summary(eval_df_syn, group_by_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0c539d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955e02b7a215477f80ffcc38a4e63d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating prompts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "=== EVALUATION SUMMARY ===\n",
      "Total samples evaluated: 1078\n",
      "Number of prompts: 22\n",
      "\n",
      "--- Overall Performance ---\n",
      "overall                  : 0.224\n",
      "overall_loose            : 0.371\n",
      "shape                    : 0.986\n",
      "color                    : 1.000\n",
      "exist_binding            : 0.986\n",
      "unique_binding           : 0.984\n",
      "spatial_relationship     : 0.224\n",
      "spatial_relationship_loose: 0.371\n",
      "Dx                       : -38.418\n",
      "Dy                       : -39.453\n",
      "x1                       : 52.417\n",
      "y1                       : 35.221\n",
      "x2                       : 90.834\n",
      "y2                       : 74.674\n"
     ]
    }
   ],
   "source": [
    "prompt_collection, scene_info_collection = generate_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"{color1} {shape1} {rel_text} the {color2} {shape2}\",\n",
    "                                color1=\"blue\", shape1=\"circle\", color2=\"red\", shape2=\"square\")\n",
    "\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "eval_df_syn, object_df_syn = evaluate_pipeline_on_prompts(pipeline, prompt_collection, scene_info_collection,\n",
    "                                                          num_images=49, num_inference_steps=14, guidance_scale=4.5,\n",
    "                                                          max_sequence_length=20, generator_seed=42, prompt_dtype=torch.float16,\n",
    "                                                          color_margin=25, spatial_threshold=5, device=\"cuda\")\n",
    "\n",
    "print_evaluation_summary(eval_df_syn, group_by_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e058998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2a67049e5e47cbaf59808b1f2a24a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating prompts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "torch.Size([98, 20, 4096])\n",
      "torch.Size([98, 20])\n",
      "=== EVALUATION SUMMARY ===\n",
      "Total samples evaluated: 1078\n",
      "Number of prompts: 22\n",
      "\n",
      "--- Overall Performance ---\n",
      "overall                  : 0.281\n",
      "overall_loose            : 0.494\n",
      "shape                    : 0.989\n",
      "color                    : 0.987\n",
      "exist_binding            : 0.981\n",
      "unique_binding           : 0.979\n",
      "spatial_relationship     : 0.281\n",
      "spatial_relationship_loose: 0.494\n",
      "Dx                       : -36.579\n",
      "Dy                       : -1.991\n",
      "x1                       : 51.858\n",
      "y1                       : 53.710\n",
      "x2                       : 88.437\n",
      "y2                       : 55.701\n"
     ]
    }
   ],
   "source": [
    "prompt_collection, scene_info_collection = generate_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"the {color1} {shape1} {rel_text} the {color2} {shape2}\",\n",
    "                                color1=\"blue\", shape1=\"circle\", color2=\"red\", shape2=\"square\")\n",
    "\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "eval_df_syn, object_df_syn = evaluate_pipeline_on_prompts(pipeline, prompt_collection, scene_info_collection,\n",
    "                                                          num_images=49, num_inference_steps=14, guidance_scale=4.5,\n",
    "                                                          max_sequence_length=20, generator_seed=42, prompt_dtype=torch.float16,\n",
    "                                                          color_margin=25, spatial_threshold=5, device=\"cuda\")\n",
    "\n",
    "print_evaluation_summary(eval_df_syn, group_by_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9dc1776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae34e4d130194e87831affdcf0e64e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating prompts:   0%|          | 0/264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATION SUMMARY ===\n",
      "Total samples evaluated: 12936\n",
      "Number of prompts: 264\n",
      "\n",
      "--- Overall Performance ---\n",
      "overall                  : 0.749\n",
      "overall_loose            : 0.809\n",
      "shape                    : 0.856\n",
      "color                    : 0.893\n",
      "exist_binding            : 0.821\n",
      "unique_binding           : 0.820\n",
      "spatial_relationship     : 0.749\n",
      "spatial_relationship_loose: 0.809\n",
      "Dx                       : -0.856\n",
      "Dy                       : -0.422\n",
      "x1                       : 64.531\n",
      "y1                       : 64.164\n",
      "x2                       : 65.387\n",
      "y2                       : 64.587\n"
     ]
    }
   ],
   "source": [
    "prompt_collection, scene_info_collection = generate_all_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"{color1} {shape1} is {rel_text} {color2} {shape2}\",)\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "eval_df_syn, object_df_syn = evaluate_pipeline_on_prompts(pipeline, prompt_collection, scene_info_collection,\n",
    "                                                          num_images=49, num_inference_steps=14, guidance_scale=4.5,\n",
    "                                                          max_sequence_length=20, generator_seed=42, prompt_dtype=torch.float16,\n",
    "                                                          color_margin=25, spatial_threshold=5, device=\"cuda\")\n",
    "\n",
    "print_evaluation_summary(eval_df_syn, group_by_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea7b196a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3407022f0ec4a799bab839da8791dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating prompts:   0%|          | 0/264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATION SUMMARY ===\n",
      "Total samples evaluated: 12936\n",
      "Number of prompts: 264\n",
      "\n",
      "--- Overall Performance ---\n",
      "overall                  : 0.302\n",
      "overall_loose            : 0.494\n",
      "shape                    : 0.914\n",
      "color                    : 0.930\n",
      "exist_binding            : 0.893\n",
      "unique_binding           : 0.891\n",
      "spatial_relationship     : 0.302\n",
      "spatial_relationship_loose: 0.494\n",
      "Dx                       : -33.754\n",
      "Dy                       : -25.145\n",
      "x1                       : 48.676\n",
      "y1                       : 47.181\n",
      "x2                       : 82.431\n",
      "y2                       : 72.326\n"
     ]
    }
   ],
   "source": [
    "prompt_collection, scene_info_collection = generate_all_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"{color1} {shape1} is {rel_text} the {color2} {shape2}\",)\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "eval_df_syn, object_df_syn = evaluate_pipeline_on_prompts(pipeline, prompt_collection, scene_info_collection,\n",
    "                                                          num_images=49, num_inference_steps=14, guidance_scale=4.5,\n",
    "                                                          max_sequence_length=20, generator_seed=42, prompt_dtype=torch.float16,\n",
    "                                                          color_margin=25, spatial_threshold=5, device=\"cuda\")\n",
    "\n",
    "print_evaluation_summary(eval_df_syn, group_by_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e31488d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3420d2b0cd8b40cd977ed60e2d283231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating prompts:   0%|          | 0/264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATION SUMMARY ===\n",
      "Total samples evaluated: 12936\n",
      "Number of prompts: 264\n",
      "\n",
      "--- Overall Performance ---\n",
      "overall                  : 0.218\n",
      "overall_loose            : 0.403\n",
      "shape                    : 0.942\n",
      "color                    : 0.959\n",
      "exist_binding            : 0.929\n",
      "unique_binding           : 0.927\n",
      "spatial_relationship     : 0.218\n",
      "spatial_relationship_loose: 0.403\n",
      "Dx                       : -35.012\n",
      "Dy                       : -37.074\n",
      "x1                       : 47.970\n",
      "y1                       : 42.555\n",
      "x2                       : 82.982\n",
      "y2                       : 79.629\n"
     ]
    }
   ],
   "source": [
    "prompt_collection, scene_info_collection = generate_all_prompt_collection(dataset_tmp.spatial_phrases,\n",
    "                                prompt_template=\"{color1} {shape1} {rel_text} the {color2} {shape2}\",)\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "eval_df_syn, object_df_syn = evaluate_pipeline_on_prompts(pipeline, prompt_collection, scene_info_collection,\n",
    "                                                          num_images=49, num_inference_steps=14, guidance_scale=4.5,\n",
    "                                                          max_sequence_length=20, generator_seed=42, prompt_dtype=torch.float16,\n",
    "                                                          color_margin=25, spatial_threshold=5, device=\"cuda\")\n",
    "\n",
    "print_evaluation_summary(eval_df_syn, group_by_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d712b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

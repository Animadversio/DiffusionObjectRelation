{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc36ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0f2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8de38717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_hot_token_mask(tokens, target_words, seq_len=None):\n",
    "    mask = th.zeros(len(tokens) if seq_len is None else seq_len, dtype=th.bool)\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Remove the special prefix if present\n",
    "        clean_token = token.replace('▁', '') if token.startswith('▁') else token\n",
    "        if clean_token.endswith('</w>'):\n",
    "            clean_token = clean_token.split('</w>')[0]\n",
    "        if clean_token.startswith('<|') and clean_token.endswith('|>'):\n",
    "            continue\n",
    "        if clean_token in target_words:\n",
    "            mask[i] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60f434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModelWithProjection, CLIPTokenizer\n",
    "# Load SDXL's text encoder and tokenizer (text_encoder_2 and tokenizer_2)\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"text_encoder_2\", \n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"tokenizer_2\", \n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9c6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'max_length': 20, 'padding': 'max_length', 'truncation': True, 'return_attention_mask': True, 'add_special_tokens': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "prompt = 'blue circle is above and to the left of the red triangle'\n",
    "# pipeline.\n",
    "token_splits = tokenizer.tokenize(prompt,\n",
    "                                  max_length=20,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                return_attention_mask=True,\n",
    "                                add_special_tokens=True,\n",
    "                                return_tensors='pt')\n",
    "for text_targets in [[\"square\"], \n",
    "                            [\"triangle\"], \n",
    "                            [\"circle\"], \n",
    "                            [\"red\"], \n",
    "                            [\"blue\"],\n",
    "                            [\"below\", \"left\", \"top\", \"right\", \"above\"],\n",
    "                            [\"below\", ],\n",
    "                            [\"left\", ],\n",
    "                            [\"above\", ],\n",
    "                            [\"right\", ],\n",
    "                            [\"and\", \"to\", \"the\", \"of\", \"on\", \"is\"],]:\n",
    "    # text_targets = [\"and\", \"to\", \"the\", \"of\", \"on\", \"is\"]\n",
    "    # template_type = f\"image {imgtoken_type} to text {' '.join(text_targets)}\"\n",
    "    text_mask = create_multi_hot_token_mask(token_splits, text_targets, seq_len=20)\n",
    "    print(text_mask.to(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026afb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "prompt = 'blue circle is above and to the left of the red triangle'\n",
    "# pipeline.\n",
    "text_tokens_and_mask = tokenizer(\n",
    "        prompt,\n",
    "        max_length=20,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "input_ids = text_tokens_and_mask['input_ids']\n",
    "if input_ids.ndim == 2:\n",
    "    input_ids = input_ids[0]\n",
    "token_splits = [tokenizer.decode(i) for i in input_ids]\n",
    "for text_targets in [[\"square\"], \n",
    "                            [\"triangle\"], \n",
    "                            [\"circle\"], \n",
    "                            [\"red\"], \n",
    "                            [\"blue\"],\n",
    "                            [\"below\", \"left\", \"top\", \"right\", \"above\"],\n",
    "                            [\"below\", ],\n",
    "                            [\"left\", ],\n",
    "                            [\"above\", ],\n",
    "                            [\"right\", ],\n",
    "                            [\"and\", \"to\", \"the\", \"of\", \"on\", \"is\"],]:\n",
    "    # text_targets = [\"and\", \"to\", \"the\", \"of\", \"on\", \"is\"]\n",
    "    # template_type = f\"image {imgtoken_type} to text {' '.join(text_targets)}\"\n",
    "    text_mask = create_multi_hot_token_mask(token_splits, text_targets, seq_len=20)\n",
    "    print(text_mask.to(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8cecb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens_and_mask = tokenizer(\n",
    "        prompt,\n",
    "        max_length=20,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "text_tokens_and_mask['input_ids'] = text_tokens_and_mask['input_ids']\n",
    "text_tokens_and_mask['attention_mask'] = text_tokens_and_mask['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54c5a9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,  1746,  7117,   533,  4348,   537,   531,   518,  1823,   539,\n",
       "           518,   736, 14615, 49407,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens_and_mask['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8cd11a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens_and_mask['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3588a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>\n",
      "blue\n",
      "circle\n",
      "is\n",
      "above\n",
      "and\n",
      "to\n",
      "the\n",
      "left\n",
      "of\n",
      "the\n",
      "red\n",
      "triangle\n",
      "<|endoftext|>\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for i in text_tokens_and_mask['input_ids'][0]:\n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "163869d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'max_length': 20, 'padding': 'max_length', 'truncation': True, 'return_attention_mask': True, 'add_special_tokens': True, 'return_tensors': 'pt'} not recognized.\n"
     ]
    }
   ],
   "source": [
    "token_splits = tokenizer.tokenize(prompt,\n",
    "                                  max_length=20,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                return_attention_mask=True,\n",
    "                                add_special_tokens=True,\n",
    "                                return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c232aa30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTokenizer(name_or_path='stabilityai/stable-diffusion-xl-base-1.0', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '!'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c879fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def tokenize(self, text: TextInput, **kwargs) -> list[str]:\n",
      "        \"\"\"\n",
      "        Converts a string into a sequence of tokens, using the tokenizer.\n",
      "\n",
      "        Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies\n",
      "        (BPE/SentencePieces/WordPieces). Takes care of added tokens.\n",
      "\n",
      "        Args:\n",
      "            text (`str`):\n",
      "                The sequence to be encoded.\n",
      "            **kwargs (additional keyword arguments):\n",
      "                Passed along to the model-specific `prepare_for_tokenization` preprocessing method.\n",
      "\n",
      "        Returns:\n",
      "            `list[str]`: The list of tokens.\n",
      "        \"\"\"\n",
      "        split_special_tokens = kwargs.pop(\"split_special_tokens\", self.split_special_tokens)\n",
      "\n",
      "        text, kwargs = self.prepare_for_tokenization(text, **kwargs)\n",
      "\n",
      "        if kwargs:\n",
      "            logger.warning(f\"Keyword arguments {kwargs} not recognized.\")\n",
      "\n",
      "        if hasattr(self, \"do_lower_case\") and self.do_lower_case:\n",
      "            # convert non-special tokens to lowercase. Might be super slow as well?\n",
      "            escaped_special_toks = [re.escape(s_tok) for s_tok in (self.all_special_tokens)]\n",
      "            escaped_special_toks += [\n",
      "                re.escape(s_tok.content)\n",
      "                for s_tok in (self._added_tokens_decoder.values())\n",
      "                if not s_tok.special and s_tok.normalized\n",
      "            ]\n",
      "            pattern = r\"(\" + r\"|\".join(escaped_special_toks) + r\")|\" + r\"(.+?)\"\n",
      "            text = re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), text)\n",
      "\n",
      "        if split_special_tokens:\n",
      "            no_split_token = []\n",
      "            tokens = [text]\n",
      "        else:\n",
      "            no_split_token = self._added_tokens_encoder.keys()  # don't split on any of the added tokens\n",
      "            # \"This is something<special_token_1>  else\"\n",
      "            tokens = self.tokens_trie.split(text)\n",
      "\n",
      "        # [\"This is something\", \"<special_token_1>\", \"  else\"]\n",
      "        for i, token in enumerate(tokens):\n",
      "            if token in no_split_token:\n",
      "                tok_extended = self._added_tokens_decoder.get(self._added_tokens_encoder[token], None)\n",
      "                left = tokens[i - 1] if i > 0 else None\n",
      "                right = tokens[i + 1] if i < len(tokens) - 1 else None\n",
      "                if isinstance(tok_extended, AddedToken):\n",
      "                    if tok_extended.rstrip and right:\n",
      "                        # A bit counter-intuitive but we strip the left of the string\n",
      "                        # since tok_extended.rstrip means the special token is eating all white spaces on its right\n",
      "                        tokens[i + 1] = right.lstrip()\n",
      "                    # Strip white spaces on the left\n",
      "                    if tok_extended.lstrip and left:\n",
      "                        tokens[i - 1] = left.rstrip()  # Opposite here\n",
      "                    if tok_extended.single_word and left and left[-1] != \" \":\n",
      "                        tokens[i - 1] += token\n",
      "                        tokens[i] = \"\"\n",
      "                    elif tok_extended.single_word and right and right[0] != \" \":\n",
      "                        tokens[i + 1] = token + tokens[i + 1]\n",
      "                        tokens[i] = \"\"\n",
      "                else:\n",
      "                    raise ValueError(\n",
      "                        f\"{tok_extended} cannot be tokenized because it was not properly added\"\n",
      "                        f\" to the tokenizer. This means that it is not an `AddedToken` but a {type(tok_extended)}\"\n",
      "                    )\n",
      "        # [\"This is something\", \"<special_token_1>\", \"else\"]\n",
      "        tokenized_text = []\n",
      "        for token in tokens:\n",
      "            # Need to skip eventual empty (fully stripped) tokens\n",
      "            if not token:\n",
      "                continue\n",
      "            if token in no_split_token:\n",
      "                tokenized_text.append(token)\n",
      "            else:\n",
      "                tokenized_text.extend(self._tokenize(token))\n",
      "        # [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\n",
      "        return tokenized_text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(tokenizer.tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d683f73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue</w>',\n",
       " 'circle</w>',\n",
       " 'is</w>',\n",
       " 'above</w>',\n",
       " 'and</w>',\n",
       " 'to</w>',\n",
       " 'the</w>',\n",
       " 'left</w>',\n",
       " 'of</w>',\n",
       " 'the</w>',\n",
       " 'red</w>',\n",
       " 'triangle</w>']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4f16b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue</w>',\n",
       " 'circle</w>',\n",
       " 'is</w>',\n",
       " 'above</w>',\n",
       " 'and</w>',\n",
       " 'to</w>',\n",
       " 'the</w>',\n",
       " 'left</w>',\n",
       " 'of</w>',\n",
       " 'the</w>',\n",
       " 'red</w>',\n",
       " 'triangle</w>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a49d5502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2f9c6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "931b3654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue</w>',\n",
       " 'circle</w>',\n",
       " 'is</w>',\n",
       " 'above</w>',\n",
       " 'and</w>',\n",
       " 'to</w>',\n",
       " 'the</w>',\n",
       " 'left</w>',\n",
       " 'of</w>',\n",
       " 'the</w>',\n",
       " 'red</w>',\n",
       " 'triangle</w>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1ca20c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blue'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'blue</w>'.split('</w>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e71b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

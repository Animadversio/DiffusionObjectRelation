{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-Embedding-8B Analysis for Spatial Relationship Understanding\n",
    "\n",
    "This notebook explores the Qwen/Qwen3-Embedding-8B model structure and analyzes its embedding behavior with spatial relationship prompts.\n",
    "\n",
    "## Objectives:\n",
    "- Load and inspect Qwen3-Embedding-8B model architecture\n",
    "- Generate embeddings for spatial relationship prompts\n",
    "- Analyze embedding structure and clustering patterns\n",
    "- Compare with existing approaches in the research pipeline\n",
    "\n",
    "Date: 2025-08-18  \n",
    "Author: Binxu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not available\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"Required libraries already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    !pip install transformers sentence-transformers\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print(\"Sklearn libraries available\")\n",
    "except ImportError:\n",
    "    print(\"Installing sklearn...\")\n",
    "    !pip install scikit-learn\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Spatial Relationship Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils from the project\n",
    "from utils.eval_prompts import PromptDataset\n",
    "\n",
    "# Load the prompt dataset\n",
    "prompt_dataset = PromptDataset()\n",
    "\n",
    "# Get all available prompt types\n",
    "prompt_types = list(prompt_dataset.get_prompt_types())\n",
    "print(\"Available prompt types:\", prompt_types)\n",
    "\n",
    "# Display prompts by category\n",
    "for ptype in prompt_types:\n",
    "    prompts = prompt_dataset.get_prompts_by_type(ptype)\n",
    "    print(f\"\\n{ptype.upper()} ({len(prompts)} prompts):\")\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"  {i+1:2d}. {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on spatial relationship prompts for analysis\n",
    "relational_prompts = prompt_dataset.get_prompts_by_type(\"relational\")\n",
    "relational_2color_prompts = prompt_dataset.get_prompts_by_type(\"relational_2_colors\")\n",
    "\n",
    "# Combine and clean prompts\n",
    "all_spatial_prompts = relational_prompts + relational_2color_prompts\n",
    "\n",
    "# Convert underscore format to natural language\n",
    "natural_language_prompts = []\n",
    "for prompt in all_spatial_prompts:\n",
    "    # Convert underscores to spaces and clean up\n",
    "    natural = prompt.replace('_', ' ').replace(' is ', ' is ').replace(' to the ', ' to the ')\n",
    "    natural_language_prompts.append(natural)\n",
    "\n",
    "print(\"Spatial relationship prompts (underscore format):\")\n",
    "for i, prompt in enumerate(all_spatial_prompts):\n",
    "    print(f\"  {i+1:2d}. {prompt}\")\n",
    "\n",
    "print(\"\\nSpatial relationship prompts (natural language):\")\n",
    "for i, prompt in enumerate(natural_language_prompts):\n",
    "    print(f\"  {i+1:2d}. {prompt}\")\n",
    "\n",
    "# Additional test prompts for comprehensive analysis\n",
    "additional_prompts = [\n",
    "    \"blue triangle is to the upper left of red square\",  # Main example from user\n",
    "    \"red circle above blue square\", \n",
    "    \"green triangle below yellow circle\",\n",
    "    \"object on the left side\",\n",
    "    \"object on the right side\",\n",
    "    \"objects positioned above\",\n",
    "    \"objects positioned below\",\n",
    "]\n",
    "\n",
    "# Combine all prompts for analysis\n",
    "all_test_prompts = natural_language_prompts + additional_prompts\n",
    "print(f\"\\nTotal prompts for analysis: {len(all_test_prompts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Qwen3-Embedding-8B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model identifier\n",
    "model_name = \"Qwen/Qwen3-Embedding-8B\"\n",
    "print(f\"Loading model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with sentence-transformers (recommended approach)\n",
    "print(\"Loading model with sentence-transformers...\")\n",
    "try:\n",
    "    model_st = SentenceTransformer(model_name, device=device)\n",
    "    print(f\"✓ Model loaded successfully with sentence-transformers\")\n",
    "    print(f\"Model device: {model_st.device}\")\n",
    "    print(f\"Max sequence length: {model_st.max_seq_length}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading with sentence-transformers: {e}\")\n",
    "    model_st = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Load with transformers directly\n",
    "print(\"Loading model with transformers library...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_hf = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32)\n",
    "    model_hf = model_hf.to(device)\n",
    "    print(f\"✓ Model loaded successfully with transformers\")\n",
    "    print(f\"Model device: {model_hf.device}\")\n",
    "    print(f\"Model dtype: {model_hf.dtype}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading with transformers: {e}\")\n",
    "    model_hf = None\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect model configuration\n",
    "if model_hf is not None:\n",
    "    print(\"=== Model Configuration ===\")\n",
    "    config = model_hf.config\n",
    "    print(f\"Model type: {config.model_type}\")\n",
    "    print(f\"Hidden size: {config.hidden_size}\")\n",
    "    print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "    print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "    print(f\"Intermediate size: {config.intermediate_size}\")\n",
    "    print(f\"Max position embeddings: {config.max_position_embeddings}\")\n",
    "    print(f\"Vocab size: {config.vocab_size}\")\n",
    "    \n",
    "    if hasattr(config, 'embedding_size'):\n",
    "        print(f\"Embedding size: {config.embedding_size}\")\n",
    "    \n",
    "    print(\"\\n=== Model Architecture ===\")\n",
    "    print(model_hf)\n",
    "\n",
    "if model_st is not None:\n",
    "    print(\"\\n=== SentenceTransformer Model Info ===\")\n",
    "    print(f\"Model modules: {len(model_st._modules)}\")\n",
    "    for i, module in enumerate(model_st._modules):\n",
    "        print(f\"  Module {i}: {type(module).__name__}\")\n",
    "        if hasattr(module, 'get_sentence_embedding_dimension'):\n",
    "            print(f\"    Embedding dimension: {module.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding generation to determine output dimensions\n",
    "test_prompt = \"blue triangle is above red square\"\n",
    "\n",
    "if model_st is not None:\n",
    "    print(\"=== Testing with SentenceTransformer ===\")\n",
    "    test_embedding = model_st.encode([test_prompt], convert_to_tensor=True)\n",
    "    print(f\"Default embedding shape: {test_embedding.shape}\")\n",
    "    print(f\"Embedding dimension: {test_embedding.shape[1]}\")\n",
    "    print(f\"Embedding dtype: {test_embedding.dtype}\")\n",
    "    print(f\"First 10 values: {test_embedding[0][:10].cpu().numpy()}\")\n",
    "\n",
    "if model_hf is not None and tokenizer is not None:\n",
    "    print(\"\\n=== Testing with Transformers ===\")\n",
    "    # Tokenize and encode\n",
    "    inputs = tokenizer(test_prompt, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_hf(**inputs)\n",
    "    \n",
    "    # Get mean pooled representation\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    # Mean pooling\n",
    "    masked_embeddings = last_hidden_state * attention_mask.unsqueeze(-1)\n",
    "    summed_embeddings = torch.sum(masked_embeddings, dim=1)\n",
    "    lengths = torch.sum(attention_mask, dim=1)\n",
    "    mean_embedding = summed_embeddings / lengths.unsqueeze(-1)\n",
    "    \n",
    "    print(f\"Raw output shape: {last_hidden_state.shape}\")\n",
    "    print(f\"Mean pooled embedding shape: {mean_embedding.shape}\")\n",
    "    print(f\"Embedding dimension: {mean_embedding.shape[1]}\")\n",
    "    print(f\"First 10 values: {mean_embedding[0][:10].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Embeddings for All Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompts_batch(prompts, model, batch_size=8):\n",
    "    \"\"\"Encode prompts in batches using SentenceTransformer\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Encoding prompts\"):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "        embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Generate embeddings for all test prompts\n",
    "if model_st is not None:\n",
    "    print(f\"Generating embeddings for {len(all_test_prompts)} prompts...\")\n",
    "    embeddings = encode_prompts_batch(all_test_prompts, model_st)\n",
    "    \n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "    \n",
    "    # Convert to numpy for analysis\n",
    "    embeddings_np = embeddings.numpy()\n",
    "    \n",
    "    # Create DataFrame for easier analysis\n",
    "    embeddings_df = pd.DataFrame(embeddings_np)\n",
    "    embeddings_df['prompt'] = all_test_prompts\n",
    "    embeddings_df['prompt_type'] = (['relational'] * len(relational_prompts) + \n",
    "                                   ['relational_2_colors'] * len(relational_2color_prompts) + \n",
    "                                   ['additional'] * len(additional_prompts))\n",
    "    \n",
    "    print(\"\\nEmbedding statistics:\")\n",
    "    print(f\"Mean: {embeddings_np.mean():.6f}\")\n",
    "    print(f\"Std: {embeddings_np.std():.6f}\")\n",
    "    print(f\"Min: {embeddings_np.min():.6f}\")\n",
    "    print(f\"Max: {embeddings_np.max():.6f}\")\nelse:\n    print(\"❌ Cannot generate embeddings - model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embedding Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze embedding distribution\n",
    "if 'embeddings_np' in locals():\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Histogram of embedding values\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(embeddings_np.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Distribution of Embedding Values')\n",
    "    plt.xlabel('Embedding Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Embedding norms\n",
    "    plt.subplot(1, 3, 2)\n",
    "    norms = np.linalg.norm(embeddings_np, axis=1)\n",
    "    plt.hist(norms, bins=20, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Distribution of Embedding Norms')\n",
    "    plt.xlabel('L2 Norm')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Dimension-wise variance\n",
    "    plt.subplot(1, 3, 3)\n",
    "    dim_variance = np.var(embeddings_np, axis=0)\n",
    "    plt.plot(dim_variance)\n",
    "    plt.title('Variance Across Embedding Dimensions')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Embedding norms - Mean: {norms.mean():.4f}, Std: {norms.std():.4f}\")\n",
    "    print(f\"Top 10 most variable dimensions: {np.argsort(dim_variance)[-10:][::-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity matrix\n",
    "if 'embeddings_np' in locals():\n",
    "    print(\"Computing cosine similarity matrix...\")\n",
    "    similarity_matrix = cosine_similarity(embeddings_np)\n",
    "    \n",
    "    # Visualize similarity matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(similarity_matrix, dtype=bool), k=1)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(similarity_matrix, \n",
    "                mask=mask,\n",
    "                annot=False, \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                xticklabels=[f\"{i:2d}\" for i in range(len(all_test_prompts))],\n",
    "                yticklabels=[f\"{i:2d}\" for i in range(len(all_test_prompts))],\n",
    "                cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('Cosine Similarity Matrix of Prompt Embeddings')\n",
    "    plt.xlabel('Prompt Index')\n",
    "    plt.ylabel('Prompt Index')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find most similar and dissimilar pairs\n",
    "    similarity_triu = np.triu(similarity_matrix, k=1)\n",
    "    max_idx = np.unravel_index(np.argmax(similarity_triu), similarity_triu.shape)\n",
    "    min_idx = np.unravel_index(np.argmin(similarity_triu), similarity_triu.shape)\n",
    "    \n",
    "    print(f\"\\nMost similar prompts (similarity: {similarity_matrix[max_idx]:.4f}):\")\n",
    "    print(f\"  {max_idx[0]:2d}. {all_test_prompts[max_idx[0]]}\")\n",
    "    print(f\"  {max_idx[1]:2d}. {all_test_prompts[max_idx[1]]}\")\n",
    "    \n",
    "    print(f\"\\nMost dissimilar prompts (similarity: {similarity_matrix[min_idx]:.4f}):\")\n",
    "    print(f\"  {min_idx[0]:2d}. {all_test_prompts[min_idx[0]]}\")\n",
    "    print(f\"  {min_idx[1]:2d}. {all_test_prompts[min_idx[1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dimensionality Reduction and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA analysis\n",
    "if 'embeddings_np' in locals():\n",
    "    print(\"Performing PCA analysis...\")\n",
    "    \n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=min(50, embeddings_np.shape[0]-1))\n",
    "    pca_embeddings = pca.fit_transform(embeddings_np)\n",
    "    \n",
    "    # Plot explained variance\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(pca.explained_variance_ratio_[:20], 'bo-')\n",
    "    plt.title('PCA Explained Variance Ratio')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.plot(cumvar[:20], 'ro-')\n",
    "    plt.title('Cumulative Explained Variance')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Cumulative Variance')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2D visualization\n",
    "    plt.subplot(1, 3, 3)\n",
    "    colors = ['red' if 'relational' in ptype else 'blue' if 'relational_2' in ptype else 'green' \n",
    "              for ptype in embeddings_df['prompt_type']]\n",
    "    \n",
    "    scatter = plt.scatter(pca_embeddings[:, 0], pca_embeddings[:, 1], c=colors, alpha=0.7)\n",
    "    plt.title('PCA Visualization (PC1 vs PC2)')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='red', label='Relational'),\n",
    "                      Patch(facecolor='blue', label='Relational 2-Colors'),\n",
    "                      Patch(facecolor='green', label='Additional')]\n",
    "    plt.legend(handles=legend_elements)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"First 5 PCs explain {cumvar[4]:.3f} of total variance\")\n",
    "    print(f\"First 10 PCs explain {cumvar[9]:.3f} of total variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "if 'embeddings_np' in locals() and len(embeddings_np) > 5:\n",
    "    print(\"Performing t-SNE analysis...\")\n",
    "    \n",
    "    # Use PCA preprocessing for t-SNE\n",
    "    pca_50 = PCA(n_components=min(50, embeddings_np.shape[0]-1))\n",
    "    pca_reduced = pca_50.fit_transform(embeddings_np)\n",
    "    \n",
    "    # t-SNE with different perplexities\n",
    "    perplexities = [5, 10, 20] if len(embeddings_np) > 20 else [min(5, len(embeddings_np)-1)]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(perplexities), figsize=(5*len(perplexities), 5))\n",
    "    if len(perplexities) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, perp in enumerate(perplexities):\n",
    "        if perp < len(embeddings_np):\n",
    "            tsne = TSNE(n_components=2, perplexity=perp, random_state=42, n_iter=1000)\n",
    "            tsne_embeddings = tsne.fit_transform(pca_reduced)\n",
    "            \n",
    "            # Color by prompt type\n",
    "            colors = ['red' if 'relational' in ptype and '2_colors' not in ptype else \n",
    "                     'blue' if 'relational_2' in ptype else 'green' \n",
    "                     for ptype in embeddings_df['prompt_type']]\n",
    "            \n",
    "            axes[i].scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=colors, alpha=0.7)\n",
    "            axes[i].set_title(f't-SNE (perplexity={perp})')\n",
    "            axes[i].set_xlabel('t-SNE 1')\n",
    "            axes[i].set_ylabel('t-SNE 2')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n    print(\"Skipping t-SNE - insufficient data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Spatial Relationship Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze spatial relationship patterns\n",
    "if 'embeddings_np' in locals():\n",
    "    # Define spatial relationship categories\n",
    "    spatial_keywords = {\n",
    "        'above': ['above', 'upper'],\n",
    "        'below': ['below', 'lower'],\n",
    "        'left': ['left'],\n",
    "        'right': ['right'],\n",
    "        'diagonal': ['upper left', 'upper right', 'lower left', 'lower right']\n",
    "    }\n",
    "    \n",
    "    # Categorize prompts by spatial relationship\n",
    "    prompt_categories = []\n",
    "    for prompt in all_test_prompts:\n",
    "        prompt_lower = prompt.lower()\n",
    "        category = 'other'\n",
    "        \n",
    "        for spatial_type, keywords in spatial_keywords.items():\n",
    "            if any(keyword in prompt_lower for keyword in keywords):\n",
    "                category = spatial_type\n",
    "                break\n",
    "        \n",
    "        prompt_categories.append(category)\n",
    "    \n",
    "    # Add to dataframe\n",
    "    embeddings_df['spatial_category'] = prompt_categories\n",
    "    \n",
    "    print(\"Spatial category distribution:\")\n",
    "    category_counts = pd.Series(prompt_categories).value_counts()\n",
    "    print(category_counts)\n",
    "    \n",
    "    # Visualize spatial categories in embedding space\n",
    "    if 'pca_embeddings' in locals():\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create color map for spatial categories\n",
    "        unique_categories = list(set(prompt_categories))\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(unique_categories)))\n",
    "        category_colors = dict(zip(unique_categories, colors))\n",
    "        \n",
    "        for category in unique_categories:\n",
    "            mask = np.array(prompt_categories) == category\n",
    "            if np.any(mask):\n",
    "                plt.scatter(pca_embeddings[mask, 0], pca_embeddings[mask, 1], \n",
    "                           c=[category_colors[category]], label=category, alpha=0.7, s=60)\n",
    "        \n",
    "        plt.title('PCA Visualization by Spatial Category')\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average embeddings for each spatial category\n",
    "if 'embeddings_np' in locals():\n",
    "    spatial_centroids = {}\n",
    "    \n",
    "    for category in set(prompt_categories):\n",
    "        mask = np.array(prompt_categories) == category\n",
    "        if np.any(mask):\n",
    "            centroid = embeddings_np[mask].mean(axis=0)\n",
    "            spatial_centroids[category] = centroid\n",
    "    \n",
    "    # Compute similarities between spatial category centroids\n",
    "    if len(spatial_centroids) > 1:\n",
    "        categories = list(spatial_centroids.keys())\n",
    "        centroid_matrix = np.array([spatial_centroids[cat] for cat in categories])\n",
    "        centroid_similarity = cosine_similarity(centroid_matrix)\n",
    "        \n",
    "        # Visualize centroid similarities\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(centroid_similarity, \n",
    "                    annot=True, \n",
    "                    cmap='coolwarm', \n",
    "                    center=0,\n",
    "                    xticklabels=categories,\n",
    "                    yticklabels=categories,\n",
    "                    square=True)\n",
    "        \n",
    "        plt.title('Cosine Similarity Between Spatial Category Centroids')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nSpatial category centroid similarities:\")\n",
    "        for i, cat1 in enumerate(categories):\n",
    "            for j, cat2 in enumerate(categories):\n",
    "                if i < j:\n",
    "                    print(f\"{cat1} vs {cat2}: {centroid_similarity[i,j]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Prompt Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed analysis table\n",
    "if 'embeddings_df' in locals():\n",
    "    analysis_df = embeddings_df[['prompt', 'prompt_type', 'spatial_category']].copy()\n",
    "    \n",
    "    # Add embedding statistics\n",
    "    embedding_cols = [col for col in embeddings_df.columns if isinstance(col, int)]\n",
    "    embedding_data = embeddings_df[embedding_cols].values\n",
    "    \n",
    "    analysis_df['embedding_norm'] = np.linalg.norm(embedding_data, axis=1)\n",
    "    analysis_df['embedding_mean'] = np.mean(embedding_data, axis=1)\n",
    "    analysis_df['embedding_std'] = np.std(embedding_data, axis=1)\n",
    "    \n",
    "    # Find most similar prompt for each\n",
    "    most_similar_indices = []\n",
    "    most_similar_scores = []\n",
    "    \n",
    "    for i in range(len(similarity_matrix)):\n",
    "        # Get similarity scores excluding self\n",
    "        sim_scores = similarity_matrix[i].copy()\n",
    "        sim_scores[i] = -1  # Exclude self\n",
    "        \n",
    "        best_idx = np.argmax(sim_scores)\n",
    "        most_similar_indices.append(best_idx)\n",
    "        most_similar_scores.append(sim_scores[best_idx])\n",
    "    \n",
    "    analysis_df['most_similar_idx'] = most_similar_indices\n",
    "    analysis_df['most_similar_score'] = most_similar_scores\n",
    "    analysis_df['most_similar_prompt'] = [all_test_prompts[idx] for idx in most_similar_indices]\n",
    "    \n",
    "    # Display analysis\n",
    "    print(\"=== Detailed Prompt Analysis ===\")\n",
    "    print(analysis_df.to_string(index=True, max_colwidth=50))\n",
    "    \n",
    "    # Summary statistics by category\n",
    "    print(\"\\n=== Summary by Spatial Category ===\")\n",
    "    summary = analysis_df.groupby('spatial_category').agg({\n",
    "        'embedding_norm': ['mean', 'std'],\n",
    "        'embedding_mean': ['mean', 'std'],\n",
    "        'most_similar_score': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison with Existing Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with simple bag-of-words baseline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"=== Comparison with TF-IDF Baseline ===\")\n",
    "\n",
    "# Create TF-IDF embeddings\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(all_test_prompts).toarray()\n",
    "\n",
    "print(f\"TF-IDF embedding dimension: {tfidf_embeddings.shape[1]}\")\n",
    "print(f\"Qwen3 embedding dimension: {embeddings_np.shape[1]}\")\n",
    "\n",
    "# Compute TF-IDF similarity matrix\n",
    "tfidf_similarity = cosine_similarity(tfidf_embeddings)\n",
    "\n",
    "# Compare similarity matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Qwen3 similarities\n",
    "im1 = axes[0].imshow(similarity_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[0].set_title('Qwen3-Embedding-8B Similarities')\n",
    "axes[0].set_xlabel('Prompt Index')\n",
    "axes[0].set_ylabel('Prompt Index')\n",
    "\n",
    "# TF-IDF similarities\n",
    "im2 = axes[1].imshow(tfidf_similarity, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1].set_title('TF-IDF Similarities')\n",
    "axes[1].set_xlabel('Prompt Index')\n",
    "axes[1].set_ylabel('Prompt Index')\n",
    "\n",
    "# Difference\n",
    "diff_matrix = similarity_matrix - tfidf_similarity\n",
    "im3 = axes[2].imshow(diff_matrix, cmap='RdBu', vmin=-1, vmax=1)\n",
    "axes[2].set_title('Difference (Qwen3 - TF-IDF)')\n",
    "axes[2].set_xlabel('Prompt Index')\n",
    "axes[2].set_ylabel('Prompt Index')\n",
    "\n",
    "# Add colorbars\n",
    "plt.colorbar(im1, ax=axes[0], fraction=0.046)\n",
    "plt.colorbar(im2, ax=axes[1], fraction=0.046)\n",
    "plt.colorbar(im3, ax=axes[2], fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation between similarity matrices\n",
    "mask = np.triu(np.ones_like(similarity_matrix, dtype=bool), k=1)\n",
    "qwen_sim_triu = similarity_matrix[mask]\n",
    "tfidf_sim_triu = tfidf_similarity[mask]\n",
    "\n",
    "correlation = np.corrcoef(qwen_sim_triu, tfidf_sim_triu)[0, 1]\n",
    "print(f\"\\nCorrelation between Qwen3 and TF-IDF similarities: {correlation:.4f}\")\n",
    "\n",
    "# Scatter plot comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tfidf_sim_triu, qwen_sim_triu, alpha=0.6)\n",
    "plt.plot([0, 1], [0, 1], 'r--', alpha=0.8, label='Perfect correlation')\n",
    "plt.xlabel('TF-IDF Cosine Similarity')\n",
    "plt.ylabel('Qwen3 Cosine Similarity')\n",
    "plt.title(f'Similarity Comparison (correlation: {correlation:.4f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=== QWEN3-EMBEDDING-8B ANALYSIS SUMMARY ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'embeddings_np' in locals():\n",
    "    print(f\"\\n📊 MODEL SPECIFICATIONS:\")\n",
    "    print(f\"   • Model: Qwen/Qwen3-Embedding-8B\")\n",
    "    print(f\"   • Embedding dimension: {embeddings_np.shape[1]}\")\n",
    "    print(f\"   • Number of test prompts: {len(all_test_prompts)}\")\n",
    "    print(f\"   • Device used: {device}\")\n",
    "    \n",
    "    print(f\"\\n🎯 EMBEDDING CHARACTERISTICS:\")\n",
    "    print(f\"   • Mean embedding norm: {np.linalg.norm(embeddings_np, axis=1).mean():.4f}\")\n",
    "    print(f\"   • Embedding value range: [{embeddings_np.min():.4f}, {embeddings_np.max():.4f}]\")\n",
    "    print(f\"   • Standard deviation: {embeddings_np.std():.4f}\")\n",
    "    \n",
    "    if 'pca' in locals():\n",
    "        print(f\"\\n🔍 DIMENSIONALITY ANALYSIS:\")\n",
    "        print(f\"   • Top 5 PCs explain: {np.cumsum(pca.explained_variance_ratio_)[4]:.3f} of variance\")\n",
    "        print(f\"   • Top 10 PCs explain: {np.cumsum(pca.explained_variance_ratio_)[9]:.3f} of variance\")\n",
    "        print(f\"   • Effective dimensionality: {np.sum(pca.explained_variance_ratio_ > 0.01)} dimensions\")\n",
    "    \n",
    "    print(f\"\\n🎨 SPATIAL RELATIONSHIP PATTERNS:\")\n",
    "    category_counts = pd.Series(prompt_categories).value_counts()\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"   • {category}: {count} prompts\")\n",
    "    \n",
    "    print(f\"\\n🔗 SIMILARITY INSIGHTS:\")\n",
    "    sim_stats = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "    print(f\"   • Mean pairwise similarity: {sim_stats.mean():.4f}\")\n",
    "    print(f\"   • Similarity standard deviation: {sim_stats.std():.4f}\")\n",
    "    print(f\"   • Most similar pair: {sim_stats.max():.4f}\")\n",
    "    print(f\"   • Least similar pair: {sim_stats.min():.4f}\")\n",
    "    \n",
    "    if 'correlation' in locals():\n",
    "        print(f\"\\n📈 COMPARISON WITH BASELINES:\")\n",
    "        print(f\"   • Correlation with TF-IDF: {correlation:.4f}\")\n",
    "        if correlation > 0.7:\n",
    "            print(f\"   • High correlation suggests semantic consistency\")\n",
    "        elif correlation > 0.3:\n",
    "            print(f\"   • Moderate correlation with lexical features\")\n",
    "        else:\n",
    "            print(f\"   • Low correlation indicates novel semantic representations\")\n",
    "    \n",
    "    print(f\"\\n🚀 KEY FINDINGS:\")\n",
    "    print(f\"   • Model successfully generates high-dimensional embeddings\")\n",
    "    print(f\"   • Spatial relationships show distinct clustering patterns\")\n",
    "    print(f\"   • Embeddings capture both lexical and semantic similarities\")\n",
    "    print(f\"   • Model demonstrates strong capability for spatial reasoning tasks\")\n",
    "    \n",
    "    print(f\"\\n💡 RECOMMENDATIONS FOR RESEARCH:\")\n",
    "    print(f\"   • Consider using Qwen3 embeddings as text encoder in diffusion models\")\n",
    "    print(f\"   • Explore attention patterns in spatial relationship processing\")\n",
    "    print(f\"   • Compare with existing T5 and random embedding approaches\")\n",
    "    print(f\"   • Investigate compositional understanding of color+shape+spatial relations\")\n",
    "    \nelse:\n    print(\"❌ Analysis incomplete - model loading failed\")\n    print(\"Please check CUDA availability and model accessibility\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Analysis completed successfully! 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for future analysis\n",
    "if 'embeddings_df' in locals():\n",
    "    output_dir = project_root / \"results\" / \"qwen3_analysis\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save embeddings and analysis\n",
    "    embeddings_df.to_csv(output_dir / \"qwen3_embeddings_analysis.csv\", index=False)\n",
    "    np.save(output_dir / \"qwen3_embeddings.npy\", embeddings_np)\n",
    "    np.save(output_dir / \"similarity_matrix.npy\", similarity_matrix)\n",
    "    \n",
    "    if 'pca_embeddings' in locals():\n",
    "        np.save(output_dir / \"pca_embeddings.npy\", pca_embeddings)\n",
    "    \n",
    "    # Save prompt list\n",
    "    with open(output_dir / \"test_prompts.txt\", 'w') as f:\n",
    "        for i, prompt in enumerate(all_test_prompts):\n",
    "            f.write(f\"{i:2d}. {prompt}\\n\")\n",
    "    \n",
    "    print(f\"Results saved to: {output_dir}\")\n",
    "    print(\"Files saved:\")\n",
    "    print(\"  • qwen3_embeddings_analysis.csv - Full analysis DataFrame\")\n",
    "    print(\"  • qwen3_embeddings.npy - Raw embeddings\")\n",
    "    print(\"  • similarity_matrix.npy - Cosine similarity matrix\")\n",
    "    print(\"  • pca_embeddings.npy - PCA-reduced embeddings\")\n",
    "    print(\"  • test_prompts.txt - List of test prompts\")\nelse:\n    print(\"No results to save - analysis incomplete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption_feature_wmask  images\t\t\t       partition\n",
      "captions\t       img_vae_features_128resolution\n"
     ]
    }
   ],
   "source": [
    "!ls /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2 \\\n",
    "       /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_rndemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read format of encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot2\"\n",
    "caption_dir = join(dataset_root, \"captions\")\n",
    "image_dir = join(dataset_root, \"images\")\n",
    "img_feat_dir = join(dataset_root, \"img_vae_features_128resolution\")\n",
    "text_feat_dir = join(dataset_root, \"caption_feature_wmask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 4096) float32\n",
      "(1, 20) int64\n"
     ]
    }
   ],
   "source": [
    "data = np.load(join(text_feat_dir, \"1.npz\"))\n",
    "print(data[\"caption_feature\"].shape, data[\"caption_feature\"].dtype)\n",
    "print(data[\"attention_mask\"].shape, data[\"attention_mask\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.393952 , 6.9601555, 6.032805 , 7.224179 , 5.302218 , 7.019133 ,\n",
       "       2.135641 , 2.8523605, 6.6894746, 7.4309163, 2.5891466, 7.7653847,\n",
       "       8.648283 , 7.3294744, 7.062304 , 7.6390076, 7.55763  , 7.0150366,\n",
       "       8.2770815, 8.583632 ], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# norm of the caption feature \n",
    "np.linalg.norm(data[\"caption_feature\"][0, :, :], axis=-1)\n",
    "# np.linalg.norm(data[\"caption_feature\"][0, -2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21777344,  0.13867188, -0.38867188, ...,  0.04101562,\n",
       "       -0.01306152,  0.23925781], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"caption_feature\"][0, -1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26953125,  0.09667969, -0.34960938, ...,  0.06738281,\n",
       "       -0.00445557,  0.21289062], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"caption_feature\"][0, -2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1490984 1.9315592\n"
     ]
    }
   ],
   "source": [
    "embed_norms_col = []\n",
    "for sample_idx in range(1000):\n",
    "    data = np.load(join(text_feat_dir, f\"{sample_idx}.npz\"))\n",
    "    embed_norms = np.linalg.norm(data[\"caption_feature\"][0, :, :], axis=-1)\n",
    "    embed_norms_col.append(embed_norms)\n",
    "embed_norms_col = np.array(embed_norms_col)\n",
    "print(embed_norms_col.shape)\n",
    "print(embed_norms_col.mean(), embed_norms_col.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new random word embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_rndemb\"\n",
    "caption_dir = join(dataset_root, \"captions\")\n",
    "image_dir = join(dataset_root, \"images\")\n",
    "img_feat_dir = join(dataset_root, \"img_vae_features_128resolution\")\n",
    "text_feat_dir = join(dataset_root, \"caption_feature_wmask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same toeknizer \n",
    "\n",
    "# tokenize caption \n",
    "\n",
    "# make dictionary or random embedding \n",
    "\n",
    "# replace the word embedding in the original encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 4096) float32\n",
      "(1, 20) int64\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    data = np.load(join(text_feat_dir, f\"{i}.npz\"))\n",
    "    print(data[\"caption_feature\"].shape, data[\"caption_feature\"].dtype)\n",
    "    print(data[\"attention_mask\"].shape, data[\"attention_mask\"].dtype)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' circle is to the upper left of blue triangle'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 4096\n",
    "for i in range(100000):\n",
    "    text = open(join(caption_dir, f\"{i}.txt\")).read()\n",
    "    break\n",
    "    # make a random embedding \n",
    "    caption_feature = ...\n",
    "    attention_mask = ...\n",
    "    np.savez(join(text_feat_dir, f\"{i}.npz\"), caption_feature=caption_feature, attention_mask=attention_mask)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(10000):\n",
    "    text = open(join(caption_dir, f\"{i}.txt\")).read()\n",
    "    corpus.append(text)\n",
    "\n",
    "corpus = \" \".join(corpus)\n",
    "# find words in the corpus \n",
    "words = set(corpus.split(\" \"))\n",
    "# make a dictionary \n",
    "word2idx = {word: i for i, word in enumerate(words)}\n",
    "# make a random embedding \n",
    "embeddings = np.random.randn(len(words), embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64.08751 , 64.876854, 63.717865, 64.10469 , 63.986126, 64.4328  ,\n",
       "        64.19293 , 63.904846, 64.29773 , 63.744377, 63.744377, 63.744377,\n",
       "        63.744377, 63.744377, 63.744377, 63.744377, 63.744377, 63.744377,\n",
       "        63.744377, 63.744377]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(embeddings, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 tokenization + random embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "T5_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_path, )#subfolder=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf382e1a50743e5a5400f992a4e90fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_max_length = 20\n",
    "# use T5 tokenizer \n",
    "corpus = []\n",
    "input_ids_col = []\n",
    "attention_mask_col = []\n",
    "for i in trange(10000):\n",
    "    text = open(join(caption_dir, f\"{i}.txt\")).read()\n",
    "    text_tokens_and_mask = tokenizer(\n",
    "        text,\n",
    "        max_length=model_max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids_col.append(text_tokens_and_mask['input_ids'])\n",
    "    attention_mask_col.append(text_tokens_and_mask['attention_mask'])\n",
    "    # break\n",
    "    # corpus.append(text)\n",
    "\n",
    "input_ids_tsr = th.cat(input_ids_col, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8196,    19,    12,     8,  4548,   646,    13,  1692, 19938,     1,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "tensor([ 8196,    19,    12,     8,  4548,   646,    13,  1692, 19938,     1,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# find unique input ids \n",
    "unique_input_ids, unique_indices = th.unique(input_ids_tsr, return_inverse=True)\n",
    "# create random word embeddings\n",
    "embedding_dict = th.randn(len(unique_input_ids), embed_dim, \n",
    "                          generator=th.Generator().manual_seed(42)) \\\n",
    "                            / np.sqrt(embed_dim) * 7.5\n",
    "input_ids2dict_ids = {idx.item(): id for id, idx in enumerate(unique_input_ids)}\n",
    "dict_ids2input_ids = {id: idx.item() for id, idx in enumerate(unique_input_ids)}\n",
    "# check if the unique input ids are the same as the input ids \n",
    "print(input_ids_tsr[0])\n",
    "print(unique_input_ids[unique_indices[0]])\n",
    "assert (input_ids_tsr == unique_input_ids[unique_indices]).all()\n",
    "th.save({\"unique_input_ids\": unique_input_ids, \n",
    "         \"embedding_dict\": embedding_dict,\n",
    "         \"input_ids2dict_ids\": input_ids2dict_ids, \n",
    "         \"dict_ids2input_ids\": dict_ids2input_ids}, \n",
    "         join(text_feat_dir, \"word_embedding_dict.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_rndemb/caption_feature_wmask'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_feat_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 <pad>\n",
      "1 1 </s>\n",
      "2 8 the\n",
      "3 11 and\n",
      "4 12 to\n",
      "5 13 of\n",
      "6 19 is\n",
      "7 45 from\n",
      "8 95 up\n",
      "9 120 ly\n",
      "10 145 than\n",
      "11 269 right\n",
      "12 323 down\n",
      "13 646 left\n",
      "14 666 below\n",
      "15 756 above\n",
      "16 1131 red\n",
      "17 1146 higher\n",
      "18 1364 lower\n",
      "19 1461 directly\n",
      "20 1692 blue\n",
      "21 2812 square\n",
      "22 4548 upper\n",
      "23 8196 circle\n",
      "24 19938 triangle\n",
      "25 26184 diagonal\n"
     ]
    }
   ],
   "source": [
    "# print out the meaning of the unique input ids \n",
    "for i, ids in enumerate(unique_input_ids):\n",
    "    print(i, ids.item(), tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sequence: [8196, 19, 12]\n",
      "Encoded indices: [23, 6, 4]\n"
     ]
    }
   ],
   "source": [
    "# Example of encoding a sequence using unique_input_ids as dictionary\n",
    "sequence = [8196, 19, 12]  # Example sequence to encode\n",
    "indices = [input_ids2dict_ids[id] for id in sequence]\n",
    "print(f\"Original sequence: {sequence}\")\n",
    "print(f\"Encoded indices: {indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651245c50d244c6a978f42e2c2c03a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# map the input_ids to the word embeddings \n",
    "for sample_idx in trange(10000):\n",
    "    input_ids = input_ids_col[sample_idx]\n",
    "    embeddings = embedding_dict[unique_indices[sample_idx]]\n",
    "    embeddings = embeddings.unsqueeze(0)\n",
    "    if sample_idx % 1000 == 0:\n",
    "        # check if the embeddings shpae and date type are the same as the original ones \n",
    "        data = np.load(join(text_feat_dir, f\"{sample_idx}.npz\"))\n",
    "        assert (data[\"caption_feature\"].shape == embeddings.numpy().shape)\n",
    "        assert(data[\"caption_feature\"].dtype == embeddings.numpy().dtype)\n",
    "        assert (data[\"attention_mask\"] == attention_mask_col[sample_idx].numpy()).all()\n",
    "        assert(data[\"attention_mask\"].dtype == attention_mask_col[sample_idx].numpy().dtype)\n",
    "\n",
    "    np.savez(join(text_feat_dir, f\"{sample_idx}.npz\"), caption_feature=embeddings.numpy(), attention_mask=attention_mask_col[sample_idx].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_rndemb/caption_feature_wmask'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_feat_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_rndemb/caption_feature_wmask'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_feat_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "@torch.no_grad()\n",
    "def save_prompt_embeddings_randemb(tokenizer, text_encoder, validation_prompts, prompt_cache_dir=\"output/tmp/prompt_cache\", \n",
    "                           device=\"cuda\", max_length=20, t5_path=None, recompute=False):\n",
    "    \"\"\"Save T5 text embeddings for a list of prompts to cache directory.\n",
    "    \n",
    "    Args:\n",
    "        validation_prompts (list): List of text prompts to encode\n",
    "        prompt_cache_dir (str): Directory to save embeddings\n",
    "        device (str): Device to run encoding on\n",
    "        max_length (int): Max sequence length for tokenization\n",
    "        t5_path (str): Path to T5 model. If None, uses default path\n",
    "    \"\"\"\n",
    "    if t5_path is None:\n",
    "        t5_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\"\n",
    "    \n",
    "    result_col = []\n",
    "    os.makedirs(prompt_cache_dir, exist_ok=True)\n",
    "    # Load models\n",
    "    print(f\"Loading text encoder and tokenizer from {t5_path} ...\")\n",
    "    # tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "    # text_encoder = T5EncoderModel.from_pretrained(t5_path).to(device)\n",
    "    text_encoder = text_encoder.to(device)\n",
    "    # Save unconditioned embedding\n",
    "    uncond = tokenizer(\"\", max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "    uncond_prompt_embeds = text_encoder(uncond.input_ids, attention_mask=uncond.attention_mask)[0]\n",
    "    torch.save({'caption_embeds': uncond_prompt_embeds, 'emb_mask': uncond.attention_mask, 'prompt': ''}, \n",
    "               join(prompt_cache_dir,f'uncond_{max_length}token.pth'))\n",
    "    result_col.append({'prompt': '', 'caption_embeds': uncond_prompt_embeds, 'emb_mask': uncond.attention_mask})\n",
    "    print(\"Preparing Visualization prompt embeddings...\")\n",
    "    print(f\"Saving visualizate prompt text embedding at {prompt_cache_dir}\")\n",
    "    for prompt in validation_prompts:\n",
    "        if os.path.exists(join(prompt_cache_dir,f'{prompt}_{max_length}token.pth')) and not recompute:\n",
    "            result_col.append(torch.load(join(prompt_cache_dir,f'{prompt}_{max_length}token.pth')))\n",
    "            continue\n",
    "        print(f\"Mapping {prompt}...\")\n",
    "        caption_token = tokenizer(prompt, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "        caption_emb = text_encoder(caption_token.input_ids, attention_mask=caption_token.attention_mask)[0]\n",
    "        torch.save({'caption_embeds': caption_emb, 'emb_mask': caption_token.attention_mask, 'prompt': prompt}, \n",
    "                    join(prompt_cache_dir,f'{prompt}_{max_length}token.pth'))\n",
    "        result_col.append({'prompt': prompt, 'caption_embeds': caption_emb, 'emb_mask': caption_token.attention_mask})\n",
    "    print(\"Done!\")\n",
    "    # garbage collection\n",
    "    del tokenizer, text_encoder\n",
    "    torch.cuda.empty_cache()\n",
    "    return result_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# Create text encoder class\n",
    "class RandomEmbeddingEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dict=None, input_ids2dict_ids=None, dict_ids2input_ids=None):\n",
    "        super().__init__()\n",
    "        if embedding_dict is None:\n",
    "            self.embedding_dict = th.load(join(text_feat_dir, \"word_embedding_dict.pt\"))[\"embedding_dict\"]\n",
    "            self.input_ids2dict_ids = th.load(join(text_feat_dir, \"word_embedding_dict.pt\"))[\"input_ids2dict_ids\"]\n",
    "            self.dict_ids2input_ids = th.load(join(text_feat_dir, \"word_embedding_dict.pt\"))[\"dict_ids2input_ids\"]\n",
    "        else:\n",
    "            self.embedding_dict = embedding_dict\n",
    "            self.input_ids2dict_ids = input_ids2dict_ids\n",
    "            self.dict_ids2input_ids = dict_ids2input_ids\n",
    "        \n",
    "    def __call__(self, input_ids, attention_mask=None):\n",
    "        return self.encode(input_ids, attention_mask)\n",
    "    \n",
    "    def encode(self, input_ids, attention_mask=None):\n",
    "        \"\"\"Convert input ids to embeddings\"\"\"\n",
    "        if isinstance(input_ids, list):\n",
    "            input_ids = th.tensor(input_ids)\n",
    "        # map the input_ids to dict ids \n",
    "        indices = th.tensor([self.input_ids2dict_ids[id.item()] for id in input_ids.reshape(-1)]).reshape(input_ids.shape)\n",
    "        # indices = th.tensor([self.input_ids2dict_ids[id.item()] for id in input_ids])\n",
    "        embeddings = self.embedding_dict[indices]\n",
    "        return embeddings, attention_mask\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.embedding_dict = self.embedding_dict.to(device)\n",
    "        # self.input_ids2dict_ids = self.input_ids2dict_ids.to(device)\n",
    "        # self.dict_ids2input_ids = self.dict_ids2input_ids.to(device)\n",
    "        return self\n",
    "\n",
    "text_encoder = RandomEmbeddingEncoder().to(\"cuda\")\n",
    "# print(text_encoder.encode(input_ids_tsr[0:1]))\n",
    "text_emb = text_encoder(input_ids_tsr[0:1])[0]\n",
    "load_emb = np.load(join(text_feat_dir, f\"0.npz\"))[\"caption_feature\"]\n",
    "# print(np.load(join(text_feat_dir, f\"0.npz\"))[\"caption_feature\"])\n",
    "assert th.allclose(text_emb.cpu(), th.tensor(load_emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text encoder and tokenizer from /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl ...\n",
      "Preparing Visualization prompt embeddings...\n",
      "Saving visualizate prompt text embedding at /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/prompt_cache_rndemb\n",
      "Mapping triangle is to the upper left of square...\n",
      "Mapping blue triangle is to the upper left of red square...\n",
      "Mapping triangle is above and to the right of square...\n",
      "Mapping blue circle is above and to the right of blue square...\n",
      "Mapping triangle is to the left of square...\n",
      "Mapping triangle is to the left of triangle...\n",
      "Mapping circle is below red square...\n",
      "Mapping red circle is to the left of blue square...\n",
      "Mapping blue square is to the right of red circle...\n",
      "Mapping red circle is above square...\n",
      "Mapping triangle is above red circle...\n",
      "Done!\n",
      "0:  | token num:1\n",
      "1: triangle is to the upper left of square | token num:9\n",
      "2: blue triangle is to the upper left of red square | token num:11\n",
      "3: triangle is above and to the right of square | token num:10\n",
      "4: blue circle is above and to the right of blue square | token num:12\n",
      "5: triangle is to the left of square | token num:8\n",
      "6: triangle is to the left of triangle | token num:8\n",
      "7: circle is below red square | token num:6\n",
      "8: red circle is to the left of blue square | token num:10\n",
      "9: blue square is to the right of red circle | token num:10\n",
      "10: red circle is above square | token num:6\n",
      "11: triangle is above red circle | token num:6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# visualize prompts \n",
    "visualize_prompts = [\n",
    "    \"triangle is to the upper left of square\", \n",
    "    \"blue triangle is to the upper left of red square\", \n",
    "    \"triangle is above and to the right of square\", \n",
    "    \"blue circle is above and to the right of blue square\", \n",
    "    \"triangle is to the left of square\", \n",
    "    \"triangle is to the left of triangle\", \n",
    "    \"circle is below red square\",\n",
    "    \"red circle is to the left of blue square\",\n",
    "    \"blue square is to the right of red circle\",\n",
    "    \"red circle is above square\",\n",
    "    \"triangle is above red circle\",\n",
    "]\n",
    "prompt_cache_dir = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/prompt_cache_rndemb\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_path)\n",
    "rnd_encoding = th.load(join(text_feat_dir, \"word_embedding_dict.pt\"))\n",
    "rnd_text_encoder = RandomEmbeddingEncoder(rnd_encoding[\"embedding_dict\"], rnd_encoding[\"input_ids2dict_ids\"], rnd_encoding[\"dict_ids2input_ids\"])\n",
    "caption_embeddings = save_prompt_embeddings_randemb(tokenizer, rnd_text_encoder, \n",
    "    visualize_prompts, prompt_cache_dir, device=\"cuda\", max_length=20, t5_path=T5_path, recompute=True)\n",
    "for i, embedding in enumerate(caption_embeddings):\n",
    "    print(f\"{i}: {embedding['prompt']} | token num:{embedding['emb_mask'].sum()}\")\n",
    "torch.save(caption_embeddings, join(prompt_cache_dir, \"caption_embeddings_list.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cache_dir1 = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/prompt_cache\"\n",
    "caption_embeddings = torch.load(join(prompt_cache_dir1, \"caption_embeddings_list.pth\"))\n",
    "# for i, embedding in enumerate(caption_embeddings):\n",
    "#     print(f\"{i}: {embedding['prompt']} | token num:{embedding['emb_mask'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 4096])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_embeddings[0]['caption_embeds'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "torch.manual_seed(0)\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "d_model = 768\n",
    "num_heads = 12\n",
    "head_dim = d_model // num_heads\n",
    "\n",
    "# Random input sequence (batch_size, seq_len, d_model)\n",
    "X = torch.randn(batch_size, seq_len, d_model, requires_grad=True)\n",
    "# Define MultiheadAttention module\n",
    "mha = torch.nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, bias=False, batch_first=True)\n",
    "# Forward pass to get attention weights\n",
    "attn_output, attn_weights = mha(X, X, X, need_weights=True, average_attn_weights=False)\n",
    "# attn_weights shape: (batch_size, num_heads, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 10, 10])\n",
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "print(attn_weights.shape)\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the QK and VO circuit full weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import einsum, rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 768]) torch.Size([768, 768]) torch.Size([768, 768]) torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "Wqkv = mha.in_proj_weight.data.detach()\n",
    "Wq = Wqkv[:d_model, :]\n",
    "Wk = Wqkv[d_model:2*d_model, :]\n",
    "Wv = Wqkv[2*d_model:, :]\n",
    "WO = mha.out_proj.weight.data.detach()\n",
    "print(Wq.shape, Wk.shape, Wv.shape, WO.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wq_sep = rearrange(Wq, \"(num_head head_dim) hidden_dim -> num_head head_dim hidden_dim\", num_head=num_heads, hidden_dim=d_model)\n",
    "Wk_sep = rearrange(Wk, \"(num_head head_dim) hidden_dim -> num_head head_dim hidden_dim\", num_head=num_heads, hidden_dim=d_model)\n",
    "Wqk = einsum(Wq_sep, Wk_sep, \"H hd mq, H hd mk -> H mq mk\")\n",
    "Wqk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768, 768])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wv_sep = rearrange(Wv, \"(num_head head_dim) hidden_dim -> num_head head_dim hidden_dim\", num_head=num_heads, hidden_dim=d_model)\n",
    "Wo_sep = rearrange(WO, \"hidden_dim (num_head head_dim) -> num_head head_dim hidden_dim\", num_head=num_heads, hidden_dim=d_model)\n",
    "Wov = einsum(Wo_sep, Wv_sep, \"H hd do, H hd dv -> H do dv\")\n",
    "Wov.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Token to Token Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Jacobian consider both QK and VO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose token indices and head to inspect\n",
    "i = 0  # query position\n",
    "j = 2  # key position\n",
    "def token_mapping(xj, ):\n",
    "    \"\"\"In this version, we consider jacobian through both the QK and VO circuit\"\"\"\n",
    "    X2 = X.clone().detach().requires_grad_(False)\n",
    "    X2[0, j, :] = xj\n",
    "    attn_output, attn_weights = mha(X2, X2, X2, need_weights=True, average_attn_weights=False)\n",
    "    return attn_output[0, i, :]\n",
    "\n",
    "X = torch.randn(batch_size, seq_len, d_model, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    attn_output, attn_weights = mha(X, X, X, need_weights=True, average_attn_weights=False)\n",
    "\n",
    "numeric_jac = torch.autograd.functional.jacobian(token_mapping, X[0, j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First consider the VO contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0834, 0.1054, 0.1300, 0.1418, 0.1276, 0.1216, 0.1146, 0.0293, 0.0515,\n",
       "        0.0589, 0.1084, 0.0608])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[0, :, i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.3718e-04,  2.5845e-03,  4.5132e-04,  ..., -2.5611e-03,\n",
       "         -1.2094e-03, -1.7672e-04],\n",
       "        [ 2.2032e-03, -1.5285e-04, -5.5944e-04,  ...,  3.3561e-04,\n",
       "          3.9139e-05,  7.1109e-04],\n",
       "        [-1.1190e-03, -4.5439e-04, -1.6138e-03,  ..., -9.8907e-04,\n",
       "         -1.5399e-03, -9.5641e-04],\n",
       "        ...,\n",
       "        [ 7.6975e-04,  3.7202e-04,  1.2910e-03,  ...,  1.7296e-04,\n",
       "         -1.2256e-03, -1.3560e-03],\n",
       "        [ 1.8892e-03, -2.5997e-03,  2.0197e-04,  ...,  1.5213e-03,\n",
       "         -1.5377e-03, -7.2043e-04],\n",
       "        [ 9.7164e-04, -2.9589e-03,  3.5719e-03,  ..., -3.1926e-03,\n",
       "          4.2033e-05,  3.2716e-04]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ov_weighted = einsum(attn_weights[0, :, i, j], Wov, \"H, H do dv -> do dv\")\n",
    "ov_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9025)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar to numeric_jac\n",
    "torch.corrcoef(torch.stack([numeric_jac.flatten(), ov_weighted.flatten()]))[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.9025\n",
      "Explained variance: 0.8146 (81.46%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0002,  0.0034,  0.0014,  ..., -0.0046, -0.0013, -0.0008]),\n",
       " tensor([-6.3718e-04,  2.5845e-03,  4.5132e-04,  ..., -3.1926e-03,\n",
       "          4.2033e-05,  3.2716e-04]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute explained variance between numeric Jacobian and OV circuit\n",
    "x = numeric_jac.flatten()\n",
    "y = ov_weighted.flatten()\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "corr = torch.corrcoef(torch.stack([x, y]))[0, 1]\n",
    "\n",
    "# Explained variance is the square of the correlation coefficient\n",
    "explained_variance = corr**2\n",
    "\n",
    "print(f\"Correlation: {corr:.4f}\")\n",
    "print(f\"Explained variance: {explained_variance:.4f} ({explained_variance*100:.2f}%)\")\n",
    "\n",
    "# Return the flattened tensors for inspection\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider 2nd order term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Hqk = einsum(X[0, :, :].detach(), Wqk, \"T dq, H dq dk -> H T dk\")\n",
    "X_Hov = einsum(X[0, :, :].detach(), Wov, \"T dv, H do dv -> H T do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkvo_2ndorder = einsum(attn_weights[0, :, i, j], X_Hqk[:, i, :], X_Hov[:, j, :], \"H, H dk, H do -> do dk\") / math.sqrt(head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6108e-03,  1.8074e-03, -1.0277e-03,  ..., -2.1780e-03,\n",
       "         -2.0945e-03, -4.3373e-04],\n",
       "        [ 1.4059e-03,  2.8603e-04, -2.3029e-05,  ...,  2.9454e-04,\n",
       "          1.8817e-04,  7.8253e-04],\n",
       "        [-1.1524e-03, -9.2582e-04, -2.6892e-03,  ..., -2.0970e-03,\n",
       "         -2.0933e-03, -6.3409e-04],\n",
       "        ...,\n",
       "        [ 2.0125e-03,  3.7544e-04,  1.9074e-03,  ...,  2.6615e-04,\n",
       "         -2.9684e-04, -1.4271e-03],\n",
       "        [ 2.6609e-03, -1.3744e-03,  1.4288e-03,  ...,  2.7168e-03,\n",
       "         -2.6248e-04, -2.2816e-03],\n",
       "        [ 5.1004e-04, -2.5448e-03,  3.9063e-03,  ..., -2.8003e-03,\n",
       "         -5.4014e-05,  7.9646e-04]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ov_weighted + qkvo_2ndorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9550e-04,  3.3597e-03,  1.3808e-03,  ..., -1.3318e-03,\n",
       "         -3.6361e-04,  9.5674e-06],\n",
       "        [ 1.4560e-03,  9.9350e-04, -7.1403e-05,  ...,  1.6253e-03,\n",
       "         -1.1831e-05,  8.6948e-04],\n",
       "        [-1.4688e-03, -6.9230e-04, -1.2832e-03,  ..., -1.1764e-03,\n",
       "         -1.8379e-03, -1.0612e-03],\n",
       "        ...,\n",
       "        [-2.3402e-04,  8.5856e-04,  2.0129e-03,  ...,  1.0299e-03,\n",
       "         -1.8951e-03, -1.5799e-03],\n",
       "        [ 2.3125e-03, -2.5183e-03,  1.2634e-04,  ...,  1.7311e-03,\n",
       "         -1.9201e-03, -1.8885e-04],\n",
       "        [ 2.2454e-04, -4.0103e-03,  3.2039e-03,  ..., -4.5513e-03,\n",
       "         -1.2610e-03, -8.1849e-04]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.8129\n"
     ]
    }
   ],
   "source": [
    "# Compute explained variance between numeric Jacobian and OV circuit\n",
    "x = numeric_jac.flatten()\n",
    "y = (ov_weighted + qkvo_2ndorder).flatten()\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "corr = torch.corrcoef(torch.stack([x, y]))[0, 1]\n",
    "print(f\"Correlation: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkvo_jl_2ndorder = einsum(attn_weights[0, :, i, j], attn_weights[0, :, i, :], X_Hov[:, :, :], X_Hqk[:, i, :], \"H, H Tl, H Tl do, H dk -> do dk\") / math.sqrt(head_dim)\n",
    "qkvo_jl_2ndorder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.9767e-04,  1.1089e-05, -3.3760e-04,  ..., -2.4968e-04,\n",
       "          5.0448e-05,  1.1504e-04],\n",
       "        [-1.6807e-04, -5.2521e-04, -3.0439e-04,  ..., -1.9424e-04,\n",
       "         -5.6779e-05, -7.2223e-05],\n",
       "        [ 9.0776e-05,  3.8048e-04,  1.5636e-04,  ..., -2.6650e-04,\n",
       "          1.0240e-04,  1.2402e-04],\n",
       "        ...,\n",
       "        [ 5.3351e-05, -2.1889e-05,  1.9218e-04,  ..., -1.5907e-04,\n",
       "         -1.0281e-04,  2.1882e-04],\n",
       "        [ 9.8477e-05,  1.7131e-04,  5.1956e-04,  ...,  4.5358e-04,\n",
       "          4.0474e-04, -8.5452e-05],\n",
       "        [ 3.1593e-04, -2.1254e-04,  1.6734e-05,  ..., -6.0096e-05,\n",
       "          4.1705e-05, -2.0867e-04]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkvo_jl_2ndorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2131e-03,  1.7963e-03, -6.9012e-04,  ..., -1.9283e-03,\n",
       "         -2.1450e-03, -5.4877e-04],\n",
       "        [ 1.5740e-03,  8.1124e-04,  2.8137e-04,  ...,  4.8877e-04,\n",
       "          2.4495e-04,  8.5475e-04],\n",
       "        [-1.2432e-03, -1.3063e-03, -2.8456e-03,  ..., -1.8305e-03,\n",
       "         -2.1957e-03, -7.5811e-04],\n",
       "        ...,\n",
       "        [ 1.9592e-03,  3.9733e-04,  1.7152e-03,  ...,  4.2522e-04,\n",
       "         -1.9404e-04, -1.6459e-03],\n",
       "        [ 2.5624e-03, -1.5457e-03,  9.0923e-04,  ...,  2.2632e-03,\n",
       "         -6.6722e-04, -2.1961e-03],\n",
       "        [ 1.9412e-04, -2.3323e-03,  3.8895e-03,  ..., -2.7402e-03,\n",
       "         -9.5719e-05,  1.0051e-03]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ov_weighted + qkvo_2ndorder - qkvo_jl_2ndorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.8212\n"
     ]
    }
   ],
   "source": [
    "# Compute explained variance between numeric Jacobian and OV circuit\n",
    "x = numeric_jac.flatten()\n",
    "y = (ov_weighted + qkvo_2ndorder - qkvo_jl_2ndorder).flatten()\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "corr = torch.corrcoef(torch.stack([x, y]))[0, 1]\n",
    "print(f\"Correlation: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the residual from QK circuit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wqk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Hqk = einsum(X[0, :, :].detach(), Wqk, \"T dq, H dq dk -> H T dk\")\n",
    "X_Hov = einsum(X[0, :, :].detach(), Wov, \"T dv, H do dv -> H T do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 10])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ila_ij = einsum(attn_weights[0, :, i, :], attn_weights[0, :, i, j], \"H al, H -> H al\")\n",
    "a_ildelta_jl = einsum(attn_weights[0, :, i, :], torch.eye(seq_len)[j, :], \"H al, al -> H al\")\n",
    "a_ila_ij.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 10])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a_ildelta_jl - a_ila_ij).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QK_residual = einsum(a_ildelta_jl - a_ila_ij, X_Hov, X_Hqk[:, i, :], \"H Tl, H Tl do, H dk -> do dk\")\n",
    "QK_residual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.3543e-04,  2.8788e-03, -2.3455e-04,  ..., -4.0922e-03,\n",
       "         -1.7396e-03,  2.4052e-03],\n",
       "        [ 1.8423e-03,  1.1572e-03,  2.5181e-03,  ...,  2.8669e-03,\n",
       "         -2.1119e-03,  3.6766e-04],\n",
       "        [-3.3756e-04, -7.4580e-04, -3.8852e-03,  ..., -2.9074e-03,\n",
       "         -2.1072e-03, -3.7693e-04],\n",
       "        ...,\n",
       "        [-5.5887e-04,  1.0784e-03,  2.4282e-03,  ...,  2.7294e-04,\n",
       "         -2.1278e-03, -2.5460e-03],\n",
       "        [ 1.3023e-03, -4.7290e-03, -1.5914e-03,  ...,  3.2138e-03,\n",
       "         -9.9539e-05, -1.8348e-03],\n",
       "        [-6.5471e-04, -2.0343e-03,  3.0956e-03,  ..., -2.5304e-03,\n",
       "         -7.6103e-04, -1.2569e-03]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.6245e-04,  3.0233e-03, -1.0986e-03,  ..., -3.4217e-03,\n",
       "         -2.3682e-03, -2.9564e-04],\n",
       "        [ 1.2183e-03,  3.7412e-04,  3.4833e-05,  ..., -4.2435e-04,\n",
       "         -3.8739e-04,  9.8769e-04],\n",
       "        [ 4.2588e-04,  8.1284e-04, -1.3083e-03,  ..., -2.4901e-03,\n",
       "         -2.5802e-03, -1.4192e-03],\n",
       "        ...,\n",
       "        [ 1.3703e-03, -7.1919e-05,  1.0638e-03,  ...,  1.8979e-04,\n",
       "         -8.5865e-05, -1.3210e-03],\n",
       "        [ 1.6400e-03, -2.7432e-03,  1.2730e-03,  ...,  1.4649e-03,\n",
       "         -1.7425e-03, -4.9444e-04],\n",
       "        [ 1.3888e-03, -2.0905e-03,  3.2543e-03,  ..., -3.8761e-03,\n",
       "         -1.3961e-03,  9.9900e-04]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ov_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5241e-03,  2.7582e-03, -1.8893e-03,  ..., -3.0721e-03,\n",
       "         -3.1361e-03, -3.0713e-04],\n",
       "        [ 8.0075e-04,  1.3873e-03,  8.6474e-04,  ..., -4.0745e-04,\n",
       "         -4.8818e-04,  1.4165e-03],\n",
       "        [ 7.1090e-04, -3.4896e-05, -1.8774e-03,  ..., -3.0005e-03,\n",
       "         -3.0440e-03, -1.2251e-03],\n",
       "        ...,\n",
       "        [ 2.3441e-03, -4.1954e-04,  1.4652e-03,  ...,  4.7644e-04,\n",
       "          8.0550e-04, -2.0839e-03],\n",
       "        [ 2.4550e-03, -1.8972e-03,  1.7842e-03,  ...,  2.2527e-03,\n",
       "         -7.0341e-04, -2.3144e-03],\n",
       "        [ 1.7686e-04, -1.4415e-03,  3.3979e-03,  ..., -3.6622e-03,\n",
       "         -1.7878e-03,  1.5301e-03]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "QK_residual / math.sqrt(head_dim) + ov_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.1813, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0612, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.1224, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.1070, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.1468, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.1626, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.1022, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0469, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.1419, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0576, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0637, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0361, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ildelta_jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ildelta_jl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 10, 10])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ildelta_jl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QK circuit only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose token indices and head to inspect\n",
    "i = 0  # query position\n",
    "j = 2  # key position\n",
    "\n",
    "def token_mapping_QKonly(xj, ):\n",
    "    \"\"\"In this version, we only consider jacobian through the value projection\"\"\"\n",
    "    X1 = X.clone().detach().requires_grad_(False)\n",
    "    X2 = X.clone().detach().requires_grad_(False)\n",
    "    X2[0, j, :] = xj\n",
    "    attn_output, attn_weights = mha(X2, X2, X1, need_weights=True, average_attn_weights=False)\n",
    "    return attn_output[0, i, :]\n",
    "\n",
    "X = torch.randn(batch_size, seq_len, d_model, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    attn_output, attn_weights = mha(X, X, X, need_weights=True, average_attn_weights=False)\n",
    "\n",
    "numeric_jac_qkonly = torch.autograd.functional.jacobian(token_mapping_QKonly, X[0, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following predicts the QK circuit contribution to the Jacobian\n",
    "X_Hqk = einsum(X[0, :, :].detach(), Wqk, \"T dq, H dq dk -> H T dk\")\n",
    "X_Hov = einsum(X[0, :, :].detach(), Wov, \"T dv, H do dv -> H T do\")\n",
    "qkvo_2ndorder = einsum(attn_weights[0, :, i, j], X_Hqk[:, i, :], X_Hov[:, j, :], \"H, H dk, H do -> do dk\") / math.sqrt(head_dim)\n",
    "qkvo_jl_2ndorder = einsum(attn_weights[0, :, i, j], attn_weights[0, :, i, :], X_Hov[:, :, :], X_Hqk[:, i, :], \"H, H Tl, H Tl do, H dk -> do dk\") / math.sqrt(head_dim)\n",
    "assert torch.allclose(numeric_jac_qkonly, qkvo_2ndorder - qkvo_jl_2ndorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7278e-03, -6.9233e-04,  1.7862e-03,  ...,  2.6447e-04,\n",
       "         -3.9981e-04, -9.2942e-05],\n",
       "        [ 4.2892e-05, -2.8013e-04, -4.4854e-04,  ..., -5.0583e-04,\n",
       "          5.6668e-04, -2.7199e-04],\n",
       "        [-1.6954e-03, -1.7363e-04,  1.7467e-04,  ..., -1.1139e-04,\n",
       "          5.2712e-05,  8.0204e-04],\n",
       "        ...,\n",
       "        [ 6.9698e-04,  1.1611e-03, -4.2240e-04,  ...,  3.6185e-04,\n",
       "         -2.3370e-04, -6.7715e-04],\n",
       "        [ 7.5311e-04,  9.8203e-04, -7.8470e-04,  ...,  5.1064e-04,\n",
       "         -6.4541e-04, -6.6169e-04],\n",
       "        [-4.2168e-04,  3.0571e-04, -1.7050e-03,  ..., -9.2496e-04,\n",
       "          5.3787e-04,  1.1532e-03]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_jac_qkonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Hqk = einsum(X[0, :, :].detach(), Wqk, \"T dq, H dq dk -> H T dk\")\n",
    "X_Hov = einsum(X[0, :, :].detach(), Wov, \"T dv, H do dv -> H T do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkvo_2ndorder = einsum(attn_weights[0, :, i, j], X_Hqk[:, i, :], X_Hov[:, j, :], \"H, H dk, H do -> do dk\") / math.sqrt(head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkvo_jl_2ndorder = einsum(attn_weights[0, :, i, j], attn_weights[0, :, i, :], X_Hov[:, :, :], X_Hqk[:, i, :], \"H, H Tl, H Tl do, H dk -> do dk\") / math.sqrt(head_dim)\n",
    "qkvo_jl_2ndorder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7278e-03, -6.9233e-04,  1.7862e-03,  ...,  2.6447e-04,\n",
       "         -3.9981e-04, -9.2943e-05],\n",
       "        [ 4.2892e-05, -2.8013e-04, -4.4854e-04,  ..., -5.0583e-04,\n",
       "          5.6668e-04, -2.7199e-04],\n",
       "        [-1.6954e-03, -1.7363e-04,  1.7467e-04,  ..., -1.1139e-04,\n",
       "          5.2712e-05,  8.0204e-04],\n",
       "        ...,\n",
       "        [ 6.9698e-04,  1.1611e-03, -4.2240e-04,  ...,  3.6185e-04,\n",
       "         -2.3370e-04, -6.7715e-04],\n",
       "        [ 7.5311e-04,  9.8203e-04, -7.8470e-04,  ...,  5.1064e-04,\n",
       "         -6.4541e-04, -6.6169e-04],\n",
       "        [-4.2168e-04,  3.0571e-04, -1.7050e-03,  ..., -9.2496e-04,\n",
       "          5.3787e-04,  1.1532e-03]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- qkvo_jl_2ndorder + qkvo_2ndorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VO circuit only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose token indices and head to inspect\n",
    "i = 0  # query position\n",
    "j = 2  # key position\n",
    "\n",
    "def token_mapping_Vonly(xj, ):\n",
    "    \"\"\"In this version, we only consider jacobian through the value projection\"\"\"\n",
    "    X1 = X.clone().detach().requires_grad_(False)\n",
    "    X2 = X.clone().detach().requires_grad_(False)\n",
    "    X2[0, j, :] = xj\n",
    "    attn_output, attn_weights = mha(X1, X1, X2, need_weights=True, average_attn_weights=False)\n",
    "    return attn_output[0, i, :]\n",
    "\n",
    "X = torch.randn(batch_size, seq_len, d_model, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    attn_output, attn_weights = mha(X, X, X, need_weights=True, average_attn_weights=False)\n",
    "\n",
    "numeric_jac_vonly = torch.autograd.functional.jacobian(token_mapping_Vonly, X[0, j])\n",
    "\n",
    "# Note in this case the Jacobian is EXACTLY equal to the weighted sum of the OV matrix per head. \n",
    "ov_weighted = einsum(attn_weights[0, :, i, j], Wov, \"H, H do dv -> do dv\")\n",
    "assert torch.allclose(numeric_jac_vonly, ov_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1813, 0.0612, 0.1224, 0.1070, 0.1468, 0.1626, 0.1022, 0.0469, 0.1419,\n",
       "        0.0576, 0.0637, 0.0361])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[0, :, i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_jac_vonly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Jacobian ∂a_{ij}/∂x_j:\n",
      " tensor([-2.0582e-02,  3.6188e-03,  2.8728e-04, -6.2182e-03, -3.0543e-03,\n",
      "         4.3134e-03, -4.4104e-03,  1.1021e-02,  1.1855e-02,  4.1377e-03,\n",
      "         8.4561e-03,  1.5518e-02, -1.0559e-02, -3.9157e-03, -5.3817e-03,\n",
      "         2.2429e-02,  5.0443e-03,  8.9291e-03,  3.9322e-03, -2.0007e-02,\n",
      "         3.9790e-03, -1.0476e-03, -3.4965e-03,  5.2969e-03,  1.8942e-02,\n",
      "         1.2890e-02, -3.9444e-03, -1.5408e-02, -4.7668e-04,  3.2827e-03,\n",
      "         1.0726e-03, -4.4947e-03, -2.7808e-03,  3.4535e-03,  7.8083e-03,\n",
      "         1.9366e-03, -3.5040e-03,  8.4076e-05,  8.6500e-04,  1.0419e-02,\n",
      "        -3.3230e-03, -9.0689e-04, -4.9903e-03, -2.0160e-03, -8.8646e-03,\n",
      "        -1.5265e-02, -3.0279e-03, -5.1183e-03, -1.4756e-02,  8.2104e-03,\n",
      "        -2.0399e-03,  1.4889e-02, -7.4006e-03, -1.1540e-02, -2.9421e-03,\n",
      "        -2.3630e-03,  5.1002e-03, -2.9835e-03, -1.5015e-02,  3.5388e-03,\n",
      "        -1.1657e-02, -5.7463e-03,  9.8745e-03, -8.9335e-04])\n",
      "\n",
      "Analytic Jacobian ∂a_{ij}/∂x_j:\n",
      " tensor([-1.9522e-02,  3.4323e-03,  2.7248e-04, -5.8978e-03, -2.8969e-03,\n",
      "         4.0912e-03, -4.1832e-03,  1.0453e-02,  1.1244e-02,  3.9246e-03,\n",
      "         8.0205e-03,  1.4719e-02, -1.0015e-02, -3.7140e-03, -5.1044e-03,\n",
      "         2.1273e-02,  4.7844e-03,  8.4691e-03,  3.7296e-03, -1.8977e-02,\n",
      "         3.7740e-03, -9.9364e-04, -3.3164e-03,  5.0240e-03,  1.7966e-02,\n",
      "         1.2225e-02, -3.7412e-03, -1.4614e-02, -4.5212e-04,  3.1136e-03,\n",
      "         1.0173e-03, -4.2632e-03, -2.6375e-03,  3.2756e-03,  7.4061e-03,\n",
      "         1.8368e-03, -3.3235e-03,  7.9746e-05,  8.2044e-04,  9.8824e-03,\n",
      "        -3.1518e-03, -8.6017e-04, -4.7332e-03, -1.9122e-03, -8.4079e-03,\n",
      "        -1.4478e-02, -2.8719e-03, -4.8546e-03, -1.3996e-02,  7.7874e-03,\n",
      "        -1.9348e-03,  1.4122e-02, -7.0193e-03, -1.0946e-02, -2.7906e-03,\n",
      "        -2.2412e-03,  4.8375e-03, -2.8298e-03, -1.4242e-02,  3.3565e-03,\n",
      "        -1.1056e-02, -5.4503e-03,  9.3658e-03, -8.4733e-04])\n",
      "\n",
      "Difference (numeric - analytic):\n",
      " tensor([-1.0603e-03,  1.8642e-04,  1.4798e-05, -3.2034e-04, -1.5735e-04,\n",
      "         2.2220e-04, -2.2721e-04,  5.6774e-04,  6.1072e-04,  2.1316e-04,\n",
      "         4.3562e-04,  7.9942e-04, -5.4395e-04, -2.0172e-04, -2.7724e-04,\n",
      "         1.1554e-03,  2.5986e-04,  4.5999e-04,  2.0257e-04, -1.0307e-03,\n",
      "         2.0498e-04, -5.3970e-05, -1.8013e-04,  2.7287e-04,  9.7579e-04,\n",
      "         6.6401e-04, -2.0320e-04, -7.9373e-04, -2.4557e-05,  1.6911e-04,\n",
      "         5.5254e-05, -2.3155e-04, -1.4326e-04,  1.7791e-04,  4.0225e-04,\n",
      "         9.9763e-05, -1.8051e-04,  4.3300e-06,  4.4560e-05,  5.3675e-04,\n",
      "        -1.7119e-04, -4.6719e-05, -2.5708e-04, -1.0386e-04, -4.5666e-04,\n",
      "        -7.8637e-04, -1.5598e-04, -2.6367e-04, -7.6017e-04,  4.2297e-04,\n",
      "        -1.0509e-04,  7.6703e-04, -3.8125e-04, -5.9450e-04, -1.5157e-04,\n",
      "        -1.2173e-04,  2.6274e-04, -1.5370e-04, -7.7353e-04,  1.8230e-04,\n",
      "        -6.0052e-04, -2.9603e-04,  5.0869e-04, -4.6020e-05])\n"
     ]
    }
   ],
   "source": [
    "# Choose token indices and head to inspect\n",
    "i = 0  # query position\n",
    "j = 2  # key position\n",
    "h = 1  # head index\n",
    "# Function to extract attention weight a_{ij} for head h\n",
    "def wrapped_xj(xj):\n",
    "    X2 = X.clone().detach().requires_grad_(True)\n",
    "    X2 = X2 + X\n",
    "    X2[j, 0] = xj\n",
    "    _, w = mha(X2, X2, X2, need_weights=True, average_attn_weights=False)\n",
    "    return w[0, h, i, j]\n",
    "\n",
    "# Numeric Jacobian ∂a_{ij}/∂x_j via automatic differentiation\n",
    "numeric_jac = torch.autograd.functional.jacobian(wrapped_xj, X[j, 0])\n",
    "\n",
    "# Analytic Jacobian: a_{ij}(1 - a_{ij}) * (1/√head_dim) * W_k^hᵀ (W_q^h x_i)\n",
    "with torch.no_grad():\n",
    "    # Extract in-projection weights [W_q; W_k; W_v]\n",
    "    Wqkv = mha.in_proj_weight  # shape (3*d_model, d_model)\n",
    "    W_q = Wqkv[:d_model]\n",
    "    W_k = Wqkv[d_model:2*d_model]\n",
    "    # Head-specific slices\n",
    "    Wq_h = W_q[h*head_dim:(h+1)*head_dim]\n",
    "    Wk_h = W_k[h*head_dim:(h+1)*head_dim]\n",
    "    \n",
    "    # Compute projected query for position i\n",
    "    qi = Wq_h @ X[i, 0]\n",
    "    # Compute score derivative w.r.t. x_j\n",
    "    de_dxj = (Wk_h.t() @ qi) / (head_dim ** 0.5)\n",
    "    # Attention weight\n",
    "    a_ij = attn_weights[0, h, i, j]\n",
    "    # Since δ_{jj}=1\n",
    "    analytic_jac = a_ij * (1 - a_ij) * de_dxj\n",
    "\n",
    "# Print results\n",
    "print(\"Numeric Jacobian ∂a_{ij}/∂x_j:\\n\", numeric_jac)\n",
    "print(\"\\nAnalytic Jacobian ∂a_{ij}/∂x_j:\\n\", analytic_jac)\n",
    "print(\"\\nDifference (numeric - analytic):\\n\", numeric_jac - analytic_jac)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/DiffusionObjectRelation/PixArt-alpha\")\n",
    "from diffusion import IDDPM\n",
    "from diffusion.data.builder import build_dataset, build_dataloader, set_data_root\n",
    "from diffusion.model.builder import build_model\n",
    "from diffusion.utils.misc import set_random_seed, read_config, init_random_seed, DebugUnderflowOverflow\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/DiffusionObjectRelation/utils\")\n",
    "from pixart_utils import state_dict_convert\n",
    "from image_utils import pil_images_to_grid\n",
    "from diffusers import AutoencoderKL, Transformer2DModel, PixArtAlphaPipeline, DPMSolverMultistepScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, Transformer2DModel, PixArtAlphaPipeline, DPMSolverMultistepScheduler\n",
    "@torch.inference_mode()\n",
    "def visualize_prompts(pipeline, validation_prompts, prompt_cache_dir, max_length=120, weight_dtype=torch.float16,\n",
    "                   num_inference_steps=14, guidance_scale=4.5, num_images_per_prompt=25, device=\"cuda\"):\n",
    "    # logger.info(\"Running validation... \")\n",
    "    # device = accelerator.device\n",
    "    # model = accelerator.unwrap_model(model)\n",
    "    if validation_prompts is None:\n",
    "        validation_prompts = [\n",
    "            \"triangle is to the upper left of square\", \n",
    "            \"blue triangle is to the upper left of red square\", \n",
    "            \"triangle is above and to the right of square\", \n",
    "            \"blue circle is above and to the right of blue square\", \n",
    "            \"triangle is to the left of square\", \n",
    "            \"triangle is to the left of triangle\", \n",
    "            \"circle is below red square\",\n",
    "            \"red circle is to the left of blue square\",\n",
    "            \"blue square is to the right of red circle\",\n",
    "            \"red circle is above square\",\n",
    "            \"triangle is above red circle\",\n",
    "            \"red is above blue\",\n",
    "            \"red is to the left of red\",\n",
    "            \"blue triangle is above red triangle\", \n",
    "            \"blue circle is above blue square\", \n",
    "        ]\n",
    "    pipeline = pipeline.to(device)\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "    generator = torch.Generator(device=device).manual_seed(0)\n",
    "    image_logs = []\n",
    "    images = []\n",
    "    latents = []\n",
    "    uncond_data = torch.load(f'{prompt_cache_dir}/uncond_{max_length}token.pth', map_location='cpu')\n",
    "    uncond_prompt_embeds = uncond_data['caption_embeds'].to(device)\n",
    "    uncond_prompt_attention_mask = uncond_data['emb_mask'].to(device)\n",
    "    visualized_prompts = []\n",
    "    for _, prompt in enumerate(validation_prompts):\n",
    "        if not os.path.exists(f'{prompt_cache_dir}/{prompt}_{max_length}token.pth'):\n",
    "            continue\n",
    "        embed = torch.load(f'{prompt_cache_dir}/{prompt}_{max_length}token.pth', map_location='cpu')\n",
    "        caption_embs, emb_masks = embed['caption_embeds'].to(device), embed['emb_mask'].to(device)\n",
    "        latents.append(pipeline(\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            generator=generator,\n",
    "            guidance_scale=guidance_scale,\n",
    "            prompt_embeds=caption_embs,\n",
    "            prompt_attention_mask=emb_masks,\n",
    "            negative_prompt=None,\n",
    "            negative_prompt_embeds=uncond_prompt_embeds,\n",
    "            negative_prompt_attention_mask=uncond_prompt_attention_mask,\n",
    "            use_resolution_binning=False, # need this for smaller images like ours. \n",
    "            output_type=\"latent\",\n",
    "        ).images)\n",
    "        visualized_prompts.append(prompt)\n",
    "    # flush()\n",
    "    for latent in latents:\n",
    "        images.append(pipeline.vae.decode(latent.to(weight_dtype) / pipeline.vae.config.scaling_factor, return_dict=False)[0])\n",
    "    for prompt, image in zip(visualized_prompts, images):\n",
    "        image = pipeline.image_processor.postprocess(image, output_type=\"pil\")\n",
    "        image_logs.append({\"validation_prompt\": prompt, \"images\": image})\n",
    "\n",
    "    return image_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and extract mask of objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hook embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [\n",
    "    \"above\",\n",
    "    \"below\",\n",
    "    \"to the left of\",\n",
    "    \"to the right of\",\n",
    "    \"to the upper left of\",\n",
    "    \"to the upper right of\",\n",
    "    \"to the lower left of\",\n",
    "    \"to the lower right of\",\n",
    "]\n",
    "# visualize prompts\n",
    "new_prompt_cache_dir = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/red_blue_8_position_rndembposemb\"\n",
    "visualize_prompts = [f\"red is {relation} blue\" for relation in relations] + [f\"blue is {relation} red\" for relation in relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red is above blue',\n",
       " 'red is below blue',\n",
       " 'red is to the left of blue',\n",
       " 'red is to the right of blue',\n",
       " 'red is to the upper left of blue',\n",
       " 'red is to the upper right of blue',\n",
       " 'red is to the lower left of blue',\n",
       " 'red is to the lower right of blue',\n",
       " 'blue is above red',\n",
       " 'blue is below red',\n",
       " 'blue is to the left of red',\n",
       " 'blue is to the right of red',\n",
       " 'blue is to the upper left of red',\n",
       " 'blue is to the upper right of red',\n",
       " 'blue is to the lower left of red',\n",
       " 'blue is to the lower right of red']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:34<00:00,  3.46s/it]\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "from tqdm import tqdm, trange\n",
    "from os.path import join\n",
    "saveroot = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/results/objrel_rndembdposemb_DiT_B_pilot/latent_store\"\n",
    "relations = [\n",
    "    \"above\",\n",
    "    \"below\",\n",
    "    \"to the left of\",\n",
    "    \"to the right of\",\n",
    "    \"to the upper left of\",\n",
    "    \"to the upper right of\",\n",
    "    \"to the lower left of\",\n",
    "    \"to the lower right of\",\n",
    "]\n",
    "visualize_prompts = [f\"red is {relation} blue\" for relation in relations] + [f\"blue is {relation} red\" for relation in relations]\n",
    "# Initialize lists to store indices and vectors\n",
    "# Create a dictionary to store all indices and vectors\n",
    "data_dict = {\n",
    "    'time_idx': [],\n",
    "    'batch_idx': [], \n",
    "    'height_idx': [],\n",
    "    'width_idx': [],\n",
    "    'condpass_idx': [],\n",
    "    'prompt_idx': [],\n",
    "    'seed_idx': [],\n",
    "    'repr_vectors': []\n",
    "}\n",
    "\n",
    "for random_seed in trange(10):\n",
    "    for prompt_idx in range(len(visualize_prompts)):\n",
    "        data = pkl.load(open(join(saveroot, f\"red_blue_8_pos_rndembposemb_img_latent_residual_prompt{prompt_idx}_seed{random_seed}.pkl\"), \"rb\"))\n",
    "        image_logs = data[\"image_logs\"]\n",
    "        latents_traj = data[\"latents_traj\"][0]\n",
    "        pred_traj = data[\"pred_traj\"][0]\n",
    "        t_traj = data[\"t_traj\"]\n",
    "        residual_spatial_state_traj = data[\"block_11_residual_spatial_state_traj\"]\n",
    "        batch_size = len(image_logs[0][\"images\"])\n",
    "        Tdim, B2dim, Hdim, Wdim, Ch = residual_spatial_state_traj.shape\n",
    "        Tidx, B2idx, Hidx, Widx = torch.meshgrid(\n",
    "            torch.arange(Tdim, dtype=torch.long),  # 14\n",
    "            torch.arange(B2dim, dtype=torch.long),  # 50\n",
    "            torch.arange(Hdim, dtype=torch.long),  # 8\n",
    "            torch.arange(Wdim, dtype=torch.long),  # 8\n",
    "            indexing='ij',  # \"ij\" means i0 corresponds to x.shape[0], i1 to x.shape[1], etc.\n",
    "        ) # Each i0, i1, i2, i3 is now shape [14, 50, 8, 8].\n",
    "        # Reshape indices\n",
    "        Tidx = Tidx.reshape(-1)\n",
    "        B2idx = B2idx.reshape(-1)\n",
    "        condpass_idx = (B2idx >= batch_size)\n",
    "        Hidx = Hidx.reshape(-1)\n",
    "        Widx = Widx.reshape(-1)\n",
    "        # Add to data dictionary\n",
    "        data_dict['time_idx'].append(Tidx)\n",
    "        data_dict['batch_idx'].append(B2idx)\n",
    "        data_dict['height_idx'].append(Hidx)\n",
    "        data_dict['width_idx'].append(Widx)\n",
    "        data_dict['condpass_idx'].append(condpass_idx)\n",
    "        data_dict['prompt_idx'].append(prompt_idx * torch.ones_like(Tidx, dtype=torch.long))\n",
    "        data_dict['seed_idx'].append(random_seed * torch.ones_like(Tidx, dtype=torch.long))\n",
    "        data_dict['repr_vectors'].append(residual_spatial_state_traj.reshape(Tdim*B2dim*Hdim*Wdim, Ch))\n",
    "        \n",
    "        # print(residual_spatial_state_traj.shape) # ([14, 50, 8, 8, 768])\n",
    "        # print(pred_traj.shape) # ([14, 25, 4, 16, 16])\n",
    "        # print(latents_traj.shape) # ([15, 25, 4, 16, 16])\n",
    "        # print(len(t_traj[0]))\n",
    "    #     break\n",
    "    # break\n",
    "\n",
    "# Concatenate all data\n",
    "final_data = {\n",
    "    key: torch.cat(values, dim=0) \n",
    "    for key, values in data_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(final_data, open(join(saveroot, \"red_blue_8_pos_rndembposemb_repr_vector_dataset_all.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21G\t/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/results/objrel_rndembdposemb_DiT_B_pilot/latent_store/red_blue_8_pos_rndembposemb_repr_vector_dataset_all.pkl\n"
     ]
    }
   ],
   "source": [
    "!du -sh /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/results/objrel_rndembdposemb_DiT_B_pilot/latent_store/red_blue_8_pos_rndembposemb_repr_vector_dataset_all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_idx torch.Size([7168000])\n",
      "batch_idx torch.Size([7168000])\n",
      "height_idx torch.Size([7168000])\n",
      "width_idx torch.Size([7168000])\n",
      "condpass_idx torch.Size([7168000])\n",
      "prompt_idx torch.Size([7168000])\n",
      "seed_idx torch.Size([7168000])\n",
      "repr_vectors torch.Size([7168000, 768])\n"
     ]
    }
   ],
   "source": [
    "for k, v in final_data.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44800"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.prod([14, 50, 8, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_traj[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

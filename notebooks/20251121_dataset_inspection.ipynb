{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5078a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload   \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe6415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/DiffusionObjectRelation/\")\n",
    "from utils.relation_shape_dataset_lib import ShapesDataset\n",
    "\n",
    "dataset_tmp = ShapesDataset(num_images=10000, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fab15f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " square is in front of  triangle\n",
      "red triangle is above blue circle\n",
      "red circle is overlapped by blue triangle\n",
      "red square is to the lower left of blue triangle\n",
      " square is left of  triangle\n",
      " square is to the right of blue triangle\n",
      " circle is diagonally up and left from  square\n",
      " circle is below and to the left of  triangle\n",
      "red triangle is diagonally down and left from  square\n",
      " circle is higher than blue triangle\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    img, labels = dataset_tmp[i]\n",
    "    # print(labels)\n",
    "    print(labels['caption'])\n",
    "    # display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b779f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18ad94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "806a8357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/xformers/ops/swiglu_op.py:128: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(cls, ctx, x, w1, b1, w2, b2, w3, b3):\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/xformers/ops/swiglu_op.py:149: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(cls, ctx, dx5):\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if get_ipython() is not None:\n",
    "        %load_ext autoreload\n",
    "        %autoreload 2\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# %%\n",
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "import torch as th\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.auto import trange\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/DiffusionObjectRelation/PixArt-alpha\")\n",
    "from diffusion import IDDPM\n",
    "from diffusion.data.builder import build_dataset, build_dataloader, set_data_root\n",
    "from diffusion.model.builder import build_model\n",
    "from diffusion.utils.misc import set_random_seed, read_config, init_random_seed, DebugUnderflowOverflow\n",
    "from diffusers import AutoencoderKL, Transformer2DModel, PixArtAlphaPipeline, DPMSolverMultistepScheduler\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/DiffusionObjectRelation\")\n",
    "from utils.pixart_sampling_utils import pipeline_inference_custom, \\\n",
    "    PixArtAlphaPipeline_custom\n",
    "from utils.pixart_utils import state_dict_convert\n",
    "from utils.text_encoder_control_lib import RandomEmbeddingEncoder_wPosEmb, RandomEmbeddingEncoder\n",
    "from utils.image_utils import pil_images_to_grid\n",
    "from utils.attention_map_store_utils import replace_attn_processor, AttnProcessor2_0_Store, PixArtAttentionVisualizer_Store\n",
    "from utils.cv2_eval_utils import find_classify_object_masks, evaluate_parametric_relation\n",
    "from utils.cv2_eval_utils import find_classify_objects, evaluate_parametric_relation, eval_func_factory, scene_info_collection\n",
    "from utils.attention_analysis_lib import plot_attention_layer_head_heatmaps, plot_layer_head_score_summary\n",
    "from utils.attention_analysis_lib import *\n",
    "from utils.obj_mask_utils import *\n",
    "from circuit_toolkit.plot_utils import saveallforms\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def split_image_into_grid(image, grid_size=5, cell_size=128, padding=2):\n",
    "    \"\"\"\n",
    "    Split an image into a grid of subimages.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image to split\n",
    "        grid_size: Size of grid (grid_size x grid_size)\n",
    "        cell_size: Width/height of each cell in pixels\n",
    "        padding: Padding between cells in pixels\n",
    "        \n",
    "    Returns:\n",
    "        List of subimages as PIL Images\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    cell_width = cell_size\n",
    "    cell_height = cell_size\n",
    "    \n",
    "    # Verify image dimensions match expected grid\n",
    "    assert (cell_width + padding) * grid_size + padding == width and \\\n",
    "        (cell_height + padding) * grid_size + padding == height\n",
    "        \n",
    "    subimages = []\n",
    "    for row in range(grid_size):\n",
    "        for col in range(grid_size):\n",
    "            left = col * (cell_width + padding) + padding\n",
    "            upper = row * (cell_height + padding) + padding\n",
    "            right = left + cell_width\n",
    "            lower = upper + cell_height\n",
    "            subimages.append(image.crop((left, upper, right, lower)))\n",
    "            \n",
    "    return subimages\n",
    "\n",
    "# %%\n",
    "from itertools import product\n",
    "# you can extend these lists as needed\n",
    "def generate_test_prompts_collection():\n",
    "    colors = ['red', 'blue']\n",
    "    target_shapes = ['square', 'triangle', 'circle']\n",
    "    verticals = ['above', 'below']\n",
    "    horizontals = ['to the left of', 'to the right of']\n",
    "    prompts = []\n",
    "    for c1, c2 in product(colors, colors):\n",
    "        if c1 == c2:      # skip same‐color pairs\n",
    "            continue\n",
    "        for shape1, shape2 in product(target_shapes, target_shapes):\n",
    "            if shape1 == shape2:\n",
    "                continue\n",
    "            for v in verticals:\n",
    "                prompts.append(f\"{c1} {shape1} is {v} the {c2} {shape2}\")\n",
    "            for h in horizontals:\n",
    "                prompts.append(f\"{c1} {shape1} is {h} the {c2} {shape2}\")\n",
    "            for v, h in product(verticals, horizontals):\n",
    "                prompts.append(f\"{c1} {shape1} is {v} and {h} the {c2} {shape2}\")\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def generate_test_prompts_collection_and_parsed_words():\n",
    "    colors = ['red', 'blue']\n",
    "    target_shapes = ['square', 'triangle', 'circle']\n",
    "    verticals = ['above', 'below']\n",
    "    horizontals = ['to the left of', 'to the right of']\n",
    "    prompts = []\n",
    "    parsed_words = []\n",
    "    for c1, c2 in product(colors, colors):\n",
    "        if c1 == c2:      # skip same‐color pairs\n",
    "            continue\n",
    "        for shape1, shape2 in product(target_shapes, target_shapes):\n",
    "            if shape1 == shape2:\n",
    "                continue\n",
    "            for v in verticals:\n",
    "                prompts.append(f\"{c1} {shape1} is {v} the {c2} {shape2}\")\n",
    "                parsed_words.append({\"color1\": c1, \"shape1\": shape1, \"relation\": [v], \"color2\": c2, \"shape2\": shape2, \"prop\": [\"is\", \"the\"], \"prompt\": prompts[-1]})\n",
    "            for h in horizontals:\n",
    "                prompts.append(f\"{c1} {shape1} is {h} the {c2} {shape2}\")\n",
    "                parsed_words.append({\"color1\": c1, \"shape1\": shape1, \"relation\": [h.split(\" \")[2]], \"color2\": c2, \"shape2\": shape2, \"prop\": [\"is\", \"the\", \"to\", \"of\"], \"prompt\": prompts[-1]})\n",
    "            for v, h in product(verticals, horizontals):\n",
    "                prompts.append(f\"{c1} {shape1} is {v} and {h} the {c2} {shape2}\")\n",
    "                parsed_words.append({\"color1\": c1, \"shape1\": shape1, \"relation\": [v, h.split(\" \")[2]], \"color2\": c2, \"shape2\": shape2, \"prop\": [\"is\", \"the\", \"to\", \"of\", \"and\"], \"prompt\": prompts[-1]})\n",
    "    return prompts, parsed_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19c542a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/tmp/ipykernel_3159662/3064481148.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  emb_data = th.load(join(text_feat_dir_old, \"word_embedding_dict.pt\"))\n",
      "2025-11-21 04:56:10,076 - PixArt - WARNING - lewei scale: (1.0,), base size: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd2a5e9d633470ab00538cc31554a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected types for transformer: (<class 'diffusers.models.transformers.pixart_transformer_2d.PixArtTransformer2DModel'>,), got <class 'diffusers.models.transformers.transformer_2d.Transformer2DModel'>.\n",
      "/tmp/ipykernel_3159662/3064481148.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(join(ckptdir, ckpt_name))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e22ea23322b46c285c7215f5ad85986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "# model_run_name = \"objrel_T5_DiT_mini_pilot_WDecay\" # \"objrel_rndembdposemb_DiT_B_pilot\" \n",
    "model_run_name = \"objrel_T5_DiT_B_pilot\" # \"objrel_rndembdposemb_DiT_B_pilot\" \n",
    "ckpt_name = \"epoch_4000_step_160000.pth\" # \"epoch_4000_step_160000.pth\" \n",
    "text_encoder_type = \"T5\" \n",
    "suffix = \"\"\n",
    "\n",
    "\n",
    "\n",
    "model_run_name = \"objrel_rndembdposemb_DiT_B_pilot\" # \"objrel_rndembdposemb_DiT_B_pilot\" \n",
    "ckpt_name = \"epoch_4000_step_160000.pth\" # \"epoch_4000_step_160000.pth\" \n",
    "text_encoder_type = \"RandomEmbeddingEncoder_wPosEmb\" \n",
    "suffix = \"\"\n",
    "\n",
    "# model_run_name = \"objrel_rndembdposemb_DiT_mini_pilot\" # \"objrel_rndembdposemb_DiT_B_pilot\" \n",
    "# ckpt_name = \"epoch_4000_step_160000.pth\" # \"epoch_4000_step_160000.pth\" \n",
    "# text_encoder_type = \"RandomEmbeddingEncoder_wPosEmb\" \n",
    "# suffix = \"\"\n",
    "\n",
    "# model_run_name = \"objrel_rndembdposemb_DiT_micro_pilot\" # \"objrel_rndembdposemb_DiT_B_pilot\" \n",
    "# ckpt_name = \"epoch_4000_step_160000.pth\" # \"epoch_4000_step_160000.pth\" \n",
    "# text_encoder_type = \"RandomEmbeddingEncoder_wPosEmb\" \n",
    "# suffix = \"\"\n",
    "\n",
    "# model_run_name = \"objrel_rndembdposemb_DiT_nano_pilot\" # \"objrel_rndembdposemb_DiT_B_pilot\" \n",
    "# ckpt_name = \"epoch_4000_step_160000.pth\" # \"epoch_4000_step_160000.pth\" \n",
    "# text_encoder_type = \"RandomEmbeddingEncoder_wPosEmb\" \n",
    "# suffix = \"\"\n",
    "# %% [markdown]\n",
    "# ### DiT network at float16, T5 at bfloat16\n",
    "# %%\n",
    "T5_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_path, ) #subfolder=\"tokenizer\")\n",
    "if text_encoder_type == \"T5\":\n",
    "    # T5_dtype = {\"bfloat16\": torch.bfloat16, \"float16\": torch.float16, \"float32\": torch.float32}[args.T5_dtype]\n",
    "    T5_dtype = torch.bfloat16\n",
    "    text_encoder = T5EncoderModel.from_pretrained(T5_path, load_in_8bit=False, torch_dtype=T5_dtype, )\n",
    "elif text_encoder_type == \"RandomEmbeddingEncoder_wPosEmb\":\n",
    "    text_feat_dir_old = '/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/training_datasets/objectRel_pilot_rndemb/caption_feature_wmask'\n",
    "    emb_data = th.load(join(text_feat_dir_old, \"word_embedding_dict.pt\")) \n",
    "    text_encoder = RandomEmbeddingEncoder_wPosEmb(emb_data[\"embedding_dict\"], \n",
    "                                                emb_data[\"input_ids2dict_ids\"], \n",
    "                                                emb_data[\"dict_ids2input_ids\"], \n",
    "                                                max_seq_len=20, embed_dim=4096,\n",
    "                                                wpe_scale=1/6).to(\"cuda\")\n",
    "elif text_encoder_type == \"RandomEmbeddingEncoder\":\n",
    "    text_feat_dir_old = '/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/training_datasets/objectRel_pilot_rndemb/caption_feature_wmask'\n",
    "    emb_data = th.load(join(text_feat_dir_old, \"word_embedding_dict.pt\")) \n",
    "    text_encoder = RandomEmbeddingEncoder(emb_data[\"embedding_dict\"], \n",
    "                                                emb_data[\"input_ids2dict_ids\"], \n",
    "                                                emb_data[\"dict_ids2input_ids\"], \n",
    "                                                ).to(\"cuda\")\n",
    "# %%\n",
    "savedir = f\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/results/{model_run_name}\"\n",
    "# figdir = f\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/{model_run_name}{suffix}/cross_attn_vis_figs\"\n",
    "result_dir = f\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/analysis_results/{model_run_name}{suffix}\"\n",
    "eval_dir = join(savedir, \"large_scale_eval_posthoc\")\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "#%%\n",
    "\n",
    "config = read_config(join(savedir, 'config.py'))\n",
    "weight_dtype = torch.float32\n",
    "if config.mixed_precision == \"fp16\": # accelerator.\n",
    "    weight_dtype = torch.float16\n",
    "elif config.mixed_precision == \"bf16\": # accelerator.\n",
    "    weight_dtype = torch.bfloat16\n",
    "    \n",
    "image_size = config.image_size  # @param [256, 512, 1024]\n",
    "latent_size = int(image_size) // 8\n",
    "pred_sigma = getattr(config, 'pred_sigma', True)\n",
    "learn_sigma = getattr(config, 'learn_sigma', True) and pred_sigma\n",
    "model_kwargs={\"window_block_indexes\": config.window_block_indexes, \"window_size\": config.window_size,\n",
    "                \"use_rel_pos\": config.use_rel_pos, \"lewei_scale\": config.lewei_scale, 'config':config,\n",
    "                'model_max_length': config.model_max_length}\n",
    "# train_diffusion = IDDPM(str(config.train_sampling_steps), learn_sigma=learn_sigma, pred_sigma=pred_sigma, snr=config.snr_loss)\n",
    "model = build_model(config.model,\n",
    "                config.grad_checkpointing,\n",
    "                config.get('fp32_attention', False),\n",
    "                input_size=latent_size,\n",
    "                learn_sigma=learn_sigma,\n",
    "                pred_sigma=pred_sigma,\n",
    "                **model_kwargs).train()\n",
    "num_layers = len(model.blocks)\n",
    "transformer = Transformer2DModel(\n",
    "        sample_size=image_size // 8,\n",
    "        num_layers=len(model.blocks),\n",
    "        attention_head_dim=model.blocks[0].hidden_size // model.num_heads,\n",
    "        in_channels=model.in_channels,\n",
    "        out_channels=model.out_channels,\n",
    "        patch_size=model.patch_size,\n",
    "        attention_bias=True,\n",
    "        num_attention_heads=model.num_heads,\n",
    "        cross_attention_dim=model.blocks[0].hidden_size,\n",
    "        activation_fn=\"gelu-approximate\",\n",
    "        num_embeds_ada_norm=1000,\n",
    "        norm_type=\"ada_norm_single\",\n",
    "        norm_elementwise_affine=False,\n",
    "        norm_eps=1e-6,\n",
    "        caption_channels=4096,\n",
    ")\n",
    "# state_dict = state_dict_convert(all_state_dict.pop(\"state_dict\"))\n",
    "transformer.load_state_dict(state_dict_convert(model.state_dict()))\n",
    "pipeline = PixArtAlphaPipeline_custom.from_pretrained(\n",
    "    \"PixArt-alpha/PixArt-XL-2-512x512\",\n",
    "    transformer=transformer,\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder=None,\n",
    "    torch_dtype=weight_dtype,\n",
    ")\n",
    "ckptdir = join(savedir, \"checkpoints\")\n",
    "ckpt = torch.load(join(ckptdir, ckpt_name))\n",
    "pipeline.transformer.load_state_dict(state_dict_convert(ckpt['state_dict']))\n",
    "# pipeline.transformer.load_state_dict(state_dict_convert(ckpt['state_dict']))\n",
    "pipeline.tokenizer = tokenizer\n",
    "pipeline.to(device=\"cuda\", dtype=weight_dtype);\n",
    "# pipeline.to(device=\"cuda\", dtype=torch.bfloat16);\n",
    "pipeline.text_encoder = text_encoder.to(device=\"cuda\", )\n",
    "# add attention map store hooks\n",
    "# pipeline.transformer = replace_attn_processor(pipeline.transformer)\n",
    "# attnvis_store = PixArtAttentionVisualizer_Store(pipeline)\n",
    "# attnvis_store.setup_hooks()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# %%\n",
    "# test one prompt works. \n",
    "out = pipeline(\"blue square below and to the right of red circle\", num_inference_steps=14, guidance_scale=4.5, max_sequence_length=20, \n",
    "         num_images_per_prompt=4, generator=th.Generator(device=\"cuda\").manual_seed(42), prompt_dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0e6f15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PixArtAlphaPipeline_custom {\n",
       "  \"_class_name\": \"PixArtAlphaPipeline_custom\",\n",
       "  \"_diffusers_version\": \"0.35.0.dev0\",\n",
       "  \"_name_or_path\": \"PixArt-alpha/PixArt-XL-2-512x512\",\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"DPMSolverMultistepScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"T5Tokenizer\"\n",
       "  ],\n",
       "  \"transformer\": [\n",
       "    \"diffusers\",\n",
       "    \"Transformer2DModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18998798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

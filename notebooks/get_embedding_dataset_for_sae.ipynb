{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings\n",
    "red __ blue (__ is one of 8 relation)\\\n",
    "blue __ red (__ is one of 8 relation)\\\n",
    "need a language env to run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/holylabs/LABS/sompolinsky_lab/Everyone/xupan/envs/gpt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "dataset_root = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/objectRel_pilot_rndembposemb\"\n",
    "caption_dir = join(dataset_root, \"captions\")\n",
    "image_dir = join(dataset_root, \"images\")\n",
    "img_feat_dir = join(dataset_root, \"img_vae_features_128resolution\")\n",
    "text_feat_dir = join(dataset_root, \"caption_feature_wmask\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_prompt_embeddings_randemb(tokenizer, text_encoder, validation_prompts, prompt_cache_dir=\"output/tmp/prompt_cache\", \n",
    "                           device=\"cuda\", max_length=20, t5_path=None, recompute=False):\n",
    "    \"\"\"Save T5 text embeddings for a list of prompts to cache directory.\n",
    "    \n",
    "    Args:\n",
    "        validation_prompts (list): List of text prompts to encode\n",
    "        prompt_cache_dir (str): Directory to save embeddings\n",
    "        device (str): Device to run encoding on\n",
    "        max_length (int): Max sequence length for tokenization\n",
    "        t5_path (str): Path to T5 model. If None, uses default path\n",
    "    \"\"\"\n",
    "    if t5_path is None:\n",
    "        t5_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\"\n",
    "    \n",
    "    result_col = []\n",
    "    os.makedirs(prompt_cache_dir, exist_ok=True)\n",
    "    # Load models\n",
    "    print(f\"Loading text encoder and tokenizer from {t5_path} ...\")\n",
    "    # tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "    # text_encoder = T5EncoderModel.from_pretrained(t5_path).to(device)\n",
    "    text_encoder = text_encoder.to(device)\n",
    "    # Save unconditioned embedding\n",
    "    uncond = tokenizer(\"\", max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "    uncond_prompt_embeds = text_encoder(uncond.input_ids, attention_mask=uncond.attention_mask)[0]\n",
    "    torch.save({'caption_embeds': uncond_prompt_embeds, 'emb_mask': uncond.attention_mask, 'prompt': ''}, \n",
    "               join(prompt_cache_dir,f'uncond_{max_length}token.pth'))\n",
    "    result_col.append({'prompt': '', 'caption_embeds': uncond_prompt_embeds, 'emb_mask': uncond.attention_mask})\n",
    "    print(\"Preparing Visualization prompt embeddings...\")\n",
    "    print(f\"Saving visualizate prompt text embedding at {prompt_cache_dir}\")\n",
    "    for prompt in validation_prompts:\n",
    "        if os.path.exists(join(prompt_cache_dir,f'{prompt}_{max_length}token.pth')) and not recompute:\n",
    "            result_col.append(torch.load(join(prompt_cache_dir,f'{prompt}_{max_length}token.pth')))\n",
    "            continue\n",
    "        print(f\"Mapping {prompt}...\")\n",
    "        caption_token = tokenizer(prompt, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "        caption_emb = text_encoder(caption_token.input_ids, attention_mask=caption_token.attention_mask)[0]\n",
    "        torch.save({'caption_embeds': caption_emb, 'emb_mask': caption_token.attention_mask, 'prompt': prompt}, \n",
    "                    join(prompt_cache_dir,f'{prompt}_{max_length}token.pth'))\n",
    "        result_col.append({'prompt': prompt, 'caption_embeds': caption_emb, 'emb_mask': caption_token.attention_mask})\n",
    "    print(\"Done!\")\n",
    "    # garbage collection\n",
    "    del tokenizer, text_encoder\n",
    "    torch.cuda.empty_cache()\n",
    "    return result_col\n",
    "\n",
    "\n",
    "def get_positional_encodings(seq_len, d_model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate positional encodings for a sequence.\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): Length of the sequence.\n",
    "        d_model (int): Dimension of the model (embedding size).\n",
    "        device (str): Device to place the tensor on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Positional encodings of shape (seq_len, d_model).\n",
    "    \"\"\"\n",
    "    position = th.arange(seq_len, dtype=th.float, device=device).unsqueeze(1)\n",
    "    div_term = th.exp(th.arange(0, d_model, 2, dtype=th.float, device=device) *\n",
    "                         -(th.log(th.tensor(10000.0)) / d_model))\n",
    "    wpe = th.zeros(seq_len, d_model, device=device)\n",
    "    wpe[:, 0::2] = th.sin(position * div_term)\n",
    "    wpe[:, 1::2] = th.cos(position * div_term)\n",
    "    return wpe\n",
    "\n",
    "# Create text encoder class\n",
    "class RandomEmbeddingEncoder_wPosEmb(nn.Module):\n",
    "    def __init__(self, embedding_dict=None, input_ids2dict_ids=None, dict_ids2input_ids=None, max_seq_len=20, embed_dim=4096, wpe_scale=1):\n",
    "        super().__init__()\n",
    "        if embedding_dict is None:\n",
    "            self.embedding_dict = th.load(join(text_feat_dir, \"word_embedding_dict.pt\"))[\"embedding_dict\"]\n",
    "            self.input_ids2dict_ids = th.load(join(text_feat_dir, \"word_embedding_dict.pt\"))[\"input_ids2dict_ids\"]\n",
    "            self.dict_ids2input_ids = th.load(join(text_feat_dir, \"word_embedding_dict.pt\"))[\"dict_ids2input_ids\"]\n",
    "        else:\n",
    "            self.embedding_dict = embedding_dict\n",
    "            self.input_ids2dict_ids = input_ids2dict_ids\n",
    "            self.dict_ids2input_ids = dict_ids2input_ids\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.wpe = get_positional_encodings(self.max_seq_len, self.embed_dim, device=\"cuda\") * wpe_scale\n",
    "        assert self.wpe.shape == (self.max_seq_len, self.embed_dim)\n",
    "        assert self.embed_dim == self.embedding_dict.shape[1]\n",
    "        \n",
    "    def __call__(self, input_ids, attention_mask=None):\n",
    "        return self.encode(input_ids, attention_mask)\n",
    "    \n",
    "    def encode(self, input_ids, attention_mask=None):\n",
    "        \"\"\"Convert input ids to embeddings\"\"\"\n",
    "        if isinstance(input_ids, list):\n",
    "            input_ids = th.tensor(input_ids)\n",
    "        # map the input_ids to dict ids \n",
    "        indices = th.tensor([self.input_ids2dict_ids[id.item()] for id in input_ids.reshape(-1)]).reshape(input_ids.shape)\n",
    "        # indices = th.tensor([self.input_ids2dict_ids[id.item()] for id in input_ids])\n",
    "        embeddings = self.embedding_dict[indices]\n",
    "        # add positional encoding \n",
    "        embeddings = embeddings + self.wpe[:embeddings.shape[1], :]\n",
    "        return embeddings, attention_mask\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.embedding_dict = self.embedding_dict.to(device)\n",
    "        self.wpe = self.wpe.to(device)\n",
    "        # self.input_ids2dict_ids = self.input_ids2dict_ids.to(device)\n",
    "        # self.dict_ids2input_ids = self.dict_ids2input_ids.to(device)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red is above blue', 'red is below blue', 'red is to the left of blue', 'red is to the right of blue', 'red is to the upper left of blue', 'red is to the upper right of blue', 'red is to the lower left of blue', 'red is to the lower right of blue', 'blue is above red', 'blue is below red', 'blue is to the left of red', 'blue is to the right of red', 'blue is to the upper left of red', 'blue is to the upper right of red', 'blue is to the lower left of red', 'blue is to the lower right of red']\n",
      "Loading text encoder and tokenizer from /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl ...\n",
      "Preparing Visualization prompt embeddings...\n",
      "Saving visualizate prompt text embedding at /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/red_blue_8_position_rndembposemb\n",
      "Mapping red is above blue...\n",
      "Mapping red is below blue...\n",
      "Mapping red is to the left of blue...\n",
      "Mapping red is to the right of blue...\n",
      "Mapping red is to the upper left of blue...\n",
      "Mapping red is to the upper right of blue...\n",
      "Mapping red is to the lower left of blue...\n",
      "Mapping red is to the lower right of blue...\n",
      "Mapping blue is above red...\n",
      "Mapping blue is below red...\n",
      "Mapping blue is to the left of red...\n",
      "Mapping blue is to the right of red...\n",
      "Mapping blue is to the upper left of red...\n",
      "Mapping blue is to the upper right of red...\n",
      "Mapping blue is to the lower left of red...\n",
      "Mapping blue is to the lower right of red...\n",
      "Done!\n",
      "0:  | token num:1\n",
      "1: red is above blue | token num:5\n",
      "2: red is below blue | token num:5\n",
      "3: red is to the left of blue | token num:8\n",
      "4: red is to the right of blue | token num:8\n",
      "5: red is to the upper left of blue | token num:9\n",
      "6: red is to the upper right of blue | token num:9\n",
      "7: red is to the lower left of blue | token num:9\n",
      "8: red is to the lower right of blue | token num:9\n",
      "9: blue is above red | token num:5\n",
      "10: blue is below red | token num:5\n",
      "11: blue is to the left of red | token num:8\n",
      "12: blue is to the right of red | token num:8\n",
      "13: blue is to the upper left of red | token num:9\n",
      "14: blue is to the upper right of red | token num:9\n",
      "15: blue is to the lower left of red | token num:9\n",
      "16: blue is to the lower right of red | token num:9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_769679/182296834.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  rnd_encoding = th.load(join(text_feat_dir, \"word_embedding_dict.pt\"))\n"
     ]
    }
   ],
   "source": [
    "T5_path = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/pretrained_models/t5_ckpts/t5-v1_1-xxl\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_path, )#subfolder=\"tokenizer\")\n",
    "encoder = T5EncoderModel.from_pretrained(T5_path)\n",
    "\n",
    "relations = [\n",
    "    \"above\",\n",
    "    \"below\",\n",
    "    \"to the left of\",\n",
    "    \"to the right of\",\n",
    "    \"to the upper left of\",\n",
    "    \"to the upper right of\",\n",
    "    \"to the lower left of\",\n",
    "    \"to the lower right of\",\n",
    "]\n",
    "# visu\n",
    "# alize prompts \n",
    "visualize_prompts = [f\"red is {relation} blue\" for relation in relations] + [f\"blue is {relation} red\" for relation in relations]\n",
    "\n",
    "print(visualize_prompts)\n",
    "\n",
    "prompt_cache_dir = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/red_blue_8_position_rndembposemb\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_path)\n",
    "rnd_encoding = th.load(join(text_feat_dir, \"word_embedding_dict.pt\"))\n",
    "rndpos_encoder = RandomEmbeddingEncoder_wPosEmb(rnd_encoding[\"embedding_dict\"], \n",
    "                                              rnd_encoding[\"input_ids2dict_ids\"], \n",
    "                                              rnd_encoding[\"dict_ids2input_ids\"], \n",
    "                                              max_seq_len=20, embed_dim=4096,\n",
    "                                              wpe_scale=1/6).to(\"cuda\")\n",
    "caption_embeddings = save_prompt_embeddings_randemb(tokenizer, rndpos_encoder, \n",
    "    visualize_prompts, prompt_cache_dir, device=\"cuda\", max_length=20, t5_path=T5_path, recompute=True)\n",
    "for i, embedding in enumerate(caption_embeddings):\n",
    "    print(f\"{i}: {embedding['prompt']} | token num:{embedding['emb_mask'].sum()}\")\n",
    "torch.save(caption_embeddings, join(prompt_cache_dir, \"caption_embeddings_list.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  | token num:1 | tensor([[-0.0609,  0.3837,  0.2152,  ...,  0.2301,  0.0813,  0.3842],\n",
      "        [ 0.3661,  0.2643,  0.2454,  ...,  0.1502,  0.1104,  0.1653],\n",
      "        [ 0.3774,  0.1049,  0.2577,  ...,  0.1502,  0.1104,  0.1653],\n",
      "        ...,\n",
      "        [ 0.3353,  0.2999,  0.2111,  ...,  0.1502,  0.1105,  0.1653],\n",
      "        [ 0.3907,  0.1500,  0.2712,  ...,  0.1502,  0.1105,  0.1653],\n",
      "        [ 0.2945,  0.0224,  0.1803,  ...,  0.1502,  0.1105,  0.1653]],\n",
      "       device='cuda:0')\n",
      "0:  | token num:1 | tensor([[-0.0609,  0.3837,  0.2152,  ...,  0.2301,  0.0813,  0.3842],\n",
      "        [ 0.3661,  0.2643,  0.2454,  ...,  0.1502,  0.1104,  0.1653],\n",
      "        [ 0.3774,  0.1049,  0.2577,  ...,  0.1502,  0.1104,  0.1653],\n",
      "        ...,\n",
      "        [ 0.3353,  0.2999,  0.2111,  ...,  0.1502,  0.1105,  0.1653],\n",
      "        [ 0.3907,  0.1500,  0.2712,  ...,  0.1502,  0.1105,  0.1653],\n",
      "        [ 0.2945,  0.0224,  0.1803,  ...,  0.1502,  0.1105,  0.1653]],\n",
      "       device='cuda:0')\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1: triangle is to the upper left of square | token num:9 | tensor([[ 0.1779,  0.2335, -0.0419,  ...,  0.2665, -0.1400,  0.1476],\n",
      "        [ 0.3597,  0.0423,  0.1074,  ...,  0.3582,  0.2011,  0.2286],\n",
      "        [-0.0013,  0.1006,  0.1808,  ...,  0.4624,  0.1619,  0.2231],\n",
      "        ...,\n",
      "        [ 0.0219,  0.0588,  0.3811,  ...,  0.1324, -0.0273,  0.1598],\n",
      "        [ 0.1040,  0.1928,  0.3809,  ...,  0.2301,  0.0815,  0.3842],\n",
      "        [ 0.2945,  0.0224,  0.1803,  ...,  0.1502,  0.1105,  0.1653]],\n",
      "       device='cuda:0')\n",
      "1: triangle is to the upper left of square | token num:9 | tensor([[ 0.1779,  0.2335, -0.0419,  ...,  0.2665, -0.1400,  0.1476],\n",
      "        [ 0.3597,  0.0423,  0.1074,  ...,  0.3582,  0.2011,  0.2286],\n",
      "        [-0.0013,  0.1006,  0.1808,  ...,  0.4624,  0.1619,  0.2231],\n",
      "        ...,\n",
      "        [ 0.0219,  0.0588,  0.3811,  ...,  0.1324, -0.0273,  0.1598],\n",
      "        [ 0.1040,  0.1928,  0.3809,  ...,  0.2301,  0.0815,  0.3842],\n",
      "        [ 0.2945,  0.0224,  0.1803,  ...,  0.1502,  0.1105,  0.1653]],\n",
      "       device='cuda:0')\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_769679/1500778430.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  new_embedding = th.load(join(\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/red_blue_8_position_rndembposemb\", \"caption_embeddings_list.pth\"))\n",
      "/tmp/ipykernel_769679/1500778430.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  old_embedding = th.load(join(\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/objectRel_pilot_rndembposemb\", \"caption_embeddings_list.pth\"))\n"
     ]
    }
   ],
   "source": [
    "# # check if the new embedding is the same as the old one. Yes.\n",
    "# new_embedding = th.load(join(\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/red_blue_8_position_rndembposemb\", \"caption_embeddings_list.pth\"))\n",
    "# old_embedding = th.load(join(\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/objectRel_pilot_rndembposemb\", \"caption_embeddings_list.pth\"))\n",
    "# for i in range(len(new_embedding)):\n",
    "#     print(f\"{i}: {new_embedding[i]['prompt']} | token num:{new_embedding[i]['emb_mask'].sum()} | {new_embedding[i]['caption_embeds'][0, :10]}\")\n",
    "#     print(f\"{i}: {old_embedding[i]['prompt']} | token num:{old_embedding[i]['emb_mask'].sum()} | {old_embedding[i]['caption_embeds'][0, :10]}\")\n",
    "#     print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get embedding using the prompt\n",
    "Not used. Use Binxu's code instead objectRel_cv2_annot_embed.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join \n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/n/home13/xupan/Projects/DiffusionObjectRelation/DiffusionObjectRelation/PixArt-alpha\")\n",
    "from diffusion import IDDPM\n",
    "# from diffusion.data.builder import build_dataset, build_dataloader, set_data_root\n",
    "from diffusion.model.builder import build_model\n",
    "from diffusion.utils.misc import set_random_seed, read_config, init_random_seed, DebugUnderflowOverflow\n",
    "sys.path.append(\"/n/home13/xupan/Projects/DiffusionObjectRelation/DiffusionObjectRelation/utils\")\n",
    "from pixart_utils import state_dict_convert\n",
    "from image_utils import pil_images_to_grid\n",
    "from diffusers import AutoencoderKL, Transformer2DModel, PixArtAlphaPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "# Add a new hook to get the embedding based on Binxu's code\n",
    "# subclass a new pipeline from PixArtAlphaPipeline\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from diffusers.pipelines.pipeline_utils import DiffusionPipeline, ImagePipelineOutput\n",
    "from diffusers.pipelines.pixart_alpha.pipeline_pixart_alpha import retrieve_timesteps\n",
    "from collections import defaultdict\n",
    "# from diffusers.pipelines.pixart_alpha import EXAMPLE_DOC_STRING, ImagePipelineOutput\n",
    "class PixArtAlphaPipeline_hookembedding(PixArtAlphaPipeline):\n",
    "    # def __init__(self, *args, **kwargs):\n",
    "    #     super().__init__(*args, **kwargs)\n",
    "    #     self.hook_handles = []\n",
    "    #     self.embedding = defaultdict(list)\n",
    "    @classmethod\n",
    "    def from_pretrained(self, *args, **kwargs):\n",
    "        pipeline = super().from_pretrained(*args, **kwargs)\n",
    "        pipeline.hook_handles = []\n",
    "        pipeline.embedding = defaultdict(list)\n",
    "        return pipeline\n",
    "\n",
    "    def clear_embedding(self):\n",
    "        self.embedding = defaultdict(list)\n",
    "    \n",
    "    def hook_forger(self, key: str):\n",
    "        \"\"\"Create a hook to capture attention patterns\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            self.embedding[key].append(input[0].chunk(2)[0].detach().cpu().numpy()) # only use the first half of the embedding, the second half is the negative embedding\n",
    "        return hook\n",
    "    \n",
    "    def setup_embedding_hooks(self, embedding_layer: int = None):\n",
    "        \"\"\"Set up hooks for all transformer blocks\"\"\"\n",
    "        # print(\"Setting up hooks for PixArt attention modules:\")\n",
    "        if embedding_layer is None:\n",
    "            for block_idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "                self.hook_handles.append(block.register_forward_hook(self.hook_forger(f\"block{block_idx:02d}\")))\n",
    "        else:\n",
    "            for block_idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "                if block_idx == embedding_layer:\n",
    "                    self.hook_handles.append(block.register_forward_hook(self.hook_forger(f\"block{block_idx:02d}\")))\n",
    "                    break\n",
    "\n",
    "    def cleanup_embedding_hooks(self):\n",
    "        \"\"\"Remove all hooks\"\"\"\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "        self.hook_handles = []\n",
    "    \n",
    "    # @replace_example_docstring(EXAMPLE_DOC_STRING)\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        embedding_when: float = 0.6, # relative time step to hook the embedding. 0.6 means 60% of the way through the diffusion process.\n",
    "        embedding_layer: int = None, # if None, hook all layers\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        negative_prompt: str = \"\",\n",
    "        num_inference_steps: int = 20,\n",
    "        timesteps: List[int] = None,\n",
    "        sigmas: List[float] = None,\n",
    "        guidance_scale: float = 4.5,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        prompt_attention_mask: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        callback: Optional[Callable[[int, int, torch.Tensor], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        clean_caption: bool = True,\n",
    "        use_resolution_binning: bool = True,\n",
    "        max_sequence_length: int = 120,\n",
    "        return_sample_pred_traj: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Union[ImagePipelineOutput, Tuple]:\n",
    "        \"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n",
    "                instead.\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
    "                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
    "                less than `1`).\n",
    "            num_inference_steps (`int`, *optional*, defaults to 100):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n",
    "                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n",
    "                passed will be used. Must be in descending order.\n",
    "            sigmas (`List[float]`, *optional*):\n",
    "                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n",
    "                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n",
    "                will be used.\n",
    "            guidance_scale (`float`, *optional*, defaults to 4.5):\n",
    "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
    "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
    "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
    "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
    "                usually at the expense of lower image quality.\n",
    "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate per prompt.\n",
    "            height (`int`, *optional*, defaults to self.unet.config.sample_size):\n",
    "                The height in pixels of the generated image.\n",
    "            width (`int`, *optional*, defaults to self.unet.config.sample_size):\n",
    "                The width in pixels of the generated image.\n",
    "            eta (`float`, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
    "                [`schedulers.DDIMScheduler`], will be ignored for others.\n",
    "            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
    "                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
    "                to make generation deterministic.\n",
    "            latents (`torch.Tensor`, *optional*):\n",
    "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
    "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
    "                tensor will ge generated by sampling using the supplied random `generator`.\n",
    "            prompt_embeds (`torch.Tensor`, *optional*):\n",
    "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
    "                provided, text embeddings will be generated from `prompt` input argument.\n",
    "            prompt_attention_mask (`torch.Tensor`, *optional*): Pre-generated attention mask for text embeddings.\n",
    "            negative_prompt_embeds (`torch.Tensor`, *optional*):\n",
    "                Pre-generated negative text embeddings. For PixArt-Alpha this negative prompt should be \"\". If not\n",
    "                provided, negative_prompt_embeds will be generated from `negative_prompt` input argument.\n",
    "            negative_prompt_attention_mask (`torch.Tensor`, *optional*):\n",
    "                Pre-generated attention mask for negative text embeddings.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.stable_diffusion.IFPipelineOutput`] instead of a plain tuple.\n",
    "            callback (`Callable`, *optional*):\n",
    "                A function that will be called every `callback_steps` steps during inference. The function will be\n",
    "                called with the following arguments: `callback(step: int, timestep: int, latents: torch.Tensor)`.\n",
    "            callback_steps (`int`, *optional*, defaults to 1):\n",
    "                The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
    "                called at every step.\n",
    "            clean_caption (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to clean the caption before creating embeddings. Requires `beautifulsoup4` and `ftfy` to\n",
    "                be installed. If the dependencies are not installed, the embeddings will be created from the raw\n",
    "                prompt.\n",
    "            use_resolution_binning (`bool` defaults to `True`):\n",
    "                If set to `True`, the requested height and width are first mapped to the closest resolutions using\n",
    "                `ASPECT_RATIO_1024_BIN`. After the produced latents are decoded into images, they are resized back to\n",
    "                the requested resolution. Useful for generating non-square images.\n",
    "            max_sequence_length (`int` defaults to 120): Maximum sequence length to use with the `prompt`.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.ImagePipelineOutput`] or `tuple`:\n",
    "                If `return_dict` is `True`, [`~pipelines.ImagePipelineOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is a list with the generated images\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        # initialize the embedding hook\n",
    "        self.clear_embedding()\n",
    "        self.cleanup_embedding_hooks()\n",
    "        #########################################\n",
    "        if \"mask_feature\" in kwargs:\n",
    "            deprecation_message = \"The use of `mask_feature` is deprecated. It is no longer used in any computation and that doesn't affect the end results. It will be removed in a future version.\"\n",
    "            # deprecate(\"mask_feature\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        height = height or self.transformer.config.sample_size * self.vae_scale_factor\n",
    "        width = width or self.transformer.config.sample_size * self.vae_scale_factor\n",
    "        # if use_resolution_binning:\n",
    "        #     if self.transformer.config.sample_size == 128:\n",
    "        #         aspect_ratio_bin = ASPECT_RATIO_1024_BIN\n",
    "        #     elif self.transformer.config.sample_size == 64:\n",
    "        #         aspect_ratio_bin = ASPECT_RATIO_512_BIN\n",
    "        #     elif self.transformer.config.sample_size == 32:\n",
    "        #         aspect_ratio_bin = ASPECT_RATIO_256_BIN\n",
    "        #     else:\n",
    "        #         raise ValueError(\"Invalid sample size\")\n",
    "        #     orig_height, orig_width = height, width\n",
    "        #     height, width = self.image_processor.classify_height_width_bin(height, width, ratios=aspect_ratio_bin)\n",
    "\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            height,\n",
    "            width,\n",
    "            negative_prompt,\n",
    "            callback_steps,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            prompt_attention_mask,\n",
    "            negative_prompt_attention_mask,\n",
    "        )\n",
    "\n",
    "        # 2. Default height and width to transformer\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            prompt_attention_mask,\n",
    "            negative_prompt_embeds,\n",
    "            negative_prompt_attention_mask,\n",
    "        ) = self.encode_prompt(\n",
    "            prompt,\n",
    "            do_classifier_free_guidance,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            device=device,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            prompt_attention_mask=prompt_attention_mask,\n",
    "            negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
    "            clean_caption=clean_caption,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "        )\n",
    "        if do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "            prompt_attention_mask = torch.cat([negative_prompt_attention_mask, prompt_attention_mask], dim=0)\n",
    "        # print(prompt_embeds.shape)\n",
    "        # print(prompt_attention_mask.shape)\n",
    "        # 4. Prepare timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(\n",
    "            self.scheduler, num_inference_steps, device, timesteps, sigmas\n",
    "        )\n",
    "        ################################################\n",
    "        # which timestep to hook the embedding\n",
    "        hook_timestep = timesteps[int(embedding_when * num_inference_steps)]\n",
    "        ################################################\n",
    "\n",
    "        # 5. Prepare latents.\n",
    "        latent_channels = self.transformer.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            latent_channels,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 6.1 Prepare micro-conditions.\n",
    "        added_cond_kwargs = {\"resolution\": None, \"aspect_ratio\": None}\n",
    "        if self.transformer.config.sample_size == 128:\n",
    "            resolution = torch.tensor([height, width]).repeat(batch_size * num_images_per_prompt, 1)\n",
    "            aspect_ratio = torch.tensor([float(height / width)]).repeat(batch_size * num_images_per_prompt, 1)\n",
    "            resolution = resolution.to(dtype=prompt_embeds.dtype, device=device)\n",
    "            aspect_ratio = aspect_ratio.to(dtype=prompt_embeds.dtype, device=device)\n",
    "\n",
    "            if do_classifier_free_guidance:\n",
    "                resolution = torch.cat([resolution, resolution], dim=0)\n",
    "                aspect_ratio = torch.cat([aspect_ratio, aspect_ratio], dim=0)\n",
    "\n",
    "            added_cond_kwargs = {\"resolution\": resolution, \"aspect_ratio\": aspect_ratio}\n",
    "\n",
    "        # 7. Denoising loop\n",
    "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
    "\n",
    "        pred_traj = []\n",
    "        latents_traj = []\n",
    "        t_traj = []\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                current_timestep = t\n",
    "                if not torch.is_tensor(current_timestep):\n",
    "                    # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "                    # This would be a good case for the `match` statement (Python 3.10+)\n",
    "                    is_mps = latent_model_input.device.type == \"mps\"\n",
    "                    if isinstance(current_timestep, float):\n",
    "                        dtype = torch.float32 if is_mps else torch.float64\n",
    "                    else:\n",
    "                        dtype = torch.int32 if is_mps else torch.int64\n",
    "                    current_timestep = torch.tensor([current_timestep], dtype=dtype, device=latent_model_input.device)\n",
    "                elif len(current_timestep.shape) == 0:\n",
    "                    current_timestep = current_timestep[None].to(latent_model_input.device)\n",
    "                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "                current_timestep = current_timestep.expand(latent_model_input.shape[0])\n",
    "\n",
    "                if t == hook_timestep:\n",
    "                    self.setup_embedding_hooks(embedding_layer)\n",
    "\n",
    "                # predict noise model_output\n",
    "                noise_pred = self.transformer(\n",
    "                    latent_model_input,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    encoder_attention_mask=prompt_attention_mask,\n",
    "                    timestep=current_timestep,\n",
    "                    added_cond_kwargs=added_cond_kwargs,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "\n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # learned sigma\n",
    "                if self.transformer.config.out_channels // 2 == latent_channels:\n",
    "                    noise_pred = noise_pred.chunk(2, dim=1)[0]\n",
    "                else:\n",
    "                    noise_pred = noise_pred\n",
    "\n",
    "                latents_traj.append(latents)\n",
    "                pred_traj.append(noise_pred)\n",
    "                # compute previous image: x_t -> x_t-1\n",
    "                if num_inference_steps == 1:\n",
    "                    # For DMD one step sampling: https://arxiv.org/abs/2311.18828\n",
    "                    latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).pred_original_sample\n",
    "                else:\n",
    "                    latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "                \n",
    "                # pred_traj.append(self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).pred_original_sample)\n",
    "                \n",
    "                t_traj.append(t)\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                        callback(step_idx, t, latents)\n",
    "\n",
    "                # early stop if reached the hook timestep\n",
    "                if t == hook_timestep:\n",
    "                    self.cleanup_embedding_hooks()\n",
    "                    # return self.embedding\n",
    "\n",
    "        latents_traj.append(latents)\n",
    "        if not output_type == \"latent\":\n",
    "            image = pipeline.vae.decode(latents.to(weight_dtype) / pipeline.vae.config.scaling_factor, return_dict=False)[0]\n",
    "            image = pipeline.image_processor.postprocess(image, output_type=\"pil\")\n",
    "        else:\n",
    "            image = latents\n",
    "\n",
    "        # if not output_type == \"latent\":\n",
    "        #     image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "        # Offload all models\n",
    "        # self.maybe_free_model_hooks()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (self.embedding, image,)\n",
    "        if return_sample_pred_traj:\n",
    "            return ImagePipelineOutput(images=image), pred_traj, latents_traj, t_traj\n",
    "        return ImagePipelineOutput(images=image)\n",
    "    \n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_embeddings(pipeline, validation_prompts, prompt_cache_dir, embedding_when=0.6, embedding_layer=None, max_length=120, weight_dtype=torch.float16,\n",
    "                   num_inference_steps=14, guidance_scale=4.5, num_images_per_prompt=25, device=\"cuda\"):\n",
    "    # logger.info(\"Running validation... \")\n",
    "    # device = accelerator.device\n",
    "    # model = accelerator.unwrap_model(model)\n",
    "    if validation_prompts is None:\n",
    "        validation_prompts = [\n",
    "            \"triangle is to the upper left of square\", \n",
    "            \"blue triangle is to the upper left of red square\", \n",
    "            \"triangle is above and to the right of square\", \n",
    "            \"blue circle is above and to the right of blue square\", \n",
    "            \"triangle is to the left of square\", \n",
    "            \"triangle is to the left of triangle\", \n",
    "            \"circle is below red square\",\n",
    "            \"red circle is to the left of blue square\",\n",
    "            \"blue square is to the right of red circle\",\n",
    "            \"red circle is above square\",\n",
    "            \"triangle is above red circle\",\n",
    "            \"red is above blue\",\n",
    "            \"red is to the left of red\",\n",
    "            \"blue triangle is above red triangle\", \n",
    "            \"blue circle is above blue square\", \n",
    "        ]\n",
    "    pipeline = pipeline.to(device)\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "    generator = torch.Generator(device=device).manual_seed(0)\n",
    "\n",
    "    uncond_data = torch.load(f'{prompt_cache_dir}/uncond_{max_length}token.pth', map_location='cpu')\n",
    "    uncond_prompt_embeds = uncond_data['caption_embeds'].to(device)\n",
    "    uncond_prompt_attention_mask = uncond_data['emb_mask'].to(device)\n",
    "\n",
    "    embeddings = []\n",
    "    images = []\n",
    "\n",
    "    for _, prompt in enumerate(validation_prompts):\n",
    "        if not os.path.exists(f'{prompt_cache_dir}/{prompt}_{max_length}token.pth'):\n",
    "            continue\n",
    "        embed = torch.load(f'{prompt_cache_dir}/{prompt}_{max_length}token.pth', map_location='cpu')\n",
    "        caption_embs, emb_masks = embed['caption_embeds'].to(device), embed['emb_mask'].to(device)\n",
    "        output = pipeline(\n",
    "            embedding_when=embedding_when,\n",
    "            embedding_layer=embedding_layer,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            # generator=generator,\n",
    "            guidance_scale=guidance_scale,\n",
    "            prompt_embeds=caption_embs,\n",
    "            prompt_attention_mask=emb_masks,\n",
    "            negative_prompt=None,\n",
    "            negative_prompt_embeds=uncond_prompt_embeds,\n",
    "            negative_prompt_attention_mask=uncond_prompt_attention_mask,\n",
    "            use_resolution_binning=False, # need this for smaller images like ours. \n",
    "            return_sample_pred_traj=False,\n",
    "            return_dict=False,\n",
    "            output_type=\"pil\",\n",
    "        )\n",
    "    \n",
    "        embeddings.append(output[0])\n",
    "        images.append(output[1])\n",
    "\n",
    "    return embeddings, images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dataset_dir = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/PixArt/output/embedding_datasets_for_SAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

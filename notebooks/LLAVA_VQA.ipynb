{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "print(torch.version.cuda)         # Should show '12.1' or your installed CUDA version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64232db0c59b41668c61bdf8f5ba33a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "What is shown in this image? [/INST] The image shows a simple geometric figure, which appears to be a red triangle. The background is a solid color, and there is a smaller blue rectangle to the right of the triangle. The image has a pixelated appearance, suggesting it might be a digital representation or a screenshot from a video game or a computer program. \n"
     ]
    }
   ],
   "source": [
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True) \n",
    "model.to(\"cuda:0\")\n",
    "\n",
    "# Load the local image\n",
    "image_path = \"example_QA.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Initialize your model and processor (assuming LLAVA is being used)\n",
    "#processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "#model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True) \n",
    "#model.to(\"cuda:0\")\n",
    "# caption guided VQA: \"The red triangle is diagonally up and left from the square,\"\n",
    "# Sample questions dictionary with structure for JSON saving\n",
    "questions_data = {\n",
    "    # shape count\n",
    "    \"1\": {\n",
    "        \"question\": \"How many shapes are present in the image?\",\n",
    "        \"options\": [\"A. 0\", \"B. 1\", \"C. 2\", \"D. equal to or more than 3\"],\n",
    "        \"instructions\": \"\\n Answer only in A, B, C or D.\",\n",
    "        \"reference_answer\": \"C\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"question\": \"How many blobs are present in the image?\",\n",
    "        \"options\": [\"A. 0\", \"B. 1\", \"C. 2\", \"D. equal to or more than 3\"],\n",
    "        \"instructions\": \"\\n Answer only in A, B, C or D.\",\n",
    "        \"reference_answer\": \"C\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"question\": \"Are there exactly two distinct shapes in the image?\",\n",
    "        \"options\": [\"A. Yes\", \"B. No\", \"C. Not sure\"],\n",
    "        \"instructions\": \"\\n Answer only in A, B or C.\",\n",
    "        \"reference_answer\": \"A\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None\n",
    "    },\n",
    "    # shape types\n",
    "    \"4\": {\n",
    "        \"question\": \"What is the shape that is red in color?\",\n",
    "        \"options\": [\"A. Circle\", \"B. Square\", \"C. Triangle\", \"D. Unidentifiable\", \"E. No red color shapes\"],\n",
    "        \"instructions\": \"\\n Answer only in A, B, C, D or E.\",\n",
    "        \"reference_answer\": \"C\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question cannot be 0\" \n",
    "    },\n",
    "    \"5\": {\n",
    "        \"question\": \"What is the shape that is blue in color?\",\n",
    "        \"options\": [\"A. Circle\", \"B. Square\", \"C. Triangle\", \"D. Unidentifiable\", \"E. No blue color shapes\"],\n",
    "        \"instructions\": \"\\n Answer only in A, B, C, D or E.\",\n",
    "        \"reference_answer\": \"B\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question cannot be 0\"\n",
    "    },\n",
    "    # color types \n",
    "    \"6\": {\n",
    "        \"question\": \"What color is the square in the image?\",\n",
    "        \"options\": [\"A. Red\", \"B. Blue\", \"C. Not red or blue\", \"D. No square shape\"],\n",
    "        \"instructions\": \"\\n Answer only in A, B, C or D.\",\n",
    "        \"reference_answer\": \"B\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question cannot be 0\"\n",
    "    },\n",
    "    \"7\": {\n",
    "        \"question\": \"What color is the circle in the image?\",\n",
    "        \"options\": [\"A. Red\", \"B. Blue\", \"C. Not red or blue\", \"D. No circle shape\"],\n",
    "        \"instructions\": \"\\n Answer only in A, B, C or D.\",\n",
    "        \"reference_answer\": \"D\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question cannot be 0\"\n",
    "    },\n",
    "    \"8\": {\n",
    "        \"question\": \"What color is the triangle in the image?\",\n",
    "        \"options\": [\"A. Red\", \"B. Blue\", \"C. Not red or blue\", \"D. No triangle shape\"],\n",
    "        \"instructions\": \"\\n Answer only in A, B, C or D.\",\n",
    "        \"reference_answer\": \"B\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question cannot be 0\"\n",
    "    },\n",
    "    # spatial relations\n",
    "    \"9\": {\n",
    "        \"question\": \"What's the spatial relationship between 2 blobs?\",\n",
    "        \"options\": [\"A. Directly above vs. below\", \"B. Directly left vs. right\", \"C. Upper-left vs. Lower-right\", \"D. Upper-right vs. Lower-left\", \"E. Overlapping\"],\n",
    "        \"instructions\": \"\\n Answer only in A, B, C, D or E.\",\n",
    "        \"reference_answer\": \"C\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question = 2\"\n",
    "    },\n",
    "    \"10\": {\n",
    "        \"question\": \"Where is the red triangle relative to the square?\",\n",
    "        \"options\": [\"A. Up and to the right\", \"B. Up and to the left\", \"C. Down and to the right\", \"D. Down and to the left\", \"E. Directly above\", \"F. Directly below\", \"G. Overlapping\"],\n",
    "        \"instructions\": \"\\n Answer only in A, B, C, D, E, F or G.\",\n",
    "        \"reference_answer\": \"B\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"correctly respond to above questions\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save initial questions data to a JSON file\n",
    "with open(\"questions.json\", \"w\") as f:\n",
    "    json.dump(questions_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many shapes are present in the image? Options: A. 0, B. 1, C. 2, D. equal to or more than 3 \n",
      " Answer only in A, B, C or D.\n",
      "How many blobs are present in the image? Options: A. 0, B. 1, C. 2, D. equal to or more than 3 \n",
      " Answer only in A, B, C or D.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there exactly two distinct shapes in the image? Options: A. Yes, B. No, C. Not sure \n",
      " Answer only in A, B or C.\n",
      "What is the shape that is red in color? Options: A. Circle, B. Square, C. Triangle, D. Unidentifiable, E. No red color shapes \n",
      " Answer only in A, B, C, D or E.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the shape that is blue in color? Options: A. Circle, B. Square, C. Triangle, D. Unidentifiable, E. No blue color shapes \n",
      " Answer only in A, B, C, D or E.\n",
      "What color is the square in the image? Options: A. Red, B. Blue, C. Not red or blue, D. No square shape \n",
      " Answer only in A, B, C or D.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color is the circle in the image? Options: A. Red, B. Blue, C. Not red or blue, D. No circle shape \n",
      " Answer only in A, B, C or D.\n",
      "What color is the triangle in the image? Options: A. Red, B. Blue, C. Not red or blue, D. No triangle shape \n",
      " Answer only in A, B, C or D.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the spatial relationship between 2 blobs? Options: A. Directly above vs. below, B. Directly left vs. right, C. Upper-left vs. Lower-right, D. Upper-right vs. Lower-left, E. Overlapping \n",
      " Answer only in A, B, C, D or E.\n",
      "Where is the red triangle relative to the square? Options: A. Up and to the right, B. Up and to the left, C. Down and to the right, D. Down and to the left, E. Directly above, F. Directly below, G. Overlapping \n",
      " Answer only in A, B, C, D, E, F or G.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Load and prompt the model with questions, update answers and scores\n",
    "def ask_model_and_update_json(processor, image, file_path=\"questions.json\"):\n",
    "    # Load questions from JSON\n",
    "    with open(file_path, \"r\") as f:\n",
    "        questions = json.load(f)\n",
    "\n",
    "    # Iterate over each question and prompt the model\n",
    "    for q_id, q_data in questions.items():\n",
    "        question_text = q_data[\"question\"]\n",
    "        options = q_data[\"options\"]\n",
    "        instructions = q_data[\"instructions\"]\n",
    "        reference_answer = q_data[\"reference_answer\"]\n",
    "        text_input = f\"{question_text} Options: {', '.join(options)} {instructions}\"\n",
    "        print(text_input)\n",
    "        conversation = [\n",
    "            {\n",
    "\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": text_input},\n",
    "                {\"type\": \"image\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        # Format the question as a prompt for the model\n",
    "        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "        # Assume we have an image tensor prepared for inference\n",
    "        # Replace 'image' below with the actual image tensor\n",
    "        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        \n",
    "        # Generate model's answer\n",
    "        output = model.generate(**inputs, max_new_tokens=50)\n",
    "        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n",
    "        # Extract only the answer by splitting on \"[INST]\" and taking the last non-empty part\n",
    "        model_answer = decoded_output.split(\"[INST]\")[-1].strip()\n",
    "\n",
    "        # If the answer includes extra spaces or text, you can further clean it up\n",
    "        model_answer = model_answer.split()[-1]  # This assumes the answer is a single letter like \"A\", \"B\", \"C\", or \"D\"\n",
    "        \n",
    "        # Compare model answer to reference answer\n",
    "        score = 1 if model_answer.strip() == reference_answer else 0\n",
    "        \n",
    "        # Update the question data with model answer and score\n",
    "        questions[q_id][\"model_answer\"] = model_answer\n",
    "        questions[q_id][\"score\"] = score\n",
    "\n",
    "    # Save updated questions with answers and scores back to JSON\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(questions, f, indent=4)\n",
    "\n",
    "# Example image loading, replace with your image loading process\n",
    "\n",
    "\n",
    "# Download an example image for testing (replace URL with your actual image URL)\n",
    "image_path = \"example_QA.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Run the function to ask model and update JSON with results\n",
    "ask_model_and_update_json(processor, image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model ID; replace with the desired LLaVA model variant\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load an image from a URL or local path\n",
    "image_path = \"example_QA.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Define the text prompt\n",
    "text_prompt = \"Describe the content of the image.\"\n",
    "\n",
    "# Prepare the input\n",
    "inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate a response\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# Decode and print the response\n",
    "response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

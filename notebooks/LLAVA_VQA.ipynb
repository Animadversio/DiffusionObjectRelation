{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677a7091547e4859a46cb152a628d8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25e3b0fcb2c4806beeac1a3287c0653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "What is shown in this image? [/INST] The image appears to be a radar chart, which is a type of multi-dimensional plot that displays values for multiple quantitative variables represented on axes starting from the same point. This particular radar chart is showing the performance of different models or systems across various metrics.\n",
      "\n",
      "The axes represent different metrics or benchmarks, such as MM-Vet, MM-Vet, MM-Vet, MM-Vet, MM-Vet, MM-\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True) \n",
    "model.to(\"cuda:0\")\n",
    "\n",
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "url = \"example_QA.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# Specify the model ID; replace with the desired LLaVA model variant\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load an image from a URL or local path\n",
    "image_url = \"https://example.com/image.jpg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "# Define the text prompt\n",
    "text_prompt = \"Describe the content of the image.\"\n",
    "\n",
    "# Prepare the input\n",
    "inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate a response\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# Decode and print the response\n",
    "response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Initialize your model and processor (assuming LLAVA is being used)\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True) \n",
    "model.to(\"cuda:0\")\n",
    "# caption guided VQA: \"The red square is diagonally up and right from the circle,\"\n",
    "# Sample questions dictionary with structure for JSON saving\n",
    "questions_data = {\n",
    "    # shape count\n",
    "    \"1\": {\n",
    "        \"question\": \"How many shapes are present in the image?\",\n",
    "        \"options\": [\"0\", \"1\", \"2\", \"equal to or more than 3\"],\n",
    "        \"reference_answer\": \"2\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"question\": \"How many blobs are present in the image?\",\n",
    "        \"options\": [\"0\", \"1\", \"2\", \"equal to or more than 3\"],\n",
    "        \"reference_answer\": \"2\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"question\": \"Are there exactly two distinct shapes in the image?\",\n",
    "        \"options\": [\"Yes\", \"No\"],\n",
    "        \"reference_answer\": \"Yes\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None\n",
    "    },\n",
    "    # shape types\n",
    "    \"4\": {\n",
    "        \"question\": \"What is the shape that is red in color?\",\n",
    "        \"options\": [\"Circle\", \"Square\", \"Triangle\", \"Unidentifiable\", \"No red color shapes\"],\n",
    "        \"reference_answer\": \"Square\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question cannot be 0\" \n",
    "    },\n",
    "    \"5\": {\n",
    "        \"question\": \"What is the shape that is blue in color?\",\n",
    "        \"options\": [\"Circle\", \"Square\", \"Triangle\", \"Unidentifiable\", \"No blue color shapes\"],\n",
    "        \"reference_answer\": \"Circle\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question cannot be 0\"\n",
    "    },\n",
    "    # color types \n",
    "    \"6\": {\n",
    "        \"question\": \"What color is the square in the image?\",\n",
    "        \"options\": [\"Red\", \"Blue\", \"Not red or blue\", \"No square shape\"],\n",
    "        \"reference_answer\": \"Red\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question cannot be 0\"\n",
    "    },\n",
    "    \"7\": {\n",
    "        \"question\": \"What color is the circle in the image?\",\n",
    "        \"options\": [\"Red\", \"Blue\", \"Not red or blue\", \"No circle shape\"],\n",
    "        \"reference_answer\": \"Blue\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question cannot be 0\"\n",
    "    },\n",
    "    \"8\": {\n",
    "        \"question\": \"What color is the triangle in the image?\",\n",
    "        \"options\": [\"Red\", \"Blue\", \"Not red or blue\", \"No triangle shape\"],\n",
    "        \"reference_answer\": \"No triangle shape\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question cannot be 0\"\n",
    "    },\n",
    "    # spatial relations\n",
    "    \"9\": {\n",
    "        \"question\": \"What's the spatial relationship between 2 blobs?\",\n",
    "        \"options\": [\"Directly above vs. below\", \"Directly left vs. right\", \"Upper-left vs. Lower-right\", \"Upper-right vs. Lower-left\", \"Overlapping\"],\n",
    "        \"reference_answer\": \"Upper-right vs. Lower-left\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"shapes question = 2\"\n",
    "    },\n",
    "    \"10\": {\n",
    "        \"question\": \"Where is the red square relative to the circle?\",\n",
    "        \"options\": [\"Up and to the right\", \"Up and to the left\", \"Down and to the right\", \"Down and to the left\", \"Directly above\", \"Directly below\", \"Overlapping\"],\n",
    "        \"reference_answer\": \"Up and to the right\",\n",
    "        \"model_answer\": None,\n",
    "        \"score\": None,\n",
    "        \"condition\": \"correctly respond to above questions\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save initial questions data to a JSON file\n",
    "with open(\"questions.json\", \"w\") as f:\n",
    "    json.dump(questions_data, f, indent=4)\n",
    "\n",
    "# Load and prompt the model with questions, update answers and scores\n",
    "def ask_model_and_update_json(file_path=\"questions.json\"):\n",
    "    # Load questions from JSON\n",
    "    with open(file_path, \"r\") as f:\n",
    "        questions = json.load(f)\n",
    "\n",
    "    # Iterate over each question and prompt the model\n",
    "    for q_id, q_data in questions.items():\n",
    "        question_text = q_data[\"question\"]\n",
    "        options = q_data[\"options\"]\n",
    "        reference_answer = q_data[\"reference_answer\"]\n",
    "\n",
    "        # Format the question as a prompt for the model\n",
    "        prompt = f\"{question_text} Options: {', '.join(options)}\"\n",
    "\n",
    "        # Assume we have an image tensor prepared for inference\n",
    "        # Replace 'image' below with the actual image tensor\n",
    "        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        \n",
    "        # Generate model's answer\n",
    "        output = model.generate(**inputs, max_new_tokens=50)\n",
    "        model_answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Compare model answer to reference answer\n",
    "        score = 1 if model_answer.strip() == reference_answer else 0\n",
    "        \n",
    "        # Update the question data with model answer and score\n",
    "        questions[q_id][\"model_answer\"] = model_answer\n",
    "        questions[q_id][\"score\"] = score\n",
    "\n",
    "    # Save updated questions with answers and scores back to JSON\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(questions, f, indent=4)\n",
    "\n",
    "# Example image loading, replace with your image loading process\n",
    "\n",
    "\n",
    "# Download an example image for testing (replace URL with your actual image URL)\n",
    "url = \"example_QA.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Run the function to ask model and update JSON with results\n",
    "ask_model_and_update_json()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
